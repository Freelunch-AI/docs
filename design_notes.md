
# __*Freelunch*__

This document tries describes Freelunch, a startup that making the next generation of MLOps tools. This is done while exploring the space of production ML. 

__Reading guide:__

* __If you have < 5 minutes to spend:__ you can read _TL;DR_

* __If you have at least 5 minutes to spend:__ you can read _TL;DR_, _Problem_ and _Explaining Freelunch in a few steps_

* __If you have at least 10 minutes to spend:__ you can read _TL;DR_, _Problem_, _Explaining Freelunch in a few steps_ and _Freelunch vs current best open source options_ sections

* __If you have at least 20 minutes to spend:__ you can read _TL;DR_, _Problem__, _Explaining Freelunch in a few steps_
, __Freelunch vs current best open source options__ and _Anatomy of Freelunch MLOps platforms_ sections

* __If you have at least 1 day to spend:__ you can read _Core_ section

* __If you have at least 1 week to spend:__ you can read it all

Thanks for reading! :beers:

<Details>
    <summary><b>Keywords</b></summary>

## Keywords:

AI, ML, MLOps, ML Platorm, ML System, ML Engineering, Declarative MLOps, Copilot
</Details>

<Details open>
    <summary><b>Author</b></summary>

## :trollface: __Author__

__Name:__ Bruno Scaglione  
__Contact:__ bruno.c.scaglione@gmail.com
</Details>

<Details open>
    <summary><b>Document disclaimer</b></summary>

## :warning: __Document disclaimer__

1. __This is an envolving document.__

    * If something is wrong or missing, please help me fix or improve it.
    * If you have some suggestion, please let me know.

2. __The document presents *Freelunch* as if it is already built, however, this is not the case. It is being built.__
</Details>

<Details>
    <summary><b>Background: MLOps problems</b></summary>
    
## __Background: MLOps problems. Building and maintaining ML Systems is hard because:__ 

1. There is a long tail of silent bugs (in addition to traditional bugs) which can be either in data, code and/or ML methods.
2. Components tend to be more coupled than in a traditional software system.
3. Models that power these systems tend to have poor interpretability.
4. Engineers have to know about Systems and Machine Learning.
5. Everything is changing fast: a lot of new method and tools (or new versions of existing tools) pop up every week.
6. In big data/big models regime, mistakes are very costly.
7. They systems can be doing more high-stakes tasks (e.g., self-driving) with unrealiable components.
8. With big data, comes big privacy responsabilities.
9. Due to the new added layer of ML, these systems becomes more complex than traditional software systems.
10. MLOps is a new field, so the best practices and patterns and not well worked out yet.

And these problems are especially important for:

1.  AI-core companies (companies that would die without AI) that rely heavily on their ML Systems.
2.  Critical sectors (e.g., Medicine and Autonomous Vehicles) that need strong QA on their ML Systems.
</Details>

<details>
    <summary><b>Proposed solution: an open source tool for continuous improvement of your MLOps/LLMOps + a copilot on top of it</b></summary>

## __Proposed solution: an open source tool for continuous improvement of your MLOps/LLMOps + a copilot on top of it__

<details>
    <summary><b>Core</b></summary>

## :clubs:__Core__

<details open>
    <summary><b>TL;DR</b></summary>

### :gem: __TL;DR__

__Helps AI teams build and improve AI Platforms. *Freelunch* follows the open-core model: (1) *Freelunch OSS*: open source tool that lets teams build ML platforms in days and keep improving their ML system (using the platform) and the platform itself. It does not reinvent the wheel, leverages existing tools that are great at what they do (e.g., Seldon Core for model deployment and Weights & Biases for model experiment tracking); (2) *Freelunch Copilot*: SaaS ML Engineering copilot; it works on top of *Freelunch OSS* and feels just like a Human ML Engineer (e.g., makes PRs, interacts via slack, builds models, operates de system, etc).<sup>1</sup>.__

__No more:__ 

1. Being lost in the sea of MLOps tools: evaluating, picking, stiching them together and integrating with your DevOps stack
2. Lock-in and lack of customization of pre-built MLOps platforms (e.g., Sagemaker or ClearML)
3. Lack of best practices
4. Hours debugging
5. ML that doesnt impact Business Metrics
6. Catastrophic or suprising production failures
7. Waiting for production for evaluating changes
8. Lack of team coordination and well-defined abstraction boundaries
9. Months to get a MLOps platform up and running
10. Need for deep ML and Systems expertise

__Freelunch is 4 things:__

1. __Platform Building__
    1. __MLOPs Standards:__ codebase, pattern and building block standardization according to best practices.
    2. __MLOPs Hub:__ composable and reusable implementations from the community that follow the aforementioned standards.
    3. __No code head-start:__ saves you time by designing (interactively with you) and building a head-start MLOps platform (which can be used independently of freelunch also) according to your needs. You then do manual fine-grained modifications on the resulting codebase, following the codebase flavour standard.
2. __Platform and System QA (Integration Tests, Evaluation, Monitoring and Observability):__ long-running integration tests, holistic evaluation and simulation-based observability of the MLOps platform and ML System in a background cluster (with simulated or production traffic) to: (1) surface problems before they appear in production, (2) identify improvement opportunities and (3) train new engineers.
3. __Platform-level Experiment Tracking:__ track, visualize, promote and delete experiments at the Platform-level (e.g. experimenting with different ML monitoring solutions and then picking one for PR). Also, do offline causal inference on top of experiement parameters and outcomes to uncover the root cause of changes in outcome, this guides you towards your next experiment.
4. __Cost tracking and optimization:__ track costs ang get cost reduction suggestions for Freelunch itself.

__Freelunch enables 2 embedded continuous ci/cd loops (B. and C.) with your DevOps ci/cd loop (A.).__

- __(A)  DevOps CI/CD Loop__: IT System Improvement using Base CI/CD. Software Engineers improve the IT System as to increase Business Metrics. This loop tests and deploys non-ML services and pipelines. This loop should not modify code related to the ML Lifecycle.

- __(B)  Outer MLOps CI/CD Loop__: ML Platform Improvement using Base CI/CD.  ML Platform Engineers improve the ML platform as to increase ML System metrics. They also debug the ML  platform in response to ML Practitioners’  complaints and include capabilities requested. This loop tests and deploys ML platform services and pipelines. This loop should only modify code related to the ML 
Platform. __Freelunch itself helps here.__

    - __(C)  Inner MLOps CI/CD Loop__: ML System Improvement using ML Platform CI/CD. ML    Practitioners (lead/manager, engineers, data scientists, subject matter experts and/or annotators ) improve artifacts (e.g., docs, config, data, models, pre/post processors, context retrievers, etc) as to increase business metrics, using the ML  platform. They also post bugs to the ML platform team. This loop tests, evaluates and deploys ML Artifacts. This loop shouldn't modify any production code. __Freelunch-built ML platforms help here.__

__For companies that already have an internal MLOps platform:__ we offer detailed documention, support and free help from our MLOps copilot (Virtual ML Engineer)  to help you make it Freelunch-compatible, so that you can start reaping systematic continuous improvement.

__Supports your entire MLOps/LLMOps journey:__ from small scale to large scale, from small team to big team, from managed pieces to open source ones, from simple platforms to complex ones and from less engineering rigour to more.

__Explaining in terms of existing tools:__ 

Freelunch provides the MLOPs lifecycle completeness of ClearML, the lego-block experience of Retool, the tool flexibility and ease of ZenML, support for the strong kubernetes ecosystem of Kubeflow, the structure and best practices of Kedro, the production-grade capabilities of managed MLOps Platforms and the artifact sharing of HuggingFace Hub; while adding steroids such as: Interactive Platform Design, Platform and System Evaluation, Simulation-based Observability, Platform-level Experiment Tracking, Platforms with ML-Business KPI Aligment and generalized CI/CD, etc.

The closest tool to Freelunch is [CLAIMED](https://github.com/claimed-framework/component-library). CLAIMED provides a set of ready-made components (analogous to our Hub that stores pattern specifications and implementations) a visual designer to help you compose the components in a no code/low code manner (analogous to our interactive designer) and a compiler that makes it easy for you to build CLAIMED components (analogous to the Freelunch compiler). 

The problem is that CLAIMED has a clear focus on rapid prototyping and does not:

1. Enforce software enginering best practices (e.g., test-driven development, clear abstraction boundaries between personas, data source abstraction, separate workspaces for different purposes)
2. Incentivize the usage of MLOps patterns (e.g., a RAG LLM deployment pattern)
3. Contain production-grade capabilities (e.g., project management with RBAC, operations, collaboration, compliance management, security testing)
4. Only help you build ML Systems, not MLOps platforms (e.g., doesnt help with building CI/CD infra, platform-level and artifact-level experimentation, evaluation and observability, data discovery, ML-Business KPI aligment)

Thus is not a suitable option for building prodution MLOps platforms.

__Notes:__ Freelunch can make use of ZenML and Kubeflow Pipelines as powerfull virtual and actual ML workflow orchestrators, respectively; and Freelunch largely makes open source MLOps platforms (e.g., ClearML) purposless. Managed MLOps platforms (e.g. Datarobot) still have their place, because their target customer are companies without much ML expertise that want the simplest solution they can control. As a matter of fact, Managed MLOps platforms can now use Freelunch behind the scenes.

<details>
    <summary><i>Footnotes</i></summary>

#### _Definitions_

1. __ML System (online):__ stack of components running on top of an isolated compute environment (logical machine (VM or bare metal machine), logical cluster (virtual cluster, namespace cluster or bare metal cluster), embedded device or app sandbox) that either: (1) executes tasks for clients via ML models (LLMs included) or (2) provides data that powers the aforementioned task execution by  a downstream ML System. Developers interact with ML Systems via an API (networking or library API) which specifies how to get inferences and which/how data needs to be sent to it.


2. __MLOps Platform (online and offline):__ the infrastructure that makes it easier to maintain and improve *ML Systems*. It makes the life of applied ML practitioners easier by giving them MLOps automation, high-level abstractions and best practices for free. Typically has 8 things:

    1. __Initial ML System (online):__ the ML System that is deployed as a starting point. Often heuristics or simple ml models are used, instead of complex ml models.

    2. __ML System State Store (online):__ receives state-changing events from the ML System and stores (in append-only manner) the events sequentially. After every period T, it calculates current state (from past state and state-changing eventssince then) and store the current state to avoid doing all the computation again (as a backup).

    3. __Generalized CI/CD (online):__ pipelines (admiistrated by a workflow orchestrator) that receive events (originated from the ML System or engineers) and deploys appropriate changes to improve the ML System.

    4. __Observability & Evaluation (offline and online)__
        1. __Evaluation (offline):__ Evaluation of the ML System according various metrics (e.g., predictive power of models, performance profiling, cost of the system, security, etc) that togehter are able to paint a picture of how good the ML System is. This is needed to guide platform-level experimentation and avoid unecessary production risk.
        2. __Observability (offline and online):__ 
            1. __Monitoring (online):__ collection, storage and analysis of telemetry data/metadata of the ML System with the goal of identifying problems to be solved. Once problems are pinned down, actions to correct them should be easy to identify and take.
            2. __Fine-grained Observability (offline):__ root cause analysis, visualization of the production ML System working, visualization of the ML Systme working in simulation and multiple evaluation views of it to find improvement opportunities.

    5. __Artifact-level Experimentation & Evaluation (offline):__ evaluate data and model artifacts. Needs an artifact evaluator and all experiments need to happen under a versioning framework. 

    6. __Data Discovery & EDA (offline):__
        1. __Data Discovery:__ data from multiple sources can be understood, fetched and aggregated into working files and data structures.
        2. __EDA:__ data is better understood via visualizations, statistical metrics and tests and fitting heuristic models. The output of this stage are EDA reports.

    7. __Management (Offline & online)__
        1. __KPI Alignment (offline):__ align MLOps platform and system KPI goals with Business KPI goals.
        2. __Collaboration (Offline & online)__
        3. __Onboarding (Offline)__

    8. __Activity Logging__ (Offline & Online)

</details>
</details>

<details>
    <summary><b>Problem</b></summary>

### __Problem__

The need for *Freelunch* comes from multiple ML teams:

[Building their own internal MLOps platforms](https://www.evidentlyai.com/ml-platforms) and:

1. Struggling to find the best combination of tools among the sea of MLOps.
2. Suffering to keep up with transitions to new tools that pop up constantly.
3. Having a hard time combining IT and DevOps world with the MLOps world.
4. Having messy codebases full of glue code and patches
5. Struggling to manage ROI of ML projects
6. Struggling with complexity of ML Systems and Platforms
7. Struggling to evaluate ML Systems and MLOps platforms
8. Struggling with integration/coupling hell, where 1 component changes, and because it is coupled with others, the others are manually changed accordingly to support the initial change.

As evidence for the proposed solution (*Freelunch OSS*):

* [A comment from a MLOps tech lead from deepsense.ai (an AI consulting company) on building a similar closed source thing (a tool for building MLOps platforms using MLOps building blocks)](https://mlops-community.slack.com/archives/C0253TQTUQ0/p1644390182275929)

* [The recent birth of the AI Infrastructure Alliance (AIIA) with the goal of standardazing MLOps and develop tool stacks](https://ai-infrastructure.org/)

* [The sucess of internal DevOps platforms, which can seen as the "older brothers" of MLOps platforms](https://hackernoon.com/an-in-depth-guide-to-internal-developer-platforms-idps)

* [OPEA: a similar tool being spearheded by Intel and incubated in the Linux Foundation](https://opea.dev/)

* [Success of ZenML for providing clear abstraction boundaries for platform and applied engineers within the ML lifecycle](https://www.zenml.io/)

</details>

<details>
    <summary><b>We are not/Our users are not</b></summary>

### __We are not/Our users are not__

#### <mark>We are __NOT__:</mark>

1. __A typical all-in-one MLOps solution.__

    1. Open-source all-in-ones
        1. General Purpose
            1. Set of Workload Components
                1. *TFX*
                2. *Xorbits*
            2. Platforms (Depends on specific tools)
                1. Incomplete Platforms (e.g., no Monitoring/Observability)
                    1. *Kubeflow*
                    2. *MLFlow*
                    3. *SystemDS*
                    4. *SuperDuperDB*
                    5. *Determined*
                    6. *OpenMLOps*
                    8. *Jina*
                    9. *PrimeHub*
                    10. *UnionML*
                2. Complete Platforms
                    1. *ClearML*
                    2. *Polyaxon*
                    3. *MLRun*
                    4. *oss-mlops-platform*
            3. Frameworks (Largely tool-agnostic)
                1. Incomplete Frameworks (e.g., no Monitoring/Observability)
                    1. *CLAIMED*
                    2. *Dstack*
                    3. *FuseML*
                    4. *Kedro*
                2. Complete Frameworks
                    1. *ZenML*
        2. Specific
            1. By sector
                1. Marketplaces: *Wyvern*
                2. Investment *qlib*
            2. By domain
                1. CV: *NVIDIA DeepStream SDK*, *Savant*
                2. Speech Processing: *espnet*        
            3. By task
                1. Chatbots: *NVIDIA NeMo*, *FastChat*, *Rasa Open Source*, *Lobe Chat*, *Superagent*, *Cheshire Cat*, *Botonic*, *Tock*, *Pipecat*, *wechaty*
                2. Recommender Systems: *NVIDIA Merlin SDK*
            4. By property
                1. Privacy-preserving: *heflow*, *concrete-ml*
            5. By method
                1. Generative ML/LLMs: *axflow*, *agenta*, *DB-GPT*, *Dify*, *llmware*, *langfuse*, *pezzo*, *bisheng*
                    1. Agents: *agentsea*, *AGiXT*, *TaskingAI*

    2. __Proprietary MLOps platforms__
        1. From Major Cloud Providers (e.g., *AWS Sagemaker*, *Vertex AI*)
        2. Cloud Agnostic
            1. No-code 
                1. Managed (e.g., *Akkio*, *Datarobot Rata Robot*, *Sagemaker Canvas*, *H20 Driverless AI*, *The AI & Analyitcs Engine*, *Obviously AI*, *Databricks AutoML*)
                2. Self-hosting (e.g., *Modela.ai*)
            2. Low-code (e.g., *Datarobot*, *Dataiku*, *H2O*, *Valohai*, *Continual.ai*, *aixplain*, *c3.ai*, *Telepath*, *getindata*, *radicalbit*, *fleak*)
            3. Medium-code 
                1. General-purpose (e.g., *Paperspace Gradient*, *W&B*, *Comet*, *MindsDB*, *Iguazio*, *Red Hat OpenShift Data Science*, *managed ClearML*, *Neu.ro*, *Outerbounds (managed Metaflow)*, *Seldon*, *BentoML*, *Modular*, *Wallaroo.ai*, *Truefoundry*, *Vessl.ai*, *Lightning.ai*, *cnvrg.io*, *deploifai*, *Prem AI*, *striveworks*)
                2. Specialized
                    1. By requirement
                        1. Real-time (e.g., *TurboML*, *Claypot AI*)
                    2. By method
                        1. LLMs (e.g., *Together.ai*, *Langsmith*, *Scale Generative AI Platform*, *Parea*, *Galileo*, *HoneyHive*, *Klu.ai*, *Freeplay*, *Giga ML*, *Lamini*, *Vellum*, *Vianai*, *Parea AI*, *Athina AI*, *AilaFlow*, *Airkit.ai*, *Rebyte*, *Keywords AI*, *Helicone*, *GradientJ*, *LastMile AI*, *Helix*, *Adaptive ML*)
                            1. By method
                                1. RAG (e.g., *Vectara*)
                                2. Agents (e.g., *Steamship*, *Langsmith*, *crewAI*, *Stack AI*, *Emergence AI*, *Lyzr*)
                                    1. Web Agents (e.g., *Lindy.ai*)
                                2. SLMs (e.g., *Arcee.ai*)
                            2. By use case
                                1. Chatbot (e.g., *Sierra*)
                    3. By focus
                        1. Data-centric (e.g., *Cleanlab*, *Scale AI*, *mirry.ai*, *markovML*, *co-one*) 
                        2. Federated ML (e.g., *Apheris*)
                        3. Secure ML (e.g., *grayswan*)
                        4. Data (e.g., *Chalk*, *hex*)
                    4. By sector
                        1. Defense: (e.g., *Revela*)
                        2. Factory (e.g., MakinaRocks)
                    5. By data modality
                        1. CV (e.g., *Dataleon*, *EyeFlow.AI*, *Picsellia*, *Atos*, *Tuba.ai*, *Tenyks*, *crowdai*, *devisionx*)
                        2. Geospatial (e.g., *Deep Block*)
                        3. NLP (e.g., *Dialogflow*, *Wit.ai*, ​​​​​​*​MonkeyLearn*, *Rossum*)
                        4. Video & Audio (e.g., Sieve)

    __*Problem with them*:__

        ""
            Companies that are creating value with machine learning are tech companies, and tech companies have their own stack. Today, that means that they've picked their own permutation of the cloud provider - programming language(s) - architecture combo, and use that to build a very specific product. At Monzo, for example, we picked AWS, Golang, & microservices on kubernetes, and we're building a bank.

            Many of the systems that I get pitched for machine learning are end-to-end AI platforms. They sell themselves on either the wonderful technologies they use under the hood, or the little code that I would have to write when using them.

            Using a separate end-to-end AI platform would mean branching all machine learning systems away from the primary way that the company ships its value. We'd be constrained to doing things the way that platform allows us to do it, rather than being able to mold that ourselves. We'd be constrained to whatever technology du jour was used to build the platform, and not the one that we want to use. We are already the minority in the tech org, and we would be introducing a completely foreign process.
        "" Neal Lathia from Gradient Labs 
                 
2. A typical piecewise MLOPs tool that helps you with some specific part of the ML Lifecycle (e.g., dvc/fds or Tensorboard for Experiment Tracking)

3. A open-core tool. We don't hold back anything from our open source tool, we provide everthing you need to build and improve your MLOps platform in a collaborative, safe and compliant way!

#### <mark>Our users are __NOT__:</mark>

1. Companies without ML expertise. 

Our __target users__ are hardcore ML teams (working on the software-side), which can be inside 3 groups of companies:

1. __Applied AI companies mature enough with ML__ (not building their first ML project) 
    1. AI-as-core companies
        1. Standard Products. They dont need to use your data for training. (e.g., *Tesla*, *Waymo*, *Cruise*, *Palantir*, *LogMeIn*, *Sensat*, *Wix*, *TravelPerk*, *Loopme*, *Clarifai*, *Vidado*, *SpotHero*, *Fermata Energy*, *Entrupy*, *ZenRows*, *biorender*, *meMR*, *Orbital Materials*, *Cradle*, *papercup*, *EyeGauge*, *Bytedance*, *OfferFit*, *unlearn*, *veed.io*, *gptzero*, *Hex*, *OtterTune*, *Hemato*, *HawqEye*, *DarkVision*, *Antiverse*, *Stanza Systems*, *tractable*, *Lynceus*, *Deepmeta*, *Clearcover*, *Cardo AI*, *Ironscales*, *seedtag*, *Vivid Machines*, *Anagenex*, *getindata*, *leap*, *pathai*, *bryte*, *luminar*, *Climate AI*, *Wordcab*, *hypothetic*, *iBusiness Funding*, *Layer*, *valohealth*, *capeanalytics*, *Blackbird.ai*, *Vorto*, *Abnormal*, *hyperfactors*, *Animal Dynamics*, *Portcast*, *pano*, *Recursion*, *recruitingfromscratch*, *Locus Robotics*, *BenchSci*, *Element Energy*, *LiveRamp*, *cambrian*, *datathings*, *orbem*, *stada*, *compilatio*, *REimagineHome*, *11x.ai*, *Trellis*, *Bioptimus*, *Diarupt*, *voiceflow*, *patented.ai*, *Lowe's Home Improvement*, *Inner AI*, *Canoe Intelligence*, *tausight*, *resolute*, *Grab*, *Intuition Machines*, *meMR health*, *taranawireless*, *odditylabs*, *Evidium*, *Marker Learning*, *Lyric*, *radai*, *Isomorphic Labs*, *Worbler.ai*, etc)
        2. Personalized Products: domain-specific platforms. They need to use your data for training. (e.g., *Automation Anywhere*, *Dynatrace*, *Shift Technology*, *Drift*, *Pluralsight*, *InMoment*, *Theloops*, *Conjura*, *Raptormaps*, *movableink*, *Klaviyo*, *Health Rhythms, *Veriff*, *datajoint*, *hyperstart*, *unboxai*, *Virgin Media O2*, *Genius Sports*, *kaksha.ai*, *movley*, *Performance Art*, *Renassaince*, *veriff*, *nemuru*, *Voxel*, *Cerrion*, *Algomo*, *Toumetis*, *Degreed*, *Intuit*, *Pandai*, *Peak.ai*, *OverJet*, *fasal*, *Salesloft*, *Recast*, *Serna Bio*, *pymetrics*, *Bayesian Health*, *Cordial*, *Best Egg*, *Impact.com*, *kraken*, *Icertis*, *rimes*, *syrup*, *Curieo*, *evolutioniq*, *ello*, *dost*, *M-KOPA*, *Yahoo News*, *Desia AI*, *Attentive*, *Clari*, etc)
    2. AI-as-helper companies (e.g., *Google*, *Microsoft*, *AWS*, *Doordash*, *Cisco*, *Meta*, *Tiktok*, *Snapshat*, *Pinterest*, *Discord*, *Match Group*, *Grindrr*, *Netflix*, *Salesforce*, *Microsoft*, *Apple*, *Spotify*, *Monzo Bank*, *Stichfix*, *Twilio*, *Adobe*, *BMW*, *Twitter*, *Dream11*, *Get your Guide*, *Slack*, *Wolfspeed*, *Duolingo*, *Nextdoor*, *Stack Overflow*, *Shopify*, *General Mills*, *BT Group*, *wolt*, *avant*, *LinkedIn*, *Financial Times*, *CNN*, *perpay*, *S&P Global*, *McDonalds*, *New York Life Insurance Co*, *Franklin Templeton Investments*, *Robinhood*, *Dell*, *Nubank*, *The New York Times*, *Autodesk*, *Bosch*, *Booking*, *Agoda*, *Volvo*, *Lucidmotors*, *syngenta*, *BioGen*, *Nomad*, *hokodo*, *Zoro*, *Simpplr*, *Urgently*, *Ballertv*, *sproutsocial*, *cookpad*, *swordhealth*, *Metrika*, *Heyjobs*, *stepstone*, *onemedical*, *Freeport Mcronan*, *therealreal*, *lemonade*, *karman*, *SImply Business*, *Sidekick*, *Corning*, *MeteoSwiss*, *iFood*, *Mercado Libre*, *crowdsec*, *360learning*, *wayfair*, *Digital Commerce Intelligence*, *mynd*, *coalition*, *Dish Network*, *Grainger*, *Workiva*, *Cyble*, *cargurus*, *weightwatchers*, *Walmart*, *StockX*, *taktile*, *DIscovery Bank*, *Lacework*, *CollegeVine*, *Navy Federal Credit Union*, *The Weather Channel*, *SailPoint*, *Chegg*, *perpay*, *kayzen*, *ING*, *Itau*, *CVS*, *DRP*, *genomicsengland*, *DrDoctor*, *homebound.*, *centene*, *artemis*, *Fundrise*, *Miro*, *Zalando*, *bees*, *GSK*, *Visa*, *Mastercard*, *DigitalOcean*, *Pie Insurance*, *Disney Streaming*, *King*, *Restaurant Brands International*, *hewlett packard enterprise (hpe)*, *smartcheats*, *iHelath*, *elucid*, *Amgen*, *fanatics*, *splunk*, *Analog Devices*, *stryker*, *mastercontrol*, *Renfroe*, *honeywell*, *iCapital*, *jobyaviation*, *Engage*, *Oho*, *highersearch*, *ID5*, *XCede*, *digitalwaffle*, *monolith*, *orbis*, *abcam*, *zebra*, *penske*, *jaguarlandrove*, *asos*, *Sixt*, *Quora*, *Belvo*, *Atlassian*, *carfax*, *trainline*, *Dream Sports*, *Prosimo*, *Home Depot*, *XP Investimentos*, *Nestlé*, *Hotmart*, *marktplaats*, *Conveyor*, *Moody's*, *AXA GO*, *Faire*, *thumbtack*, *Ford Motor Company*, *Workday*, *Wellcome Sanger Institute*, *TripAdvisor*, *Dell*, *VSCO*, *IAG Loyalty*, *Roche*, *Lodgify*, *Caterpillar*, *BrokerChooser*, *PicNic*, *Handshake*, *Wikimedia Foundation*, *IAGLoyalty*, *adyen*, *Equal Experts EU*, *MSD Pharmaceuticals*, *Chan Zuckerberg Initiative*, *appfolio*, *Airbus*, *Safra*, *Globo*, *Digio*, *FARFETCH*, *US Foods*, *Ipiranga*, etc).
2. __Managed ML Services__
    1. SaaS: AIaaS
        1. Models 
            1. Platforms (e.g., Twelve Labs)
            2. APIs 
                1. Broad (e.g., *OpenAI*, *Cohere*, *Anthropic*, *Groq*, *Cerebras*, etc)
                2. Narrow (*One Concern*, *Unitary*, *Graft*, *Symbl.ai*, *Jasper*, *Yobi*, Major clouds (*GCP AI*, *Azure AI*, *AWS AI*, *IBM AI*) etc)
        2. AutoML (e.g., *Akkio*, *Obviously AI*, *ModularMind*, *Tuba.ai*, etc)
    2. PaaS: MLOps platforms
        1. General (e.g., *Datarobot*, *Sagemaker*, *Paperspace Gradient*, *c3.ai*, *Cleanlab*, *attri*, *Datagran*, etc).
        2. Specific 
            1. Domain-specific (e.g., *Roboflow*, *Landing AI*, *Revela*, *Dialogflow*, *Wit.ai*, *MonkeyLearn*, *Rossum*, etc)
            2. Scope-specific (e.g., *Lakera*, *Portkey*, etc)
3. __AI Consultancy and Outsourcing companies__ (e.g., *Quantum Black*, *Whinder.ai*, *Boston Consulting Group*, *Accenture*, *Beam Data*, *Max Kelsen*, *Hop Labs*, *IBM*, *Flamelit*, *Neoway*, *bitstrapped*, *Gradient*, *Bain & Company*, *alexanderthamm*, *Beyond Limits*, *Criterion Systems*, *Booz Allen*, *JR*, *smartims*, *Capco*, *Peacock Technologies*, *Monstarlab*, *WeDo*, *Data Idols*, *Mantelgroup*, *Aptford*, *You Got Us*, *ManTech*, *aptford*, *ML6*, *Hop Labs*, *Hitachi Digital*, etc)

</details>

<details>
    <summary><b>Explaining Freelunch in a few steps</b></summary>

### __Building a MLOps platform with Freelunch can be summarized in a few steps:__

0. __Interactive Specification Builder (Powered by the MLOps Copilot):__ An application that interacts with the user with the goal of producing formal requirments fot he ML platform. The user gives as input an informal specification in natural language, then the Interactive Specification Builder returns a formal specification for it, if the specification is imcomplte because the user didnt provide enough data, the Specification Builder asks for more info to infer the complete specification. It aoutputs a formal specification that the user can edit if he wants.

1. __Interactive Design__: you, interactively, provide requirements to the Interactive Designer (constraints and preferences: (1) constraints: latency ceiling for some ML task, direct support for specific ml tasks and/or algos and/or data modalities and/or data volume and/or types of workloads, follow open standards/specifications, format of interfaces, dont change specific parts of your current mlops platform, use specific programming languages, template flavour, use/integrate_with/avoid specific tools/types of tools, filter tools by security level, scale, SLOs, compute & networking hardware specification, cluster multitenancy, cost ceiling, secure by default, specific places where backwards compatibility is needed; (2) preferences: cost range, capabilities, methods used, tools/types of tools used, similarity of tools to ones your team has expertise on, IO formats, patterns, sample projects, types of interfaces (GUI, CLI, Library, API), ease of use) and receive a well suited MLOPs architecture specification (a set of non-overlapping pattern distributions -- will be explained later) from Freelunch. 

    Note: Design constraints and preferences dont need to be provided in structured format directly, we provide am Design Formalizer (using LLM under the hood) that maps your preferences and constraints in plain english to structured format, making your life easier. It might not get it 100% right in the first go, but then you can edit the generated strctured format with a GUI to make final adjustments.

    __A MLOps platform specification can be constructued as a set of non-overlapping pattern distributions__ (e.g., 
        {
            base ci/cd with feature branch ci v3,
            type-safe python coding v2, 
            cost-aware compute provisioner v3, 
            streaming-based data collection v5, 
            data mesh v2, 
            virtual feature store v3,
            model/prompt registry v2,
            programmatic labelling v1,
            sql-based feature engineering v2, 
            lambda pattern v1, 
            lakehouse pattern v1, 
            synchronous data validation v2, 
            pipeline-centric feature/model experimentation environment v3, 
            logging-based feature/model experiment tracking v2, 
            pull-based metadata gathering v2, 
            active learning system v1,
            data-parallel distributed training for task H,
            edge-mobile deployments with federated learning for ML task A v1, 
            rl-based recommender system for task B v4, 
            one service per model for ML task B v2, 
            online learning for ML task B v1, 
            service than loads and runs pipeline of models for task C v2, 
            retraining for ML task C v3, 
            no-retrain (time modelled explictly) model for ML task D, 
            human-in-the-loop prediction review when model is not shure for task D v1, 
            LLM RAG with API store for task E v4, 
            Streaming and LLM API-based LLM service for task F v1, 
            Team of agents system for task G v2, 
            sql-based model workflow for task H, 
            Online learning for task I v4, 
            edge microcontroller deployment with cloud retraining for task J v3,
            hybrid fallback model deployment for task H v2,
            Internal LLM with small scale model experimentation v1, 
            LLM gateway with llm routing and caching v2, 
            prediction manager with support for inference config and pipelines v1, 
            support for GNNs for task G v1, 
            hyperparameter tuning post-training v2, 
            real-time feature store v5, 
            frontend with interactive model explainer v3, 
            gitops v4, 
            serverless for task B v3, 
            subplatform-based mlops (feature system, data collection system, deploy platform, discovery platform...) v2, 
            component-resposible data validation v5, 
            OAuth v1, 
            k8s operator pattern v2, 
            Infrastrcture-from-Code (IfC) v3, 
            config-based (declarative) model training v1, 
            differentially-private model experimentation v4, 
            container-based v2, 
            serverless ML pipeline v1, 
            RPC for internal HTTP APIs v1, 
            CPU for training and GPU for inference v5, 
            always-up offline ephemeral workloads cluster v1, 
            static analysis, notebook conversion and artifact substitution in pre-commit hooks v1,
            ...
        }
    )
     
        Each pattern has metadata such as intent, motivation, solution, tradeoffs and alternatives, and can have multiple pattern distributions (a pattern distribution assigns a method to a pattern, which makes it more specific), each one being an architecture with standard component distributions (which can be functions, container images, plugins, drivers, etc). Each component can have multiple distributions which can differ by: (1) their interface formats/protocols, (2) being configurable (flexible) or hardcoded (faster), (3) language implementation, (4) scalability. Also, a component distribution can be used by multiple pattern distributions of the same or different patterns. __A building unit is an implementation of a component distribution. A building block is an implementation of a pattern distribition == set of non-overlapping component distributions.__
    
    
    __The output is a MLOps platform specification that fully specifies the MLOps platform in terms of available pattern distributions__ (from backup, infrastructure provisioning, ML System, CI/CD and Observability) Anatomy of a Freelunch-built MLOps platform is discussed later. 
    
    __When its time for implementation (next step), configurable building blocks (which contain: (1) smart defaults, (2) properties, (3) tests and (4) benchmarking results (operational: latency, throughput, memory and storage footprints; security vulnerabilities; scalability; tool-related: maturity, active community, ease of install, ease of use, similarity with existing tools, seamless integrations, pre-requisite knowledge (of tools & theory))) are matched with the MLOps platform Design and User constraints & Preferences__.

    __Think of Freelunch building block as a standardized Juju Charm__. It is a infrastrcture-agnostic package that implementes one or more component specifications.

    __Types of implementations:__

    1. __Freelunch implementations (building units and building blocks)__ (these follow freelunch's standard. Ideally, all implementations should be like this, but in practice, teams have to transition smoothly to freelunch and freelunch wont support all necessary implementations out-of-the-gate.)

        1. __Open source:__ the building block is open source, available in the *Freelunch Hub* and deployed in your compute infrastructure with no additional cost.
        2. __Proprietary:__ the building block is proprietary, only its signature and tests are available in the *Freelunch Hub*, its deployment is hidden from you, you only interact with its APIs directly or indirectly (via UI) and there is a additional cost for using it. Note: if it contains a GUI, the GUI must be implemented as a *Freelunch Plugin* (which has 2 modules: a payment module and a usage module), so that Freelunch user can use it diretly from *Freelunch* without having to go to the comapanies' website. It is pretty easy to transfer a standard web frontend to a *Freelunch plugin*.
    
    2. __External implementations__ (any implementation that doesnt follow the Freelunch building block standard. The disavantages of these is that they werent necessarily carefully designed to fit into the Freelunch ecosystem of patterns, dont have built in evaluation metrics and cant be automatically identified by the platform evaluator.)
    
    __Pattern variations:__ are new patterns that just slight modifications of existing patterns, therefore they lieve within the main pattern, as variations that can be applied to it. A specific type of pattern variation worth mentioning is a pattern plugin: incorporation of of a capability.
    
    __Evolutions:__ Evolutions are a sequence of maturity-evolving patterns (more automation, capabilities, best practices, quality and performance at the expense of more cost and expertise) or platform specifications (set of non-overlapping patterns) for an organization to use in their MLOps journey, which allows them to keep easily extending their current platform for better MLOps and only introduce more complexity when really needed. A pattern or platform can have multiple evolution branches. An evolution typically: reduces manual work, reduces failures, increases compute costs, incoporates complexity via more abstractions and tooling, substitutes managed tools for open source ones and imposes more standards. __Example of how evolutions can look like: serverless with no ML at all (using heuristics) --> serverless just with traditional ML and low traditional MLOPs maturity --> mixed oss and proprietary software just with traditional ML and medium traditional MLOPs maturity on standard IaaS --> majority oss just with traditional ML and high traditional MLOPs maturity on high-performance IaaS --> majority oss incorporating generative ML with high traditional MLOPs maturity and medium LLMOps maturity on high-performance IaaS --> majority oss incorporating generative ML with high traditional MLOPs maturity and high LLMOps maturity on high-performance IaaS__

    __Golden patterns:__ sometimes, we/you want a pattern, but current tools are not suited for making it work elegantly, therefore pattern variations are made to this original pattern that aim to relax a bit of the constraints and adapt more to the existing toolset. However, call for new tools/tool updates for the original pattern continues. We call the original main pattern the golden pattern (more fun name).

    __Out-of-the-box MLOPs platforms:__ these are ready and implemented MLOPs platforms (architectures) that you cant just use. They are a set of implemented non-overlapping pattern distributions for specific popular use-cases that should get you started very quickly.

    But how does Freelunch choose a set of on-overlapping patterns distributions that satisfy your constraints and preferences? It uses 3 things: 
    
    1. Interactive design storage: that stores Out-of-the-box MLOPs platforms, patterns distributions, building blocks, constraints and heuristics to combine pattern distributions.
    2. Engineer Feedback: to modify the proposed solution. 
    3. AI: specifically an LLM specialized in MLOPs architecture design.

2. __Automatic Implementation__: Freelunch does design verification and implements a the specified MLOps platform, using building blocks. The platform follows one of Freelunch's codebase strcture flavours (e.g., all flavours decouple interface from implementation, but they can differ by folder strcture pattern). Freelunch uses the best tool stack with smart default config for the job (by following your tool constraints, matching the design specification (which is tool-agnostic) with building blocks (which are tool-dependent) and giving prefernce to building blocks with better benchmarking results). You can deploy and run the tests in a integration testing cluster. The MLOps platform deployable is a 2-step meta pipeline: (1) step 1 provisions ML pipeline infrastrcture and (2) step 2 deploys ML pipeline to that infrastrcture (Note: ML Pipeline == deployment of initial ML System + Generalized CI/CD to improve the ML System).

It also spits a Initial Platform manifest. Which is a file that specifies the platform (architecture with pattern distributions used, building blocks used with placeholders for custom private stuff) makes it possible for others to build the exact same initial MLOps platform using this file, without having to go trough interactive design and long-running automatic implementation.

3. __Manual Low-level Customization:__ 

    1. __Changing Building Blocks__

        1. __Changing configuration__

        You manually edit configurations of building blocks according to your necessities that were not covered by the first step.

        2. __Changing code__

        There are some things you might want to implement and our not supported out-of-the box by Freelunch. In this case, you change the building block's code directly, change it's metadata, benchmark it using the Building Block Benchmarker we provide and publish the new building block to the hub.
    
    2. __Changing or Adding Components__

    This type of change is more radical and is done when you need a change in the architecture itself, so you modify or add component specifications. You then publish to the hub your new component specification.

4. __Sandbox: Offline Evaluation, Observation and Security Testing__: you evaluate the ML System and MLOps platform (according to your use-cases) in a background cluster (provisioned on-the-fly or always available) where an evaluation suite (evaluations can be: (1) requirements: integration tests, backwards-compatibility, compliance, privacy, security, deployment pattern, SLOs; (2) benchmarks: predictive power loss (loss of predictive power wrt offline model), performance profiling, observability, replayability, scalability, model robustness (natural and adversarial), system robustness (fault tolerance and adaptability), cloud/premise cost, data-efficiency, CI/CD speed, fairness, explainability, uncertainty estimation, technical debt, application-specific benchamarks (e.g., degenerate feedback loop handling and causal/intervention analysis); and evaluations can be: (1) automated: (1.1); Not AI-powered (1.2) AI-powered; (2) Manual) is run against your platform. 

    You can observe the evaluations being done via a GUI to get better undertanding of bugs and improvement opportunities. You can also observe platforms under data collection and user traffic: (1) your current platform that is in production (under real-time traffic, replay it under historic production traffic); or (2) a platform experiment (under production or synthetic traffic). Observation can be done externally (with restricted permisssions) in the case of external audits. You also do security (fuzz, insider and penetration) testing on the MLOps platform to try to find complex integration-level vulnerabilities that can be surfaced or exploited.

    This is acomplished by mimicking the core production clusters + emulating: (1) client machines and networks; (2) data sources machines and networks; (3) Geographically separated Clusters and CDNs; and general events from external services (e.g. repo events or S3 changes) + performing an evaluation suite on top of it. All of this can be done inside a real physical cluster or a virtual cluster (e.g. kubernetes in docker (kind), minikube, k3d).

5. __Platform-level Experimentation__: you edit design and generate a new build or modify the codebase directly (following the codebase standard. Note: the codebase standard makes it easy to switch component implementations or even plug-in your own custom ones). Note: experiments are tracked by the experiment tracker. The experiment tracker which is hooked up to your remote repo (github, gitlab or gitea) and each evaluated commit is an experiment (basically a UI for git/dvc with built-in Freelunch format support). The experiment tracker is actually Freelunch Hub itself (will be explained in the following sections), you can have either a private or public project there. Built-in support for PoCs (with Wizard-of-Oz support), these are experiments that are not meant to go to production, they are just meant to prove that more efforts such be put in certain direction. If an experiment turns out to be bad, you just restore to the last good one or go back to a specific one, by using the Experiment Tracker's GUI.

6. __*Goto 4. until satisfied*__: you repeat the *edit/build/evaluate and observe* cycle until you confident the new MLOps platform can be pushed to production.

7. __Push to master__: pull master branch, resolve changes and push to master

8. __DevOps CI/CD to your favourite infrastructure stack__: (1) your devops ci/cd spins up your meta pipeline integration testing logical cluster and production logical cluster. Integration testing logical cluster is: cloud cluster provider/on-premise machine cluster + meta pipeline integration tester. Production logical cluster is <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator> + meta pipeline artifact store + meta pipeline observer (with a lineage store); (2) Tests the meta pipeline in your integration testing logical cluster; (3) Deploys the meta pipeline to your production logical cluster.

9. __Sandbox: Online Evaluation, Observation, Security Testing and Onboarding__: same as *4.* but, now that there is already a platform deployed. Onboarding is added now because it is important to help new engineers catch-up to how the platform works.

10. __Pull master and experiment__: pull master, check experiments done as a git-like tree via Experiment Tracker's GUI and decide the next experiments to do. The make changes to the MLOps platform to try these experiment ideas. The changes can be done via interactive design with Freelunch and/or you can make the changes to the repo all by yourself. If an experiment turns out to be bad, you just restore to the last good one or go back to a specific one, by using the Experiment Tracker's GUI.

11. __Repeat the experimentation cycle__: *Go to 4.*

12. __Creation your own building units and building blocks__

Freelunch provides a compiler that lets you build implementations: (1) building units (e.g., k8s pod) from code, tests and metadata; and (2) building blocks from building units, tests and metadata and config. You can create private or public implementations and upload them to Freelunch Hub.

13. __Repeat the dev cycle__: *Go to 1.* 

</details>

<details>
    <summary><b>Freelunch vs current best open source options</b></summary>

### __*Freelunch* vs current best open source options__

*CLAIMED*, *Kubeflow Pipelines*, *MLFlow*, *Metaflow*, *MLeap*, *Flyte*, *Sematic*, *Kedro*, *ZenML*, *ClearML*, *Polyaxon*, *SystemDS*, *SuperDuperDB*

* __Similarities:__
    1. Pipeline-centric: *Freelunch*, *CLAIMED*, *Kubeflow Pipelines*, *Metaflow*, *Kedro*, *ZenML*, *ClearML* and *Polyaxon* aim to build a ML pipeline.
    2. Tool-agnostic: *Freelunch*, *CLAIMED* and *ZenML* make it easy for you to use multiple best-in-class MLOps tools as building blocks. 
    3. Best practices: *Freelunch* and *Kedro* provide project structure with embedded software engineering best practices. Actually, Freelunch's low-level layer (MLOps codebase that can be holistically evaluated and deployed in multiple ways) is similar to Kedro (and Hamilton btw) but more fine-grained (also contains structure for internals of the ML System) and with more capabilities (e.g., evaluation).
    4. Decoupled infrastructure: all, excpept *SuperDuperDB*, decouple deployables from infrastructure.
    5. Automatic Container builds: all can abstract away from you the process of building containers

* __Differences:__
    1. *MLFlow*, *ClearML*, *Polyaxon*, *SystemDS* and *SuperDuperDB* are not tool-agnostic, whereas *Freelunch* and *ZenML* are, enabling fast adaptation to best in class tool at the moment and progressive maturity.
    2. Lack of fundamental things that *Freelunch*, *Kubeflow Pipelines*, *Metaflow*, *MLeap*, *Flyte*, *Sematic*, *Kedro*, *ZenML* and *ClearML* have.
        1. Lack of support for pipelines: *MLFlow* and *SuperDuperDB* dont provide good support for building pipelines
        2. Lack of support for pipelines and deployment: *Polyaxon* doesnt provide any support for these things.
    3. Lack software engineering best practices: *Kubeflow Pipelines*, *MLFlow*, Metaflow*, *MLeap*, *Flyte*, *Sematic*, *ClearML*, *Polyaxon*, *ZenML*, *CLAIMED* and *SuperDuperDB* lack things like testing structure, data storage abstraction and separate workspaces; whereas *Freelunch* and *Kedro* have these things.
    4. Dont provide support for ML infrastructure: *Kedro* is just a tool-agnostic pipeline specification tool (which need a workflow orchestrator to execute it). *Kubeflow Pipelines*, *Metaflow*, *MLeap*, *Flyte* and *Sematic* are just fancy workflow orchestrators; whereas *Freelunch*, *MLFlow*, *ClearML* and *Polyaxon* offer.
    5. No intuitive low-level hacking: *MLFlow*, *MLeap*, *ZenML*, *Polyaxon* and *SuperDuperDB* dont provide this, whereas the others do.
    6. Lack of important capabilities. *All* lack these *Freelunch* capabilities:
        1. Production-grade capabilities such as: project management with RBAC, collaboration, compliance management, security testing, operations, golden path support, progressive maturity support.
        2. Platform-level experiment tracking (tracking new platform experiments)
        3. Declarative and interactive high-level API for designing MLOps platforms (Disclaimer: *SystemDS* provides a declarative approach for building ML Systems)
        4. MLOps platform and ML System Evaluator
        5. Cost tracking and optimization
        6. Built-in support for building <ci/cd-based, cost-aware, replayable, FM/LLM/Agent-friendly, edge-friendly, secure, compliant, AutoML-equipped, PoC-equipped, helpdesk-equipped, ML-Business KPI aligned, activity logged, privacy-preserving, lineage-complete, UI-customizable> MLOps platforms

<details>
<summary><b>Some root problems of open source all-in-ones</b></summary>

#### __Some root problems:__

1. Many of these (*MLFLow*, *Metaflow*, *ZenML*, *ClearML* and *Polyaxon*) have paid managed enterprise versions, they keep the open source version without important production capabilities on purpose (e.g., IAM, Security, Collaboration, etc).

</details>

</details>

<details>
    <summary><b>Paid plans</b></summary>

### Paid plans:

All plans enjoy SLAs.

1. __SaaS. Specifically AI as a service (AIaaS): *MLOps Copilot* (also available as self-hosted)*. Comes with Priority Customer Support (which can be extended to Dedicated 24/7 Support)__<sup>1</sup>.

Imagine Cognition Labs' Devin, but specifically for MLOps. You can superchage Freelunch with our MLOps copilot (Virtual ML Engineer)  that plays the role of multiple ML Engineers, with half the cost of a single engineer. The MLOps copilot (Virtual ML Engineer)  can help you both in building the MLOps platform and operating the MLOps platform. Helps you along all MLOps phases such as: project management, learning, design, coding/configuring, dataset construction, offline and online experimentation, ci/cd, prediction client code (frontend or embedded device), operations, security testing, ROI analysis.

It has access to your comapany's internal tools (those explictly made available to it) and interacts with your team via slack: telling you what he has done, asking for approvals, reviews and asking questions. You can teach it directly from slack also: every message you send to it with #teaching tag will be stored as training data as used to fine-tune it.

It is continiuouly fine-tuned to your company's MLOps workflows using telemetry data from the ML platform (state of the ML System, telemetry data of the ML System, actions of practitioners). So it keeps getting better and better using what is commonly known as a "data flywheel".

_Note:_ in the beggining, the MLOps copilot (Virtual ML Engineer) might not be very good (like a data scientist starting his first MLOps job), however we can bootstrap with free plans and improve it by leveraging online user data (engineers using the MLOps Copilot) as supervised data. An then keep improving with online data forever, making our own data flywheel that creates a clear unfair advantage over other players.

2. __PaaS. Freelunch is managed for you__ Managed Freelunch + Success Manager + Dedicated 24/7 Customer Support__

3. __SaaS + PaaS. Freelunch is managed for you + *MLOps Copilot*:__ Managed Freelunch + Success Manager + Dedicated 24/7 Customer Support + *MLOps Copilot*

<details>
    <summary><i>Footnotes</i></summary>

#### _Footnotes_

1. __*MLOps Copilot*__: how to build it? Besides using LLM Agents and AutoML techniques, I think more things are needed. One idea I have is the *AgentPool Project* for which I wrote a specific document. This document is attached at the end of this file. In a nutshell: making the best dataset possible for an LLM to learn technical stuff.

For critical ops activities it can be used in shadow mode, where it generates actions it usually generates, but the actions are not atcully performed, just shown to you in slack with the #shadow tag. We you fell confident with it, you can remove shadow mode and let it actually perform actions.

Note: should be ebnchamrked against *SWE-Bench*, which is a benchamark about solving real world github issues (with and without assistance).

</details>

</details>

<details>
    <summary><b>Business model</b></summary>

#### :moneybag: __Business model__

__We provide a B2B open source PaaS tool; and we charge for pay/usage SaaS services highly integrated with the open source tool.__ We charge for:

1. __AI-assisted MLOps__
2. __Dedicated Support Channel__

__Target buyer: we will be selling to MLOps leads/managers or MLOps Platform leads/managers.__

__Simple reasoning to show we can be a unicorn:__

We estimate that we can make 150 million dollars/year in profit in the medium term. 

Revenue/year := # of potential customers (ML teams) * potential market share * how much we could charge/customer/year. 

150M dollars/year := 15k * 0.5 * 20k dollars/year

Why 15k? We assume ML teams composed of 10 ML Engineers and used [this baseline](https://thenewstack.io/tech-works-how-to-fill-the-27-million-ai-engineer-gap/)

Why 20k dollars/year? We assumed 1/5 of human ML Engineer salary and used [this baseline](https://www.iu.org/blog/salaries/data-scientist-salary-expectations/#:~:text=An%20individual%20with%20a%20Bachelor's,%E2%82%AC69%2C500%20per%20year%5B3%5D)

</details>

<details>
    <summary><b>Why now?</b></summary>

### :date: __Why now?__

1. __Problems__
    1. __Complexity__. ML Systems are getting too complex, especially with big data/models. This make it hard for humans to do risk assesment & find improvement opportunities, systematically & efficently.
    2. __High Costs__. Maintaining these systems is very costly. With the wrong practices, companies can end up sinking in cloud/premise costs.
    3. __Regulations__. Data & AI Laws, that enforce certain properties of ML Systems, started popping (e.g., GDPR & Explanability for Medical/Finance).
    4. __High Bar__. Users have low tolerance for errors and want fast systems.
    5. __Fierce Competition__. ML is widespread across companies & better systems can be the competitive advantage that lets one company win over others.
2. __Solution Enablers__
    1. __Research & Tools__. Reserch & Tools in ML are exploding, thus it is hard to keep up with the fast-paced landscape.
    2. __Holisitic Problem Awareness__. Awareness is high of all problems that can happen with ML in Production (e.g., bias). Focus shifted from building ML Models to building ML Systems.
    3. __Tech__
        1. __ML__. FM/LLM Paradigm. LLMs can do: interactive code (actual code & test code) debugging, generation and explanation. You can build LLM agents that are able to use software tools, make & execute plans and collaborate with other agents; like humans do.
        2. __Formal Verificaton__. Recent Interactive Theorem Provers such as _Lean Theorem Prover_ show that we can debug & prove properties of programs, given formal specifications of these properties.
        3. __Kubernetes__. Enable application-infrastrcture decoupling.
    4. __Data:__ tons of freely-licenced publicly available repos. More specifcally, "The Stack" (Kocetkovet al., 2022), a large collection of permissively licensed github repositories with inspection tools and an opt-out process. (e.g., _github/gitlab/gitea (self-hosted) Copilot_ was trained on public code).
    5. __More AI-core Companies__. Companies, in which AI is at the core of their value proposition, are popping fast. These are the companies that may die if their ML Systems arent good enough.
</details>

<details>
    <summary><b>Freelunch and the MLOps world</b></summary>

### Freelunch and the MLOps world

__The following process happens continuously:__

1. __By Freelunch team and contributors__

    1. __Use-case Discovery.__ Discover new MLOps use cases, and give preference to the most popular ones.
    2. __Platform Specification.__ Come up with new MLOps platform specifications (set of component specifications that work together) for the most popular use-cases.
    3. __Pattern Formalization.__ Come up with new patterns. A Pattern should be an umbrella for similar specifications across use-cases
    4. __Pattern Distribution Formalization.__ Allow pattern variations under different pattern distributions.
    5. __Pattern Distribution Integration__ Freelunch needs to support the new Pattern Distribution Integration in its hub, format, evaluation suite and native tools.
    6. __Call for implementations.__ Publish the pattern distribution, so that piecewise implementations can cover the need.
        1. __1-off implementations:__ a new version of a specific pattern distribution is made from existing tools and custom code.
        2. __Tools (preferable):__ supports multiple pattern distribution versions, multiple pattern distributions (maybe an entire pattern) or even multiple patterns. Either (1) existing tools change to incorporate the standard; (2) wrappers around existing tools are made to adapt them to the standard; (3) forks of existing tools are made to change the tools to conform to the standard; or (4) new tools are made that natively conform to the standard. In this way, we can improve way faster than if we were to make the tools ourselves
    7. __Implementation integration__ Freelunch integrates with the new pattern distribution implementation (set of building blocks). This happens by hosting building blocks in the Freelunch Hub under the pattern distribution.

2. __By Freelunch users__

    1. __Custom MLOps Platform construction__ 
        1. __Hot-start:__ teams construct a custom MLOps platform using: (1) the interactive designer and (2) the platform builder.
        2. __Fine-grained customization__ 
            1. __Plugin inclusion__ teams customize the platform by making their own custom component specifications and building blocks.
            2. __Repo-level editing__ teams modify the repo in any way they want, but they must conform to one of the standard codebase structure specifications, or they must create their own custom codebase strcuture specification (So that the evaluation suite can understand the codebase).
    2. __Custom MLOps Platform and ML System continuous improvement__
        1. __Problem pinning__
            1. __Monitoring__ (detect that something is wrong)
            2. __Observability__ (detect where its wrong and how to fix/improve it)
        2. __Making changes__
            1. __Corrections: Monitoring actions__
                1. __Automated actions: artifact CI/CD__ (e.g., data, config (service and task config), model chains, fallback models, inteliggent appeal policies, ensembles, pre/post processors, retrievers, etc)
                2. __Manual actions: generalized CI/CD__
            2. __Improvements__
                1. __Artifact-level__
                    1. __Experimentation__
                    2. __Evaluation__
                    3. __Artifact CI/CD__ (e.g., data, config (service and task config), model chains, fallback models, inteliggent appeal policies, ensembles, pre/post processors, retrievers, etc)
                2. __Platform-level__
                    1. __Experimentation__ 
                    2. __Evaluation__
                    3. __Code CI/CD__
    3. __Component Specification Requests__ teams request for the inclusion of component specifications that are not currently suported, that they are using via Fine-grained customization.
    4. __Native inclusion of requested Component Specifications__ teams refactor to include the specification and building block natively, which is much more clean and than the previous manual inlcusion via Fine-grained customization.

__*Ideally, we would sponsor the projects behind new tools that are created in a similar fashion to what the Cloud Native Computing Foundation (CNCF) does; and we should work closely with: AI Infrastructure Aliance (AIIA) and Full Stack Deep Learning (FSDL)*__
</details>

<details>
    <summary><b>Freelunch Hub: the place to track MLOps platform experiments, document, share, discover and play with MLOps platforms</b></summary>

### __Freelunch Hub: the place to track MLOps platform experiments, document, share, discover and play with MLOps platforms__ 

__*What HuggingFace Hub is for ML models we are for ML platforms*.__

Uploading your platform to the hub takes a single click/command/line of code. 

__Freelunch Hub is where you store, version, visualize, annotate, explore, manage, document and share your MLOps platform experiments. Experiments are versioned in git-like tree. There are 3 tiers of MLOps platform experiments (from least to most intrusive):__

1. __Building block tier:__ architecture isnt changed, just the implementation modules. This can happen by changing config, tool or code.

2. __Component tier:__ architecture is changed a bit, component specification are introduced, removed or changed. This happens via modifying the current pattern distribution by changing the codebase. The new pattern distribution is a new version of the previous one.

3. __Platform tier:__ architecture is changed a lot, component specification are introduced, removed or changed. This happens via modifying the current pattern distribution by changing the codebase. The new pattern distribution is not a new version of the previous one, it gets a new name/id and starts its own version history.

__Documentation can be of 2 types:__

1. __Internal (for the ml team):__ internal playbooks, guidelines, knowledge-base, design docs, architectural decision records (ADRs), coupling description/dataflows, etc;
    1. __MLOps Platform docs__
    2. __ML System docs__
    
2. __External (for other teams within the company)__ how to interact with the ml system, expected behaviour of the API/UI (model biases, model failures modes, latency, etc). 
    
In addition to standard documenting formats (e.g., markdown), we support notion-like notebooks and latex editing directly in the Hub. 

__When a platform (implemented set of non-overlapping pattern distributions) is in Freelunch Hub, people can:__

1. Read about it and comment
2. Visualize its evaluation scores
3. Play with it and observer it working directly in the browser (killercoda-style) via Freelunch Hub GUI Playground (but not on the owners' infrastructure, we hide it from the public an substitute it for a equivalent Freelunch managed one)
4. Download it or artifacts associated with it (e.g., (1) Freelunch artifacts: pattern distribution metadata, pattern evolutions, components specifications, building blocks, etc; (2) MLOps platform artifacts: data, models (ML and/or manual models), pre/post processors (e.g., input filters to detect adversarial and ood inputs or to detect sensitive data (in case an external model API is used), and output filters to detect innapropriate and risky outputs)
5. Find its repo and make PRs (if it has a public repo that accepts PRs)
6. Participate in a competition to improve the MLOps platform (if the platform is registered in the competition)
    1. Uncover and fix security vulnearabilities.
    2. Make it more efficient.
    3. Add capabilities.

__Platform organization and search:__

1. __*Pattern Distributions*__ are organized and can be searched by:

    1. __Constraints__ (e.g., use/integrate_with/avoid/security level of specific tools/types of tools, follow open standards specifications, hardware requirements)
    2. __Most Popular Sectors that used it__
    3. __MLOps platforms that use it__
    4. __Pattern it belongs to__
    5. __MLOps Maturity level of it__
    6. __Popularity of its implementations (Building Blocks)__
    7. __Evaluation Metrics of its implementations (Building Blocks)__

2. __*MLOps platforms*__ are organized and can be searched by:
    
    1. __Constraints__ (e.g., direct support for specific ml tasks and/or data modalities and/or types of workloads, follow open standards/specifications, format of interfaces, dont change specific parts of your current mlops platform, template flavour, use/integrate_with/avoid specific tools/types of/security level of tools, hardware requirements, cluster multitenancy, cost ceiling, specific places where backwards compatibility is needed)

    2. __Sector (Required)__
        1. MLOps platforms as a service
        2. Applied
            1. Healthcare
            2. Science
                1. Physics
                2. Chemistry
                3. Biology
            3. Finance
                1. Banking
                2. Investing
                3. Betting
                4. Insurance
            4. Autonomous Robotics
                1. Autonomous Vehicles
                2. Autonomous Humanoids
            5. Engineering
                1. Marine
                2. Aeronautics & Aerospace
                3. Computing Hardware
                4. Telecommunications
                5. Energy
                6. Cybersecurity
                7. Civil Construction
                8. Brain-Computer Interfaces
            6. Operations
                1. Manufacturing & Industry 4.0
                2. Logistics
                3. Supply Chain & Inventory Management
                4. Business Management
            7. Education 
            8. Entertainment
                1. Home Assistants
                2. Videogames & Animations
                3. Social Media
                4. Music
                5. Sports
                6. Video Streaming
                7. Tourism
            9. Weather and Natural Disaster Forecasting
            10. Agriculture
            11. Retail
                1. Digital
                    1. E-commerce
                    2. Marketplace
                2. Phsyical
                    1. Stores
                    2. Restaurants/Bars
            12. Security
                1. Surveillence
                2. Military Defense
            13. Jornalism
            14. Talent Recruiting
            15. Humanitarian Aid

    3. __By Native Task Support (Optional)__
        1. Simulators & Control
        2. Chatbots
        3. Recommeder Systems
        4. Outlier/Anomaly Detection
        5. Signal/Time-Series Forecasting
        6. Regression and Classification
        7. Causal Inference and Interventions
        8. Search and Optimization
        9. CV-specific Tasks
            1. 3D Reconstruction
            2. Optical Character Recognition (OCR)
            3. Object Detection and Semantic Segmentation
        10. Dynamic pricing
        11. Semantic Search
        12. Matching
        13. Summarization
        14. Translation
            1. Text-to-speech
            2. Speech-to-text
        15. Generation
            1. Image
                1. Super-resolution
                2. Editing
            2. Video
                1. Super-resolution
                2. Editing
            3. Audio
            4. Text
        16. Survival Analysis and Predictive Maintenance
        17. Robotic Process Automation (RPA)
        18. Solving Differential Equations

    4. __By Data Modality (Required)__
        1. Tabular
        2. Graphs
        3. Text
        4. Audio
        5. Computer Vision
            1. Image
            2. Video without Audio
            3. Video
        6. Signals/Time-series
        7. Spatial-temporal
            1. 2D-time (e.g., geospatial)
            3. 3D-time (e.g., point clouds or molecules) 

    5. __By MLOps platform Evaluation Metrics__
        1. __Requirements__
            1. __Constrained environments__ (using Environment mimicking tools (e.g., Selenium or Cypress for browsers, EdgeCloudSim, EnvisEdge or Auptimizer for edge devices) to simulate hardware (computing & networking), OS (if any) & more constraints (if any; e.g., browser constraints). Uses cases: deployments on: mobile, embedded devices (microcontrollers with OS, microcontrollers without OS, FPGAs), browsers, wearables (e.g., glasses, watches) and other devices (e.g., TVs, interactive panels, etc)) (Tools: MLPerf Tiny)
                1. __Inference__ (memory usage, power consumption, inference time) (contraints: discontonuos function, only need to be below treshold (memory usage in single-task microcontrollers (no OS)); goals: continuous function, need to optimize (e.g., power usage after treshold is reached)) (small models: Microcontrollers (memory, power, processing speed); medium models: Mobile (memory, processing speed); big models: any cloud instance (memory) (on a fixed high-performant instance: assuming same <machine, OS, processor used, cgroup>)). (_Note:_ has to do various runs and compute <mean, std deviation> because of stochasticity of hardware implementation)
                2. __Training__ (memory usage, power consumption, training time) (on a fixed high-performant instance: assuming same <machine, OS, processor used, cgroup>) (_Note:_ has to do various runs and compute <mean, std deviation> because of stochasticity of hardware implementation)
            2. __Security__
                1. __ML-agnostic: Dynamic Application Security Testing (DAST)__ (test your ML System for known cybersecurity attacks with the help of our AI-powered white-hat hacker, generate reports & alerts/notifications)
                    1. __By Atacker Knowledge-level__
                        1. __Black/Grey Box attacks__ (see if deployed system is secure to external attacks. Automated solutions are often called Breach & Attack Simulation (BAS) tools.)
                        2. __White-box Attacks__ (see if deployed system is secure (or responds well) to internal attacks.)
                    2. __By Attack Type__
                        1. __Privacy__
                2. __ML-dependent__
                    1. __General Analysis__
                        1. __Influence Surface of Models__
                        2. __Innapropriate outputs (Txocicity, Privacy, Intellectual property)__
                        3. __Misuse of Models__
                    2. __Attack types__
                        1. __IP (Model Stealing) Attacks__
                        2. __Data Reconstruction Attacks (We want differentially private pipelines and models)__
                        3. __Adversarial Attacks (Includes Prompt Injection)__
                            1. __Integraty attacks__
                                1. __Black-box__
                                2. __Grey-box__
                                3. __White-box__
                            2. __Privacy attacks__
                                1. __Black-box__
                                2. __Grey-box__
                                3. __White-box__
                            3. __Availability attacks (for agents)__
                        4. __Dependency (Data and/or Model) Attacks__
                    3. __Formal Verification of Security Properties__
                        1. __Microservices interact with each other in a precise manner__
                        2. __Models that are provably secure against certain kinds of attacks__
            3. __Compliance__
                1. __Functional Compliance__ (test for functional compliance (e.g., data privacy: role-based access control, GDPR, sensitive data handling; or cybersecurity: encryted data at rest, in transit & maybe even in-memory, and defense & remdiation automated protocols), manage regulations, certificates and agreements, configure & schedule report generation; manage & share reports)
                2. __IP/Licence Compliance__
                    1. __Dangerous Code Copy/Paste__ (Our VSCode extension works together with our Chrome Extension to detect risky code copy/paste. If risky code is detected, *Freelunch* generates alternative free-to-use code that does the same thing as the original copyleft code. This way, Engineers can be free to do what they like most: copy/paste code :joy:.)
                    2. __IP/Licence Scanning__ Explicit declaration of third-party dependencies & their licences (a simple file where you organize from where you got your third-party code, data & models that we use to verify if you are adhering to their licences. We help you build this file: we scan your repo in an attempt to find dependencies you missed mentioning there or detect licences that got updated; automatically generate a PR with updated dependency declaration file)
                    3. __Compliance support 4 AI generated code__ is checking with the training code data to see any matches, if there is very similar code (of singificant complexity), the licence of that code is linked to it; also: you can ask for the AI Assistant to generate free-to-use code that does the same thing as the original copyleft code.
            4. __Non-crucial unit and integration tests__ (because when they are crucial, they need to be inside CI/CD)
                1. __Traditional tests__
                    1. __Backwards-compatibility tests__
                2. __ML-related tests__
                    1. __Training-serving skew__
                    2. __Model-Service-System same inference results sanity check__
                    3. __Reproducibility__
            5. __Backwards-compatibility__

        2. __Benchmarks__ (soft metrics, which you want to optimize for)
            1. __Model-related__
                1. Predictive power loss (loss of predictive power wrt offline model)
                2. Explainability/Interpretability
                3. Uncertainty Estimation (includes OOD Detection)
                4. Intervention Analysis (using Causal Inference)
                5. Performance: model profiling under some deployment context
                6. Fairness
                7. Model scalability (scaling laws)
            2. __Subsystem-related__
                1. __Inference Pipelines__
                    1. __Using RAG__
                        1. Embedding Power (how well does the embedding distance reflect the actual semantic distance between documents)
                        2. Document Relevance (how usefull are the documents for the tasks that need to be done)
                        3. Retrieval Power (how well the retriever gets the adequate set of documents for a user input)
                    2. __Pipeline of models and pre/post processors__
                        1. Error Recovery (how well can revoer from errors in each step)
            3. __System-related__
                1. Only for RAG:
                2. Degenerate Feedback Loop Handling
                3. Replayability
                4. CI/CD speed & efficiency
                5. Cost (Total Cost and Cost Profiling)
                6. Scale and Scalability
                7. Performance: profiling of services, pipeliens and the whole system
                8. Wise training data selection
                9. Robustness
                10. Technical Debt/Maintanability (e.g., based on Google's ml test score or on comments with technical debt hints)
                11. Carbon Footprint
                12. User as Data Owner
                    1. Data Deletion
                    2. Data Compensatation
                13. Evaluation Coverage (problems identified in read teaming that are not covered in current evaluations)

    6. __By Components or Building Blocks__

    7. __By Tech__
        1. AR/VR
        2. Web3

__Security guardrails__: Freelunch automatically identifies security vulnearabilities (e.g., leaked secrets and confidential data) in your project repo, alerts you and does not allow it to go public.
</details>

<details>
    <summary><b>Get started with our free fully managed Freelunch playground in your browser</b></summary>

### __Get started with our free fully managed Freelunch playground in your browser__

We offer a 1-week free fully managed Freelunch playground for you to tinker with and get to know how building and evaluating ML platforms feels like with Freelunch. It works directly in your browser and requires no setup at all. You can also tinker with any of the MLOps platforms in the Freelunch Hub (will be explained in the following section).
</details>

<details>
    <summary><b>Freelunch's properties</b></summary>

### Freelunch is:

1. __0 lock-in__ (Freelunch builds platforms that are independent of Freelunch itself. So, if you want to stop using Freelunch, just stop using it! No MLOps platform rewrites are necessary for you to continuw your MLOps journey without Freelunch.)

2. __Extensible__ (you can extend it without having to refactor the codebase (Note: the codebase standard makes it easy to switch component implementations or even plug-in your own custom ones) via building custom plugins (your own component and/or building block), when it doesnt support some tool you need or for your own internal tools. We provide a plugin generator to make your life easier. Then, you register your new plugin so that the Interactive Designer and Platform Builder can use them later.)

3. __Persona/role and team-based__ (built-in separate interfaces and levels of abstraction for stakeholder (e.g., can play with a lot of models, comment, but can't edit anything), manager/lead, subject matter expert, researcher, data scientist, ml engineer, data engineer, software development engineer (sde), software reliability engineer (sre), security engineer (red and blue), lawyer, data & security auditor, AI auditor; while each persona can visualize the other persona's work in a higher-level. You can setup multiple separate platforms teams working in parallel in different subsystems of the MLOps platform and you can setup multiple separate business teams working in parallel within the same MLOps platform. Each team has a sublead/manager that manages only his team. Lead/managers configure coarse-grained IAM (authorization rules) and delegate fine-grained IAM to subleads/submanagers.)

4. __Following Software Engineering best practices:__ standard codebase structures for projects; built-in static analysis: code smell detection, coding style checks, security checks, compliance enforcement, service-level input expectations (making sure the input is in the expected format and with expected bounds); design patterns by default; built-in different abstractions for different personas, separate workspaces for different purposes.

5. __Licence-enforced__ (automatically detects dependencies, retrieves licenses, builds a file declaring licenses of dependencies and alerts you if you are using it in a waty that might go against its licence. Licences can be for code, data or models.)

6. __Cost-aware__ (Use cost as a preference to make Freelunch build low cost MLOps platforms. Track and profile your cloud/premise cost, get automatic cost reductions (e.g., via spot VMs and removing idle resources) and get suggestions for cost reductions that will incite a trade-off with other things (e.g., build and evaluation time), only retrain/tune models when their estimated metric improvement is worth the training/tuning cost)

7. __Experiment Tracked__ (all experiments (different MLOps platform versions with resulting metrics) are versioned and available in a beatifull UI in Freelunch Hub. Note: repo contains pointer to experimentation environment image in image registry. Built-in support for PoCs (with Wizard-of-Oz support), these are experiments that are not meant to go to production, they are just meant to justify production efforts in certain direction (E.g., estimation of metrics improvement from PoC to stable production).)

8. __Statefull and Resilient__ (MLOps platforms built by Freelunch contain a  ML System State Store that stores and versions data and config states of ML Systems via backups and patches. This ML System State Store is used to provide ML System state when needed (e.g., for doing replays or for maintaining state across integration testing/staging/prod). If you try a new project, new databases, your clusters break or your spot machines get reclaimed, where is the state of the previous one persisted in an easily-retrievable way? It is persisted via backups and patches in a Data Lake deployed at the start of every Freelunch project. The  ML System State Store is just a virtual DB that uses the data lake under the hood. You can also choose to takeout all of the Data Lake data as a set of .rar/.zip file dumps, then you can kill the Data lake. Freelunch also lets you visualize and interact directly with the  ML System State Store)

9. __Multi-team and multi-platform__ (Each ML Patform (and all of its versions) are a single Freelunch project. You can build separate MLOps platforms in parallel by having multiple projects. This can be usefull in these scenarios: (1) large comapnies with multiple ML teams with different needs, (2) when you want parts of your system to "see" other parts as external third-party services for higher-level modularization, or (3) you want to completely separate critical ML Systems from non-critical ones, so that failures in non-critical ones can't affect the critical ones. The latter can be usefull for example for reproducibility (e.g., using same feature store for all ci/cd stages; in this case the feature store is a separate Freelunch project and is seen by the rest of the system as an external service which only exposes a usefull interface (not internal knowledge about it is known)). Inter-team collobaoration is also supported: each team can see other team's progress & updates, chat with messages from them with links that take you directly to the problem, receive help & issue requests (issues are posted on github repi automatically).)

10. __No-code for giving you a headstart__ (Note: Freelunch is not a no-code tool! Freelunch just uses a no-code interactive approach to generate advanced project setup that fits your needs, to give you a true hot-start. This is better than picking templates.)

11. __Maturity-guided__ (Freelunch uses the concept of evolutions. Evolutions are a sequence of maturity-evolving (more automation, capabilities, best practices, quality and performance at the expense of more cost and expertise) patterns or whole platform specifications (set of non-overlapping patterns) for an organization to use in their MLOps journey, which allows them to keep easily extending their current platform for better MLOps and only introduce more complexity when really needed. A pattern or platform can have multiple evolution branches. An evolution typically: reduces manual work, reduces failures, increases compute costs, incoporates complexity via more abstractions and tooling, substitutes managed tools for open source ones and imposes more standards.)

12. __Layered__ (has a 2-layer API. Layer 1 is easier to use, provides higher-level abstractions and uses a bunch of smart defaults for low-level abstractions). Layer 2 is harder to use but provides higher-level of control by giving the user the possibility to access and edit the lower-level abstractions, including editing the smart defaults used in Layer 1. Ideally, users start at layer 1, gradually move towards using more layer 2, establish custom smart defaults and stabilize at using both layers.)

13. __Very well documented__ (besides standard API reference and "getting started", contains also: (1) multiple end-to-end examples for different sectors, contraints and preferences; (2) real-world case studies; (3) detailed pros and cons of MLOps tools; (4) browser sandbox for tinkering with mlops platforms)

</details>

<details>
    <summary><b>Anatomy of Freelunch MLOps platforms</b></summary>

### __Anatomy of Freelunch MLOps Platforms__

1. Governance
    1. Manual Documentation: any documentation outisde of automatic codebase documentation
        1. __Needs__ (subjective goals of the ml system)
            1. __Current needs__
            2. __Needs evolution__ (how you expect the needs to evolve in the next 5 years)
        2. __Requirements__ (specific constraints and engineer preferences for the platform)
            1. __Current requirements__ (tool usage and integration, operational metrics (SLOs and SLAs), running cost, predictive power, observability, etc)
                1. __Constraints__ (only need to be above a certain threshold)
                2. __Preferences__ (need to be above a certina threshold but resources shuld be allocated for further improvement)
            2. __Requirements evolution__ (how you expect the requirements to evolve in the next 5 years)
                1. __Constraints__
                2. __Preferences__
        3. __Problem formalization__ (problem formalized in terms of ml and business KPIs, requirements and assumptions)
            1. __Current problem formalization__
            2. __Problem formalization evolution__ (how you expect problem formalization to evolve in the next 5 years)
        4. __Design Docs__ (Solution Analysis. Alternatives to build the MLOps Platform, tradeoffs, discussions and lessons learned. Append-only doc.)
        5. __Risk Analysis__ (Past failures (out or in prodution), Failure modes with cause (e.g., fault trees or Failure Modes and Effects Analysis (FMEA)), how often hey happen, how bad they are, solution specification changes that make the system less risker and their tradeoffs (e.g., increase cost and latency) and methods to detect and adress the failures gracefully and in a contained manner)
    2. Ownership/Accountability: who is responsible for which part of the development or operation of the MLOps platform
    3. Compliance
        1. Certifacates
        2. Issues
        3. Tests
        4. Declaration of licences of dependencies
    4. Initial Platform manifest: a file that others can supply to freelunch to build the exact same initial MLOps platform (We say initial because the repo afterwards, but then you just have to follow the commit history)
    5. subject matter expert Knowledge files (files that try to distill subject matter expert knowledge about the real-world process we are tring to model)

2. Base CI/CD
    1. Git/Github stuff
    2. Pre-commit hooks
    3. Automatic Codebase Documentation (automatically generate and deploy beatifull solution specification documentation (platform manifest, architecture diagrams, schemas, coupling description/dataflows, artifact (model, filter, pipeline, dataset, etc) cards, line-by-line explanation, line-by-line explanation)
    4. Collaboration Knowledge Base Setup
    5. Meta Pipeline Cluster Setup
    6. Container Image and Package Registries setup
    7. Base Workflow Orchestrator Setup
    8. Base CI/CD tool setup (with monitoring and alerts)

3. MLOps Platform

    1. ML System State Store. It is important because: (1) even if all your clusters break, you will have your data in a safe backup; (2) enables easy use of the same data across MLOps platforms experiments (e.g. changing the database that powers your feature store).
        1. System State (config, schema, data, deploy) Store: versions & provides any state in history or combination of config, schema, data and deploy states (makes it possible to go back to any state of the system) Stores versioned system config (e.g., data imputation method to be used by feature store, error handling protocol to be used by prediciton manager, DAGs to be implemented by Data Pipelines, etc. configs can be changed in your repo or the state store itself (by athrorized clients) and then distributed to systems or can be done independently by the systems and then sent to the state store so that it can be updated.), schema (db migrations and file format changes) data (backups & patches (telemetry)).
            1. Config Store (Tools: Apollo, NACOS, Salt, Configu, OmegaConf Hydra, DynaConf)
            2. Schema Store (Tools: Liquibase)
            3. Data Store (Tools: (1) Event storing: EventStore; (2) Backup versioning: LakeFS + object storage or DB)
            4. Deployment Store (e.g., k8s deploy state)
            5. API Store (stores history of APIs for each service)
        2. State change message broker/stream transport (delivers state change events to system state store)

    2. Logic (deployed often): 2-step Meta Pipeline. The Meta Pipeline is deployed by your base CI/CD tool (e.g. Jenkins) and contains code that makes up the MLOps platform.

        1. Meta Pipeline itself

            1. Provision Infrastrcture
                1. Privision ML Pipeline (MLOps platform) Infrastructure (provisioned on-demand).
                    1. Integration testing workspace (provisioned on-demand OR at the first time and then always up): <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator> + ML pipeline artifact store + ML pipeline integration tester.
                    2. Staging workspace (provisioned on-demand OR at the first time and then always up): same as *Production workspace*
                    3. Evaluation workspace (provisioned on-demand OR at the first time and then always up): <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator> + ML pipeline artifact store + ML pipeline evaluator.
                    4. Production workspace (provisioned at the first time and then always up): <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator> + ML pipeline artifact store + ML pipeline observer (with a lineage store).
                
            2. ML Pipeline deploy

                1. ML Pipeline itself

                    1. ML Pipeline startup

                        1. Deploy Services

                            1. Initial ML System

                                1. Provision ML System Infrastructure
                                    1. By type
                                        1. Operator Worskpace. Hosts k8s operators which extend kubernetes native functionality (e.g., autoscaling): <cloud cluster provider/on-premise cluster OR cluster namespace + VPN server + DNS server + cd operator + autoscaling operators + backup operators>
                                        2. Logic workspace. Host logic, driver programs: <cloud cluster provider/on-premise cluster OR cluster namespace + VPN server + DNS server + python virtual environment + notebook ecosystem + installed common dev tools>
                                        3. Workloads Workspace. Hosts workloads driven by logic services.
                                            1. ML System Pipeline Workloads Workspace. Hosts pipelines driven by ML System services: <cloud cluster provider/on-premise cluster OR cluster namespace + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator>
                                            2. ML System Distributed Processing Workloads Workspace. Hosts distributed processing workloads driven by either ML System logic services directly or ML System pipeline workloads: <cloud cluster provider/on-premise cluster OR cluster namespace + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator>
                                        4. Storage Workspace. Hosts big data storage services (used by more than one service or task) such as artifact stores, feature stores, data lakes and data wharehouses used by logic services: <cloud cluster provider/on-premise cluster OR cluster namespace + VPN server + DNS server>
                                    2. By stage
                                        1. Integration testing workspace (provisioned on-demand OR at the first time and then always up): + Services OR Pipeline integration tester.
                                        2. Staging workspace (provisioned on-demand OR at the first time and then always up): + Production Traffic Simulator
                                        3. Evaluation and Observability (bacground cluster) workspace (provisioned on-demand OR at the first time and then always up)
                                        4. Production workspace (provisioned at the first time and then always up): + nothing

                                2. Deploy ML System itself: services that are initally deployed (e.g., feature store, etl/elt pipelines, artifact store, metadata store (lineages, logs and metrics), data wharehouse, data lake, Inference Services, image registry, API gateway, message broker, frontend, prediction manager, llm orchestrator, llm gateway, agent actuators, agent observers, etc)

                            2. Operations Center

                                1. Provision Operations Center Infrastructure
                                    1. By type
                                        1. Operator Worskpace. Hosts k8s operators which extend kubernetes native functionality (e.g., autoscaling): <cloud cluster provider/on-premise cluster OR cluster namespace + VPN server + DNS server + cd operator + autoscaling operators + backup operators>
                                        2. Logic Workspace
                                            1. Development Workspace. Hosts dev and experimentation (feature and model) environments: <cloud cluster provider/on-premise cluster + VPN server + DNS server>
                                            2. Operations Logic Workspace. 
                                                1. Services. Hosts operation center services: <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator>
                                                2. Environment. Host ML Operations environment: <cloud cluster provider/on-premise cluster + VPN server + DNS server>
                                        3. Workloads Workspace
                                            1. Artifact Experimentation Workspace. 
                                                1. Artifact Production Workspace. Hosts ephemeral experimentation workloads (aka experimentation pipelines): <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator>
                                                2. Artifact Evaluation and Optimization Workspace. Hosts ephemeral artifact evaluation and optimization experimentation workloads. Hosts ephemeral experimentation workloads (aka experimentation pipelines): <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator>
                                            2. Operations Workspace.    
                                                1. Operations Pipeline Workloads Workspace. Hosts pipelines driven by operation center services: <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator>
                                                2. Operations Distributed Processing Workloads Workspace. Hosts distributed processing workloads driven by either operations logic services directly or via pipeline workloads: <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator + workflow orchestrator OR managed workflow orchestrator>
                                        4. Storage Workspace. Hosts big data storage services (used by more than one service or task) such as artifact stores, feature stores, data lakes and data wharehouses used by logic services: <cloud cluster provider/on-premise cluster + VPN server + DNS server + (optional) container orchestrator>
                                    2. By stage
                                        1. Logic Workspace
                                            1. Integration testing workspace (provisioned on-demand OR at the first time and then always up): + Services OR Pipeline integration tester.
                                            2. Staging workspace (provisioned on-demand OR at the first time and then always up): + Production Traffic Simulator
                                            3. Production workspace (provisioned at the first time and then always up): + nothing
                                        2. Workloads Workspace
                                            1. Artifact Experimentation Workspace: ephemeral workspace (provisioned on-demand OR at the first time and then always up). Debugging and execution is done in this workspace.
                                            2. Operations Workspace.
                                                1. Integration testing workspace (provisioned on-demand OR at the first time and then always up): + Services OR Pipeline integration tester.
                                                2. Staging workspace (provisioned on-demand OR at the first time and then always up): + Production Traffic Simulator
                                                3. Production workspace (provisioned at the first time and then always up): + nothing
                                        3. Storage Workspace.
                                            1. Integration testing workspace (provisioned on-demand OR at the first time and then always up): + Services OR Pipeline integration tester.
                                            2. Staging workspace (provisioned on-demand OR at the first time and then always up): + Production Traffic Simulator
                                            3. Production workspace (provisioned at the first time and then always up): + nothing
                                        

                                2. Deploy Operations Center itself: dashboard equipped with backend services that contain everthing necessary to operate the ML system

                                    1. Generalized CI/CD pusher services (Note: gitops is generalized ci/cd concerning engineer-generated events (what is meant usually by ci/cd), specifically using git repos for this) (their output goes directly to generalized ci/cd): instead interacting directly with the repo, you interact with intuitive UIs. Under the hood, pushes to the repo are being done your you. These can be: IT Asset Manager + Security (Vulnerability and Malware) Scanner (for machines,networks, endpoints, cloud, secrets, pure code, dependencies, containers, kubernetes, web and mobile) + ML Artifact Manager + ML System Decorator (e.g., adds/remove prperties that span multiple ML service. E.g., update A/B testing rules or change from mysql to postgres) + ML Config Manager + Documentation Builder + Data Pipeline Builder (compiles to and from tool-agnostic pipeline) + Model Builder (support for Distributed Training, Hyperparameter Tuning, Knowledge Distillation, Ensembling and deciding which features to remove and make a model for each value/value range of it) + Prompt Builder + Filter Builder + ML Featured Dataset Builder (EDA + wrangling + augmentation + labeling + feature engineering) + ML Filter Builder + ML PoC Builder (with Wizard-of-Oz support) + MLOps Architecture Documentation Builder + ML Experimentation Environment (feature and model) + Content Management System (CMS) (with support for docs and model frontend components) + Frontend Mocker (Support for broser, desktop, mobile, agent simulation environments and game engines) + Rollbacker (lets you easily rollback to old versions of code, config and artifacts). Note: as soon as some of these service dont need to be interactive anymore, they can be moved to within generalized CI/CD itself for streamlined workflow (e.g., Builder Services)!

                                    2. Helpers: that provide some kind of help, but dont deploy to your ML System
                                        1. Services: Single-Sign-On (SSO)/Secrets Manager + Services Observer + Security Observer (Identification of vulnearabilities and attacks + internal communication + implementation of incident response protocols) + Security Testing (Fuzz, Insider and Penetration) Tracker + ML Artifact Experiment Tracker (with scaling law estimator) + Discoverer (Data, Service, Artifact and Package Discovery) + Artifact Expert (Evaluation, Analysis & Visualization, Optimization, Compilation, Scaling Law Estimation, Stamping and Mocking) + ML Observer (Training and Inference Monitoring + ML System Observer (fine-grained monitoring + debugging/improvement analysis with observability stores (model, metadata, lineage, production state checkpoint and engineer activity)) GUI for your DBs) + ML-Business KPI Aligner + Notifier + ML Compliance Manager + ML Project Manager (Management and Collaboration) + Offline Container Image Registry + External Dependency Changes Notifier (e.g., change in gpt version) + Interactive Designer + R&D Playground + Input Data Distribution Forecaster.
                                        2. Tasks: replay the ML System in the background cluster, penetration Test (recon, build and attack), onboard new team member, benchmark a building block, recover the state of the ml system after a crash occurred to the cluster, migrate the ML System (using the ML System State Store) to another cloud provider if the current cloud provider crashes or raises price abruptly.
                                        3. REPL Operations: ML Operations Environment with Notebook + Terminal with action tracker (for doing stuff currently not supported by your Operations Center GUI)
                                        4. Chat Interface (AIOps): where you chat with the MLOps copilot (Virtual ML Engineer)  to give you action suggestions in the operations Center
                                    
                                    3. Webserver: serves single Operation Center Frontend. 

                        2. Deploy Initial State of ML System

                            1. Test Inital State manifest file together with the  ML System State Store (which is always running, it was deployed when we started a Freelunch MLOps platform project).

                            2. Build Inital State from Desired Inital State manifest file. This is via getting ML System data and config versions frm the  ML System State Store.

                            3. Deploy Initial State to the ML System.

                    2. Execute ML Pipeline loop: generalized CI/CD to the ML System. Pipelines (implemented by a workflow orchestrator) that ci/cd's code and artifacts ((1) interface artifacts: schema (data lake schema, db/wharehouse schemas, file schemas, message broker/transport schemas, model schema, filter schema, retriever schema, function schema (signature), elt schema, etl schema, pipeline schema, networking api schemas); (2) implementation artifacts: config (service, ci/cd and frontend), data, schema expectations/data validation (ranges, distributions, cosntraints, units, etc), prompter chains, compute artifacts (fallbacks, intelligent appeal policies, ensembles, pre/post processors (input, prediction and/or data filters), FaaS functions, elt queries, pipeline components (tasks or services))). Generalized ci/cd events can come from: (1) repo PR; (2) internal ml system events; (3) external events. Generalized ci/cd events can be read via: (1) making requests to statefull services/subsystems; (2) making requests to message brokers; (3) listening to requests from statefull services/subsystems; (4) listening to streaming messages from services or brokers.

                        1. Code ci/cd

                            1. Dev (Note: before the actual ci/cd) (uses Pre-commit hooks)

                                1. Conversions: notebook-to-script conversion, artifact substitution
                                2. Static Analysis: Linting, security and compliance checks
                                3. Dependecy Injection (making functions/classes more testable by putting their dependencies as arguments and modifying tests to adhere to this)
                                4. Testing
                                    1. Unit Testing and function/class-level integration testing (functions/classes and services)
                                    2. COntainer-level Integration Testing (local virtual dev cluster, remote virtual dev cluster or remote dev cluster). A stage-x-to-production estimator tool is used to adapt load accordingly as to be representative of the large scale production version

                            2. CI
                                1. Satelletite Branch CI
                                    1. Static Analysis: Linting, security and compliance checks
                                    2. Unit Testing (functions/classes and services)

                                    3. Preview (preview remote machine, pod or cluster). Where developers can preview their application interactively in a nice playground/killercoda-style GUI, to gain more confidence in their PR. Note: integration tests are not run here.

                                2. Main Branch CI
                                    1. Static Analysis: Linting, security and compliance checks
                                    2. Unit Testing (functions/classes and services)

                                    3. Integration Testing (integration testing cluster). Where you testing services are working as expected together. To avoid unnecessary time and cost, operates on the smallest scale possible. A stage-x-to-production estimator tool is used to adapt load accordingly as to be representative of the large scale production version. Does diff integration testing between specific services of new release (PR to release branch) and previous release (PR to release branch), using production data (past and future estimate).

                            3. CD

                                1. Staging (staging clusters). The final verification that everything is ok, should mimic production, clusters at the same scale and geographical locations. Offcluster end-2-end testing of new release (PR to release branch) and Diff end-2-end testing between new release (PR to release branch) and previous release (PR to release branch), using production data.

                                2. *Optional*: Evaluation (background cluster). The ml system and mlops platform are evalauted in multiple dimensions, before it is deployed to production. It is optional because it can slow down your ci/cd, therefore you might choose only some evaluations.
                                
                                3. Production (production clusters)

                                4. Evaluation and Observability (background cluster). The ml system and mlops platform are evaluated in multiple dimensions, after it is deployed to production; and where you can do simulation-based observability to uncover problems proactively and to find improvement opportunities.

                                5. PR to stable branch or rollback
                                    1. If Evaluation and Observability dont show problems during some time x: PR to stable branch
                                    2. Elseif problems are identified: take a step back. Active Fallback or rollback a service/subsytem to a previous version. This can be automatic (based on metrics gathered in a brief time after the release (PR to release branch)) or manual.

                        2. Artifact ci/cd (data artifacts (configs, schemas, datasets) and computational artifacts (e.g., models, ensembles, pre/post processors, DSL programs, etc)

                            1. CI

                                1. Static Analysis: data validation (artifact format; artifact expectations; format and completeness of artifact card)

                                2. Artifact Signing Verification: artifacts need to contain a aignature of some employee with clearence to push it.

                                3. *Optional*: Evaluation (background cluster). Where the artifacts are evaluated evaluated in multiple dimensions, before it is deployed to production. It can slow down your ci/cd, therefore you might choose only some evaluations.

                            2. CD

                                1. Staging (staging clusters). The final verification that everything is ok, should mimic production, clusters at the same scale and geographical locations. Offcluster end-2-end testing of new release (PR to release branch) and Diff end-2-end testing between new release (PR to release branch) and previous release (PR to release branch), using production data (past and future estimate).

                                2. Production (production clusters)

                                3. Evaluation and Observability (background cluster). Where the artifacts are evaluated in multiple dimensions, after they are deployed to production; and where you can observed them to get more insights (e.g., visualize model working)

                                5. PR to stable branch or rollback
                                    1. If Evaluation and Observability dont show problems during some time x: PR to stable branch
                                    2. Elseif problems are identified: take a step back. Active Fallback or rollback a service/subsytem to a previous version. This can be automatic (based on metrics gathered in a brief time after the release (PR to release branch)) or manual.
                
                2. ML Pipeline tests
                    1. Unit Tests
                    2. Contract Tests
                    3. Integration Tests

        2. Meta Pipeline Tests (Note: these tests are run by your IT CI/CD, which covers your whole IT System)
            1. Unit Tests
            2. Contract Tests
            3. Integration Tests

</details>

<details>
    <summary><b>Properties of Freelunch MLOps platforms</b></summary>

### Freelunch can build MLOps platforms that are:

1. __Production-grade__ (e.g., ml project management, software engineering best practices, collaboration, secrets manager, authentication/authorization, compliance enforcement, security testing and operations (monitoring, observability and actions))

2. __Complete__ (covers project management, learning, design, coding/configuring, dataset construction, offline and online experimentation, ci/cd, prediction client code (frontend or embedded device), operations, security testing, ROI analysis and loop back)

3. __Extensible__ (you can extend it without having to refactor the codebase (Note: the codebase standard makes it easy to switch component implementations or even plug-in your own custom ones) via building custom plugins (your own component and/or building block), when it doesnt support some tool you need or for your own internal tools. We provide a plugin generator to make your life easier.)

4. __Python-based__ (made for coding with python and doesnt break any of python's functionality you are used to)

5. __Compatible with your existing infrastructure__ (you can require Freelunch to use/not use specific tools for the build)

6. __Operations-heavy:__ multiple ops capabilities such as: project management, collaboration, codebase management, IT operations, ML operations (e.g., retraining, killing buggy ML workload, fixing data problem, etc), internal assets search.

7. __Control-level-dependent:__ you can create MLOps platforms that contraint a lot applied ML Engineers (e.g., only allow them to do a handfull of activities (e.g., ci/cd artifacts and ML minitoring) or you can create MLOps platforms that give full control to applied ML Engineers (e.g., edit project management, IT infrastructure, ML infrastrcture, Data Pipelines and even the MLOps platform codebase)). You can also do anything in the middle of this spectrum.

8. __CI/CD-based:__ (multiple embedded generalized ci/cd (that ci/cd code/config/data/artifacts) loops orchestrated by a workflow orchestrator. Each loop can have ci/cd stuff such as: static analysis (code quality, compliance and security), testing (unit and integration testing), building, storing images, staging, deploying, documentation generation. 

Supports:
    * Edge & Hybrid Model Deployments ("There are many possible designs where models are deployed both on servers and user-facing devices (or other infrastructure in a network). A common scenario is to deploy a smaller or special-purpose model on the client side and a larger model on the server, where the smaller model is used in many settings as a first step and the larger server-side model is used for more challenging inputs. For example, voice-activated smart speakers often have a small client-side model to detect an activation sequence such as “Alexa” or “Hey Siri” and send the subsequent audio with the main command to a server for analysis")
    * Different git servers (e.g., github, gitlab, gitea). 

New changes can be ine one of the following levels (from most conservative to least):
    (1) component/system-level: diff testing in integration testing and/or staging cluster
    (2) component-level: deployed only to background cluster; 
    (3) component-level: shadow deployment;
    (4) with enforced manual PR acceptance (code review, manual evaluation, etc);
        (1) component/system-level: rollback automatically if some abnormal condition is met in an inital timeframe; 
        (2) component-level: canary deployment; 
        (3) component-level: A/B deployment;
        (4) component-level: gradual deprecation (like an A/B test, but insted of two compting versions running together, you have old and new versions working together, and the old version gets removed after a time period, assuming clients has time to adapt to the new version already)
        (5) system-level: gradual deployment (like an A/B test, but insted of two compting versions running together, you have old and new versions working together, and slowly transition other components to the new version, when no component depends anymore on the old version, the old version gets removed)
        (6) component/system-level: vanilla deployment.
    (5) without enforced manual PR acceptance (code review, manual evaluation, etc);
        (1) component/system-level: rollback automatically if some abnormal condition is met in an inital timeframe; 
        (2) component-level: canary deployment; 
        (3) component-level: A/B deployment;
        (4) component-level: gradual deprecation (like an A/B test, but insted of two compting versions running together, you have old and new versions working together, and the old version gets removed after a time period, assuming clients has time to adapt to the new version already)
        (5) system-level: Gradual deployment (like an A/B test, but insted of two compting versions running together, you have old and new versions working together, and slowly transition other components to the new version, when no component depends anymore on the old version, the old version gets removed)
        (6) component/system-level: vanilla deployment.

Any change can:
    (1) be manually or atomatically rolled-back to a previous version (ehn certain production metric conditions are met).
    (2) be scheduled based on time or a condition.
    (3) pipelined with automatic triggers (e.g., if" shadow deployment" goes well, trigger "rollback automatically if some abnormal condition is met in an inital timeframe")
        
Since ML Systems have significant coupling between components, new changes often require that other parts of the system are changed accordingly, to avoid breaking things. These other parts are highlighted for every new change via a coupling description/dataflows file of the ML System. Also, each change gets an estimated cost increase/decrease (e.g., feature store is changed by removing a feature --> models need to be changed --> downstream models need to be changed) so that ML practitioner can think if the change is really worth it.

The ci/cd loops are depicted below (assuming a platform with full control for applied ML Engineers):

* __*Freelunch* management: cost analysis and configuration --> *ML Lead/Product Manager*__
    * __Generalized CI/CD Monitoring Wrapper (give you traces (Change --> CI --> CD --> Effect), metrics and visual diffs) --> *ML Engineer (MLOps)>MLOps platform Engineer (MLOps)*__
            * __[ML Platform improvement using base ci/cd] IT infrastructure/DevOps (e.g., codebase management, cloud IaaS (machines, proxies, networks), cloud storage, IAM, IT observability actions, container image generation, kubernetes, ci/cd) ci/cd loop --> *Software Engineer (DevSecOps)>Software Development Engineer (SDE), Software Engineer (DevSecOps)>Software Tester Engineer, Software Engineer>Software Reliability Engineer (SRE), Cybersecurity Engineer (SecOps)>*.__ (other role names such as: Infrastructure Engineer, DevOps Engineer, System Administrator, IT Manager and Security Engineer also belong here)
                --> Artifact ci/cd (spans: data and computational artifacts) (note: data state of a statefull component or system, saved as a backup, counts as an artifact)
                --> Code ci/cd (spans: functions, scripts, libraries, executables, container images, app pods, operator pods, plugins and macros)
                --> Config ci/cd (spans: build and runtime config) (for: (1) ML System Config: (1.1) Services Config: Config Manager config, Generalized ci/cd config; (1.2) Task config; (2) Observability Config: (2.1) Services Config: helper services config; (2.2) Tasks Config: helper tasks config)
                * __[ML Platform improvement using base ci/cd] ML infrastructure (e.g., development of ml services (e.g., workflow orchestrator, feature store, Model/Prompt Registry, model deployer, monitoring system) and mlops ci/cd)  --> *ML Engineer (MLOps)>MLOps platform Engineer, Frontend Engineer and Cybersecurity Engineer (SecOps)*.__
                    --> Artifact ci/cd (spans: data and computational artifacts) (note: data state of a statefull component or system, saved as a backup, counts as an artifact)
                    --> Code ci/cd
                    --> Config ci/cd (spans: build and runtime config) (for: Config Manager config, Generalized ci/cd config, Observability Config, ML System config)
                    * __[ML System improvement using ML platform ci/cd] ML project roadmap, schedule, deadlines, engineer management and business monitoring actions --> *ML Lead/Product Manager*__ 
                    --> Artifact ci/cd (spans: data artifacts)
                    --> Config ci/cd (spans: runtime config) (for analytics config)
                    * __[ML System improvement using ML platform ci/cd] ML datasets --> *ML Engineer (MLOps)>ML System (Applied) Engineer & subject matter expert*__
                        --> Data Sources (spans: data and schema) ci/cd
                        * __[ML System improvement using ML platform ci/cd] Data engineering: storage, pipelines, transport and sync ci/cd loop --> *Data Engineer, Frontend Engineer and Cybersecurity Engineer*.__
                            --> Artifact ci/cd (spans: data and computational artifacts) (note: data state of a statefull component or system, saved as a backup, counts as an artifact)
                            --> Code ci/cd
                            --> Config ci/cd (spans: build and runtime config) (for: Config Manager config, Generalized ci/cd config, Observability Config, ML System config)
                            --> Data Sources (spans: data and schema) ci/cd
                            * __[ML System improvement using ML platform ci/cd] ML model/prompt chains --> *ML Engineer (MLOps)>ML System (Applied) Engineer and Cybersecurity Engineer*__
                                --> Artifact ci/cd (spans: computational artifacts) (for: cd of model files (e.g., ONNX))
                                --> Code ci/cd (for: cd of model code (e.g., executable or library))
                                * __[ML System improvement using ML platform ci/cd] Frontends --> *Frontend Engineer and Cybersecurity Engineer>Web and Mobile Security Engineers*__
                                    --> Code ci/cd 
                                    --> Config ci/cd (spans: build config)
                                    * __[ML System improvement using ML platform ci/cd] PoC --> *Data Scientist & subject matter expert*__
                                        --> Artifact ci/cd (spans: computational artifacts) (for cd of model files (e.g., ONNX, PMML, PFA))
                                        --> Code ci/cd (for cd of model code (e.g., executable or library))
                                        --> Config ci/cd (spans: build config) (for cd of config-based training workload)
                                        * __(Optional) Your own Foundation Models System__ (think of it as a sub MLOps platform within the general MLOps platform, the same loops would apply here aswell, but I didnt put them because it would become to messy) __--> *ML Engineer (MLOps)>ML System (Applied) Engineer>FM Engineer (FMOps/LLMOps) & ML Researcher>FM Researcher*__
                                            --> Artifact ci/cd (spans: data and computational artifacts) (Note: data state of a statefull component or system, saved as a backup, counts as an artifact)
                                            --> Code ci/cd
                                            --> Config ci/cd (spans: build and runtime config)
                                            * __(Optional) Development of new MLSys methods, models, datasets & evaluations and better ML understanding --> *ML Researcher>MLSys Researcher and ML Researcher>FM Researcher*__ 
                                                --> Artifact ci/cd (spans: data and computational artifacts) (for: cd of datasets, model files, learning materials)
                                                --> Code ci/cd (for: cd of notebook, repo or demo)

The stages of ci/cd are depicted below:

1. Code ci/cd

    1. Dev (Note: before the actual ci/cd) (uses Pre-commit hooks)

        1. Conversions: notebook-to-script conversion, artifact substitution
        2. Static Analysis: Linting, security and compliance checks
        3. Dependecy Injection (making functions/classes more testable by putting their dependencies as arguments and modifying tests to adhere to this)
        4. Testing
            1. Unit Testing and function/class-level integration testing (functions/classes and services)
            2. COntainer-level Integration Testing (local virtual dev cluster, remote virtual dev cluster or remote dev cluster). A stage-x-to-production estimator tool is used to adapt load accordingly as to be representative of the large scale production version

    2. CI
        1. Satelletite Branch CI
            1. Static Analysis: Linting, security and compliance checks
            2. Unit Testing (functions/classes and services) and function/class-levle integration testing

            3. Preview (preview remote machine, pod or cluster). Where developers can preview their application interactively in a nice playground/killercoda-style GUI, to gain more confidence in their PR. Note: integration tests are not run here.

        2. Main Branch CI
            1. Static Analysis: Linting, security and compliance checks
            2. Unit Testing (functions/classes and services) and function/class-levle integration testing

            3. Container-level Integration Testing (integration testing cluster). Where you testing services are working as expected together. To avoid unnecessary time and cost, operates on the smallest scale possible. A stage-x-to-production estimator tool is used to adapt load accordingly as to be representative of the large scale production version. Does diff integration testing between specific services of new release (PR to release branch) and previous release (PR to release branch), using production data (past and future estimate).

    3. CD

        1. Staging (staging clusters). The final verification that everything is ok, should mimic production, clusters at the same scale and geographical locations. Offcluster end-2-end testing of new release (PR to release branch) and Diff end-2-end testing between new release (PR to release branch) and previous release (PR to release branch), using production data (past and future estimate).

        2. *Optional*: Evaluation (background cluster). The ml system and mlops platform are evalauted in multiple dimensions, before it is deployed to production. It is optional because it can slow down your ci/cd, therefore you might choose only some evaluations.
        
        3. Production (production clusters)

        4. Evaluation and Observability (background cluster). The ml system and mlops platform are evaluated in multiple dimensions, after it is deployed to production; and where you can do simulation-based observability to uncover problems proactively and to find improvement opportunities.

        5. PR to stable branch or rollback
            1. If Evaluation and Observability dont show problems during some time x: PR to stable branch
            2. Elseif problems are identified: take a step back. Active Fallback or rollback a service/subsytem to a previous version. This can be automatic (based on metrics gathered in a brief time after the release (PR to release branch)) or manual.

2. Artifact ci/cd (data artifacts (config, data sources) and computational artifacts (e.g., models, ensembles, pre/post processors, DSL programs, etc)

    1. CI

        1. Static Analysis: data validation (artifact format; artifact expectations; format and completeness of artifact card)

        2. Artifact Signing Verification: artifacts need to contain a aignature of some employee with clearence to push it.

        3. *Optional*: Evaluation (background cluster). Where the artifacts are evaluated evaluated in multiple dimensions, before it is deployed to production. It can slow down your ci/cd, therefore you might choose only some evaluations.

    2. CD

        1. Staging (staging clusters). The final verification that everything is ok, should mimic production, clusters at the same scale and geographical locations. Offcluster end-2-end testing of new release (PR to release branch) and Diff end-2-end testing between new release (PR to release branch) and previous release (PR to release branch), using production data (past and future estimate).

        2. Production (production clusters)

        3. Evaluation and Observability (background cluster). Where the artifacts are evaluated in multiple dimensions, after they are deployed to production; and where you can observed them to get more insights (e.g., visualize model working)

        5. PR to stable branch or rollback
            1. If Evaluation and Observability dont show problems during some time x: PR to stable branch
            2. Elseif problems are identified: take a step back. Active Fallback or rollback a service/subsytem to a previous version. This can be automatic (based on metrics gathered in a brief time after the release (PR to release branch)) or manual.

    )

9. __Cost-aware__ (provision cluster of cloud vendor with lowest price, track and profile your cloud/premise cost, get automatic cost reductions (e.g., via spot VMs and removing idle resources), get suggestions for cost reductions that can incite a trade-off with other things (e.g., infrastrcture quality, latency, tool choice), get an alaysis to decide the profit-optimal ML system setting.

10. __Compliance-supported:__ (1) For your own MLOps platform: compliance todo's, compliance tests, external audit support, certificate management (e.g., (1) Data & Security: (1.1) Standard: GDPR, CCPA, SOC2, ISO 27001, HIPAA, FERPA, PCI DSS, GLBA; (1.2) Custom: agreements (SLAs, DPAs, etc); (2) AI-specific: EU AI Act, NY AI Bias Law, AI Bill of Rights, SB 1047, Equal Employment Opportunity Commission (EEOC) commission’s twelve factors of discrimination, etc), audit tracking; (2) For managed third-party dependencies: certificate management and audit tracking; 

11. __Secure:__ privacy-preserving ML (private EDA, model building, task execution and deployment; via usage of on-premise machines, data profiling, anonymization, fair representations, differential privacy, homomorphic encryption, zero-knowledge proofs and trusted execution environments (TEEs)), you can config the cybersecurity setup of your instances, run static and dynamic security tests, manage security compliance audits, scan for vulnearabilities in dependencies, operate with security-focused observability, best security practices are incentivized and enforced, configure defense and remediation protocols, do breach and attack simulations, cve tracking for open source projects.

12. __Observable__ (simulate your system in the background cluster to provide a safe space for online (including penetration) testing, debugging, improvement analysis, onboarding. 2 levels of observability: (1) IT Observability: microservice-level; (2) ML Observability: application-level)

13. __Replayable__ (make shure your simulated system can be replayed from any point in time to produce the same outputs, artifacts and metrics; usefull for debugging and governance purposes. This is done via a state system that stores history of data and config states of the system + a replay job)

14. __Following Software Engineering best practices__ (standard codebase structures for projects; built-in static analysis: code smell detection, coding style checks, security checks, compliance enforcement; patterns as default; built-in different abstractions for different personas, separate clusters for different purposes.)

15. __Documentation-friendly__
    1. __Manual Documentation:__ any documentation outisde of automatic codebase documentation
        1. __Internal Documentation: for the ML Platform team__
            1. __Needs__ (subjective goals of the ml system)
            2. __Requirements__ (specific constraints and engineer preferences for the platform)
            3. __Problem Formalization__ (problem formalized in terms of ml and business KPIs, requirements and assumptions)
            4. __Design Docs__ (Solution Analysis. Alternatives to build the MLOps Platform, tradeoffs, discussions and lessons learned. Append-only doc.)
            5. __Solution Specification__ (Platform manifest (architecture with pattern distributions used, building blocks used with placeholders for custom private stuff) which makes it possible to reproduce the platform easily with Freelunch)
            6. __Risk Analysis__ (Past failures (out or in prodution), Failure modes with cause (e.g., fault trees or Failure Modes and Effects Analysis (FMEA)), how often hey happen, how bad they are, solution specification changes that make the system less risker and their tradeoffs (e.g., increase cost and latency) and methods to detect and adress the failures gracefully and in a contained manner)
        2. __External Documentation: for all ML Platform users (Applied ML Engineers, Data Scientistis, Data Engineers, subject matter experts, Stakeholders, Lawyer etc)__
            1. __Getting Started__
            2. __Tutorials__ (learning-oriented)
            3. __How-to Guides__ (goal-oriented)
            4. __Under the Hood Explanations__ (understanding-oriented)
    2. __Automatic Codebase Documentation__ (automatically generate and deploy beatifull documentation (UML-like, Service API specification, Internal Library API specification, artifact cards (model, filter, pipeline, dataset, etc), line-by-line explanation, architecture diagrams, support for free fields) from codebase)

16. __Edge-friendly__ (supports edge ci/cd (with native acceleration, data backups and OTA updates) and simulated edge deployment (simulated edge environments and wireless networks) & evaluation)

17. __GPU-friendly__ (you can use multiple backend tools that leverage GPUs)

18. __Streaming-friendly__ (supports stream transport, streaming pipelines and models)

19. __FMOps/LLMOps-supported__ (supports the lifecycle of of FMOps/LLMOps:
    1. LLM building from scratch
    2. LLM finetuning and orchestration within programs
        1. Model Alignement methods
            1. RLHF (Reinforcement Learning with Human Feedback) and RLAIH (Reinforcement Learning with AI Feedback)
            2. DPO
            3. TKO
        2. Constructing fine-tuning datasets
    3. Prompts: Experimentation, Validation, Compression, Evaluation, Automatic Prompt Generation (support formatting tricks), Automatic Prompt Adapatation (when swapping LLMs),  Breakdown of a prompt into a a prompt chain
    4. Support for Chains and Agents
        1. Building chains and agents: pipelines of models, prompters, retrievers and pre/post processors (input, prediction and/or data filters)
        2. Observing each step of chain and agent execution
    5. Support for multimodal LLMs
    6. LLM-powered dataset construcution
        1. Data Annotation
            1. For training and Evaluation
                1. Labelling
                2. Scoring (used for annotating generative model outputs)
            2. Just for training
                1. Ranking (used for annotating generative model outputs, e.g., RLHF)
        2. Synthetic data generation
    7. Prompt/LLM Evaluation
        1. By method
            1. Constructing internal test datasets
                1. Types
                    1. Representative of Production Data
                    2. Hard: Edge cases and unerrepresented cases (Lower Bound)
                2. Coverage: "high coverage: nearly every user input in production has a low-distance x in your test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant), in embeddding space"
            2. LLM-powered evaluation
            3. Using public benchmarks (e.g., ARC, MMLU, UCF101) and leaderboards (e.g., OpenLLM, Chatbot Arena, Massive Text Embedding Benchmark Leaderboard, Big Code Models Leaderboard, Text-To-Speech Synthesis on LJSpeech, Open ASR Leaderboard, Object Detection Leaderboard, Open Parti Prompt Leaderboard, Action Recognition on UCF101 Leaderboard, Text-to-Video Generation on MSR-VTT Leaderboard, Visual Question Answering on MSVD-QA Leaderboard)
        2. Supports
            1. Evaluation of LLM chains/agents
            2. Evaluation of LLM plugins (when llm gets to decide wether or not its going to use the tool in a step in the chain)
    8. Self-hosting path: *Things already mentioned* + Construting Training Datasets, Training, Deploying (including to the edge), Retraining and Operating your own LLMs
    9. Streaming LLMs (e.g., for real-time translation)
    10. LLM output
        1. Format Enforcement and Validation
        2. Uncertainty Level Identification
    11. LLM-specific Observability
        1. Monitoring metrics
        2. Observing via:
            1. Runtime checks and tests
            2. Logs
            3. Traces
            4. Simulation-based Observability  
    12. Explainability
        1. Prompt token importance
    13. Security
        1. Defense (Detection and Response) against attacks
            1. Prompt Injection
                1. Integraty attacks
                2. Privacy attacks
                3. Availability attacks (for agents)
            2. Inapropriate Outputs (Model shouldnt give inapropriate outputs (applies to Human-native data: NLP & CV & Audio). (e.g., spam, incentivizing going agaist law, slurs, pornography, etc) (Note: need ML Models doing this also, because we cant harcode for all the innapropriate scenarios)
            3. Dataset poisoning Attacks (indentifying poisoned data and/or malicous user or being robust to it)
            4. Sycophancy (when the model tailors its response to the user in detriment of the truth)
            5. Dangerous Misuse (cant allow using models to do bad stuff)
        2. Lowering Risk
            1. Influence Surface of LLMs (if your model outputs are succesfully manipulated by an attacker/troll: which other systems can get affected by this, how severe the downstream effects can be on downstream components/systems that function based on this output (directly or indirectly). E.g., currently if you are using LLMs, you want dont want to let them mess with very important data & do irreversible things)
    14. Client Code and User Interface (UI).

    Note: LLMs generally come in 4 flavours nowadays:
    1. Generative LLMs: receives sequential data as input and outputs sequential data
        1. Text output: 
            1. Text2Text: (e.g., Llama, Reflection, DBRX, Gemma, R Family, Zephyr, OLMo, MPT, Falcon, PaLM, Vicuna, Alpaca, Dolly, t5x, Mamba, Zamba, Jamba, s4, LFM, LOLA)
                1. Domain-specific:
                    1. Time-series (e.g., TimesFM, chronos-forecasting)
                    2. Language Translation (e.g., seamless_communication, NLLB/OpenNLLB)
                2. SLMs: (e.g., Llama, Phi-3, Mixtral, Gemma, etc)
            2. Audio2Text: (e.g., Whisper, SpeechGPT)
            3. Text2Audio: (e.g., bark)
            4. Audio2Audio: (e.g., VALL-E)
            5. Text&Image&Video2Text: (e.g., Vitron, Pixtral, MiniCPM, X-LLM, BakLLaVA, CogVLM, Qwen-VL, MiniGPT-4, NExT-GPT, PALM-E, OpenFlamingo, Video-LLaMA, Otter, Multimodal-GPT, AnyGPT, Orca, Phi, Macaw, Grok, VILA, Show-o, VMamba, LWM)
            6. Observation&Goal2Action: (e.g., octo, openvla)
)
        2. Image output: reiceves images and/or text and input and output images (e.g., StableDiffusion)
        
    3. Feature Extractor LLMs: receives sequential data as input and outputs a variable (e.g., BERT)
    4. Emedding LLMs: receives some type of data as inout as outputs a vector embedding that represents that input as good as possible, given the vector lenght constraint (e.g., jina-embeddings-v2, LASER, fastText)

    When using words like "prompts", "agents", "LLM-powered" and "RLHF"; we are refering only to Generative LLMs.
    )

20. __Licence-enforced__ (automatically detects dependencies, retrieves licenses, builds a file declaring licenses of dependencies and alerts you if you are using it in a waty that might go against its licence. Licences can be for code, data or models.)

21. __Lineage-complete__ (all artifacts generated (offline or online) have their lineages stored)

22. __Distributed-friendly:__ supports distributed training, tuning and inference

23. __Experimentation-friendly:__ support for notebook-based hardware-flexible experimentation environments (feature and model) for data scientists (e.g., GUI for: pipelining, formatting, diffs, exporting, etc). Data scientist can provide hardware requirements on-the-fly (e.g., machine with GPU or large memory) and a experemtation environment adapts acoordingly, while maintaining state.

24. __Artifact Experiment Tracked:__ (all experiments (different artifacts with resulting metrics) are versioned and available in a beatifull UI in Freelunch Hub. Note: repo contains pointer to experimentation environment image in image registry. Built-in support for PoCs (with Wizard-of-Oz support), these are experiments that are not meant to go to production, they are just meant to justify production efforts in certain direction (E.g., estimation of metrics improvement from PoC to stable production).)

25. __RLOps-supported:__ RL-specific stuff: (1) Simulated Environment Evaluation: latency, property verification, deployment and analysis of human episodes (for the simulated environment) to real-world deployment to see compatibility. (2) Model building: support for building RL controllers for multiple (standard and custom) environments, offline RL, Imitation Learning, Model-based RL, Causal RL; (3) Model Evaluation: total reward, security guarantees; (4) Monitoring: real-time GUI-based RL monitoring + Standard MLOps stuff.

26. __RSS-equipped__ (built-in RSS feed to keep you updated with latest ML tech: newsletters, relevant new papers/videos/articles/twitter threads, paper/discord server summarization, github tools watch, ability to build your own notes with just a few clicks and ci/cd them to a comapny or personal blog)

27. __UI-complete__ (Single UI for operations. Also, pick your UI flavour. Use GUIs when you need ease, use CLI when you need speed, use API when you need reproducibility)

28. __Online/Continual-learning supported__ (Supports deploying models that fine-tune online with production data)

29. __Hyperparameter optimization-friendly__ (support for distributed hyperparameter optimization, which essentially is multiple training wrappers deployed together that work as a team to find the best hyperparameter setting)

30. __Causality-friendly__ (supports causal models, bandit algorithms/AB testing for online experimentation, uses causal inference for alignment of ML KPIs to Business KPIs and optimal ML System parameter values (aka online adaptation) (e.g., for monitoring: thresholds and window size))

31. __PoC-supported__ (support for building PoCs with smart baselines, alpha-testing them and estimating minimum viable PoC metrics based on history of metric improvement over time of past PoCs that moved to production. Also supports wizard-of-oz pattern, where a human plays the role of the ML Model in real-time.)

32. __Equipped with Artifact Discovery__ (easily discover and get open source datasets and pretrained models from various locations (e.g., Hugging Face Hub, ONNX Model Zoo, github/gitlab/gitea (self-hosted), etc), inckuding custom-defined locations, using a single GUI)

33. __Data-mesh-supported__ (supports data mesh pattern. E.g., each sub-team can (1) easily spin up and can own a lightwieght data lake --> ml data wharehouse --> feature store, with each of these storages offering external endpoints for other teams to use, where these external endpoints have to obey a standard global specification and register them selves to a central data discovery tool. Other team use the data discovery tool to cosnume data from this team under their rules. All of this activity metadata is sent to data observability platfrom where the data mesh activities can be scrutinized. It is the job of the data platform team to build an mantain: generalized ci/cd that allows (1), data discovery tool, data observability platform (also operate it).)

34. __Equipped with helpdesk capabilties__ (provides built-in support for getting user feedback/requests from you frontends, storing and analyzing them via a ticketing system. Can be used in alpha/beta tests and/or general public deployments. The module also makes it easy for engineers to provide async support for users (answer FAQs, forumns, slack/discord, direct channel). Finally, it also makes it easy for engineers to have real-time chat with users.)

35. __RSS-equipped__ (built-in RSS feed to keep you updated with latest ML tech: newsletters, relevant new articles/twitter threads, paper/discord server summarization, github/gitlab/gitea (self-hosted) tools watch, ability to build your own notes with just a few clicks and ci/cd them to a personal blog)

36. __Online/Continual-learning supported__ (Supports deploying models that fine-tune online with production data)

37. __Equipped with Human-in-the-lopp (HIL) Prediction Reviewing__ (In some use-cases (e.g, high-stakes and not latency-bound) it is a good pattern to a human review the prediction when the model is not shure (model need to be calibrated))

38. __Equipped with Alerts, Notifications and Automated Frontend PRs__ (alerts and notifications can be triggered by various events. These can be sent via variosu channels (e.g., slack and email). You can also trigger PRs to your frontends in response to changes in the ML API gateway.)

39. __Equipped with User-based Resource Allocation__ (setting experimentation compute quotas for practioners. After quota is reached, practioner looses his compute preference. User with high preference can either get preference in line or get more compute dynamically provisioned for him.)

40. __Equipped with Activity Logging and Clear Ownership__ (store all activities done by practitioners in the platform in a strctured way. Also, each component of the pipeline has a set of owners (people who built them). Managers/leads then can query and visualize this data in a nice UI.)

41. __Equipped with automated Monitoring actions__ (automatically trigger monitoring actions (e.g., retraining) when metrics reach certain tresholds or on a schedule)

42. __Equipped with application-level caches__ (support for client-side caches and server-side caches)

43. __Onboarding-friendly__ (makes onboarding a simple and automated process)

44. __Data Labelling-friendly__ (support for manual, semi-automatic (programmatic labelling and proxy labels) and automatic labelling (via ML models)within your workflows + seamless collaboration among annotators + versioning + governance + "quality control mechanisms such as inter-annotator agreement analysis, review workflows, and data validation checks". Also support for specific data modalities such as: text, images, audio and video)

45. __Synthetic Data-friendly__ ((1) Data Augmentation: support for manual, semi-automatic (transform functions) and automatic augmentation (via LLMs and/or Stable Diffusion) within your workflows; (2) For CV: simulator data; (3) For NLP: LLM-genereted data)

46. __Equipped with Frontend Support__
    1. Content Management System (CMS) (important for: (1) automated frontend adaptation in response to backend changes; (2) easy UI/UX changes; (3) Automated UI testing;)
    2. Frontend Mocker (a minimal frontend package with the mlops platform that emulates the real frontends. The frontends can be: (1) Browser; (2) Desktop; (2) Mobile; (3) Agent Simulation Environment; (4) VideoGame; (5): Embedded System.)
    3. Screenshot-to-code (enables you to take a screnshot of a website ang get the code for it, usefull for getting started quickly with a nice solution)

47. __Equipped with support for internal packages__ (an "Internal PyPi" for you to post and get internal packages)

48. __Equipped with support for both monorepo and multirepo__

49. __Support for all major data modalities and multimodal workflows__
    1. Tabular
    2. Graphs
    3. Text
    4. Audio
    5. Computer Vision
        1. Image
        2. Video without Audio
        3. Video
    6. Signals/Time-series
    7. Spatial-temporal
        1. 2D-time (e.g., geospatial)
        3. 3D-time (e.g., point clouds or molecules)

50. __Equipped with a Production Bug Queue__ (where bugs can be added, removed, edited, handled and resolved by on-call engineeers via a ticketing system.)

51. __Equipped with Ready Environments that can be customized and reproduced by all team members__ (IT dev environment, ML Infra dev environment, Data Engineering dev environment, feature/model experimentation environment and security testing experimentation environment. All packaged as container, pod>container or python venv. Dev environment contain a Project Editor (can edit project structure and upload a snapshot of the experimentation environment as a new updated experimentation environment image to a image registry, we update the dockerfile for you accordingly)
)

52. __Equipped with Dotfile Management__ (symlinks setup, encryption, secrets manager and machine-adptable templates)

53. __Equipped with Configuration and Shema Versioning (of schema itself and schema expectations)__ 

54. __Equipped with Version Bumping__ (upgrading package versions when a new one is avalable)

55. __Infrastructure-agnostic__ (support local, on-premise cluster, private & public cloud and multi-cloud clusters)

56. __Equipped with AutoML__ (automatic dataset construction and model building from raw data as a head-start/baseline that can be further edited)

57. __Equipped with Statefull Monitoring__ (e.g., sequential calls for a streaming LLM, calls for a RL controller, calls to Statefull ML Services such as LLM services with finetuning and long-term memory)

58. __Non-capability-redundant__ (avoids doing the same capability in diffferent places unnecessarily. Many mlops tools offer overlapping capabilities and it is easy for you to end up with redundant processing that sows you down and increases costs. E.g., your feature store and you observability system might do the same data validations if you are not carefull; or your model deployment tool and observability system both monitor online experiments (e.g. ab tests or shadow deployments))

59. __Equipped with golden-path support__ (easily create, store, edit, get and use golden-paths. Golden paths are basically smart defaults + macros that should give you an 80% solution right off the gate (e.g. for deploying a statefull service) using standard best practices. You can create macros easily from engineer ops activity by extracting and refining pieces of the activity history (e.g., retraining is not natively supported by the platform you created, but an engineer spots the necessity to retrain and does the necessary actions to have a model retrained on fresh data in production. This acitivity was logged in a structrued way that makes it easy to convert the activity into a reproducible macro) and then build a workload out of it (service or task) and extend your platform with this new capability that now becomes just 1 click or 1 line of code for the next engineer)

60. __Layered__ (every Freelunch-built mlops platform has two working layers: (1) High-level and (2) Low-level. High-level is easy to use, provides nice abstractions over underlying infrastrcture and smart defaults; ideal for getting things to work. Low-level requires more experience and is for engineers that need to extract the maximum juice out of their mlops platform for their use cases, ideal for making improvements down the road.)

61. __UI-customizable__ (you can edit the operations center UI to suit your preferences, via no-code, config or code)

62. __Equipped with new tools__
    1. The first Scientific ML Framework: [Uli](__*Uli*:-the-first-non-differentiable-ML-framework.__)__ (when you want to leverage domain knowledge and a GUI for builiding ML models, you use uli, not pytorch. However, uli is backwards compatible with pytorch, ensuring the same pytorch experience.)
    2. Model Scaling Law Estimator: a model that takes as input your ML model architecture specification and estimates its scaling law, this enables experiemntation of different architectures and methods in small scale and then transfer it to large scale with success.
    3. Model Stamper:
        1. Training data prover: makes a computational proof that your model M was trained on training dataset D. This is usefull to assure your model was not edited maliciously.
        2. Authour singature: cryptogtaphically sign your models.
    4. Predictive Pod Horizontal Autoscaler

63. __Equipped with GUI-to-config__ (anyhting you do in a GUI can be converted to reproducible config. 2 options: (1) Dev: export result as config and then edit the resulting config as necessary; Ops: record activities then edit the resulting config as necessary)

64. __Equipped with Platform Namespaces__ (this is usefull in 2 situations: a comapsny wants to give the same platform functionality to separated ml teams (each ML team lives within a namespace) or a company that is building a managed mlops platform (each customer of theirs lives within a namespace).)

65. __Equipped with an easy integration testing framework__ ((1) inverted integration testing: instead of writing integration tests, the platform automatically writes multiple tests for you, and then you either visually inspect the result or write an output validator to make it into a real integration test; (2) you can also write your own integration tests like you write unit tests; (3) you can put fast and/or crucial integration tests to run in ci/cd and the rest on the background evaluation/observability cluster.)

66. __That can make ML Systems with a single APIs__ (Note: this approach should not be used if you are exposing a public API to your ML System. Data ingestion, configuration, inference and querying can be done via a single API, and then have an API per use job underneath. This works by using an API gateway that traslates the single-API calls to actual data ingestion and inference requests, using different APIs.)

</details>

<details>
    <summary><b>Tool ecosystem</b></summary>

### Tool ecosystem

<details>
    <summary><b>Tools Freelunch uses</b></summary> 

### Tools Freelunch can use/integrate with.

__*Freelunch* works on top of all these (open source and proprietary) IT, DevSecOps, Data Engineering, MLOps, Analytics and Graphics Programming tools:__  *d2*, *Diagrams*, *Draw.io*, *Miro Board*, *excalidraw*, *mermaid.js*, *Structurizr*, *PlantUML*, *obsidian*, *penpot*, *VSCode*, *vscode-debug-visualizer*, *code-server*, *&Coniniue*, Linux Distributions (*Ubuntu*, *Debian*, *Fedora*, *Talos*), distrobox, Embedded Linux Distributions (*OpenBMC*, ), *systemd*,  *spack*, *GDB*, *eBPF*, *ecapture*, *GNU parallel*, *fd*, *wtf*, *rush*, *pigz*, *curl*, *HTTPie CLI*, *unzip*, *starship*, *git*, *github*, *gitlab*, *gitea*, *Ansible, *Terraform*, *Terragrunt*, *terramate*, *terranetes-controller*, *typhoon*, *Pulumi, Crossplane*, *nitric*, *OpenTofu*, *klotho*, *Terraformer*, *Brainboard*, *AWS CloudFormation*, *Hashicorp Packer*, *laf*, *Go CDK*, *Puppet*, *Chef*, *SaltStack*, *OpenTofu*, *warewulf*, *Virtual Box*, *VMWare*, *Vagrant*, *Neutron*, *KubeVirt*, *Harvester*, *Kata Containers*, *APT*, *DNF*, *Zypper*, *Nix*, *yum*, *chocolatey*, *yarn*, *homebrew*, *apt-get*, *fpm*, *LXD*, *paru*, *chezmoi*, *pixi*, *Python*, *fastcore*, *asyncio*, *venv*, *pyenv*, *pipx*, *uv*, *manylinux*, *ptpython*, *hy*, *coconut*, *more-itertools*, *returns*, *Rust*, *tokio*, *Zig*, *Java*, *Golang*, *gf*, *Dart*, *testify*, *lo*, *Scala*, *zio*, *Monix*, *C++*, *googletest*, *cpputest*, *Catch2*, *doctest*, *palanteer*, *mimalloc*, *imgui*, *Cosmopolitan Libc*, *gdb*, *qira*, *rr*, *Julia*, *Flux.jl*, *Javascript*, *es-toolkit*, *V8*, *Node.js*, *Deno*, *npm*, *bun*, *Jest*, *jasmine*, *Express.js*, *hono*, *poyro*, *Mojo*, *Erg*, *vlang*, *taichi lang*, *Codon*, *gpu.cpp*, *OpenGL*, *Vulkan*, *Metal*, *D3D12*, *DirectX*, *CUDA*, *cccl*, *nccl*, *LibreCUDA*, *Triton Lang*, *Bend*, *chapel*, *Halide*, *ThunderKittens*, *Futhark*, *gpu-operator*, *gpustat*, *Kokkos*, *wgpu*, *dawn*, *Emu*, *vGPU_Unlock*, *blis*, *cuBLAS*, *OpenBLAS*, *ROCm*, *HIP*, *Metal*, *MSL*, *MPS*, *OpenCL*, *pyopencl*, *HVM/Blend*, *OpenMP*, *oneTBB*, *HIP*, *OpenACC*, *Taskflow*, *oneTBB*, *Kompute*, *poptorch*, *make*, *remake*, *just*, *pyInvoke* *snakemake*, *CMake*, *Gradle*, *Maven*, *turborepo*, *Bazel*, *fastlane*, *buildozer*, *python-for-android*, *localsend*, *Meson*, *Ninja*, *dagger*, *koin*, *Swinject*, *Fx*, *DependencyInjection Component*, *TypeDI*, *dig*, *KodeinDI*, *Fruit*, *Needle*, *SimpleInjector*, *Injector*, *DI*, *Ant*, *aiomultiprocess*, *gRPC*, *Finagle*, *Thrift*, *Numpy*, *cuda-python*, *Numba*, *MatX*, *mlx*, *tensorstore*, *Scipy*, *statsmodels*, *Cupy*, *ArrayFire*, *matplotlib*, *seaborn*, *facets*, *bokeh*, *folium*, *holoviz*, *nomic*, *pygwalker*, *Altair*, *optimus*, *scrub*, *dataprep*, *img2dataset*, *augmentoolkit*, DataGradients*, *FastDup*, *dedupe*, *zingg*, *YData Synthetic*, *Gretel Synthetics*, *SVD*, *AugLy*, *nlpaug*, *TextAttack*, *FoleyCrafter*, *pyaudio*, *python-sounddevice*, *playsound*, *simpleaudio*, *winsound*, *winutil*, *python-sounddevice*, *pydub*, *wavio*, *soundfile*, *torchaudio*, *pyAudioAnalysis*, *librosa*, *audiomentations*, *OpenVoice*, *albumentations*, *inflect*, *textaugment*, *kornia*, *batchgenerators*, *synthea*, *temporian*, *pgmpy*, *pyAgrum*, *BayesFlow*, *bayespy*, *Stan*, *pyro*, *PyMC3*, *pomegranate*, *Scikit Learn*, *cuML*, *hyperlearn*, *NetworkX*, *cuGraph*, *mapie*, *human-learn*, *sklearn-expertsys*, *XGBoost*, *TextBlob*, *SpaCy*, *scispacy*, *spacy-llm*, *PaddleNLP*, *opennlp*, *CoreNLP*, *pyLDAvis*, *scattertext*, *flair*, *XGBoost*, *Catboost*, *deepdetect*, *Bayesian Optimization*, *BoTorch*, *Keras*, *Pytorch (Plain, Lightning & Ignite)*, *Tensorflow*, *sonnet*, *HF Transformers*, *ml-4m*, *tiktoken*, *LASER*, *fastText*, *sentence-transformers*, *clip-as-service*, *mmpretrain*, *JAX*, *Endia*, *FLAX*, *TRAX*, *Penzai*, *MNN*, *Volwpal Wabbit*, *River*, *scikit-multiflow*, *Burn*, *Candle*, *CoreNet*, *DL4J*, *dlj*, *BeyondML*, *mlpack*, *flashlight*, *MindSpore*, *Paddle Paddle*, *Oneflow*, *Pytorch3D*, *Kaolin*, *Dopamine*, *ReAgent*, *TensorTrade*, *PARL*, *acme*, *pymarl*, *DI-engine*, *MALib*, *WarpDrive*, *OpenRL*, *detectron2*, *MMDetection*, *Lightly SSL*, *timm*, *LAVIS*, *CV-CUDA*, *supervision*, *ImageAI*, *cupoch*, *deformgs*, *champ*, *tsai*, *darts*, *kats*, *Time-Series-Library*, *hierarchicalforecast*, *gluonts*, *skforecast*, *pydlm*, *biotite*, *NoLabs*, *deepchem*, *fairchem*, *pyscf*, *pennylane*, *QuantumKatas*, *lifelines*, *devspace*, *che*, *nocalhost*, *flox*, *okteto*, *telepresence*, *gitpod*, *Tilt*, *coder*, *daytona*, *Github Codespaces*, *envd*, *Kubeflow Pipelines*, *Kubeflow Model Training Operator*, *ZenML*, *mlinfra*, *mlstacks*, *CLAIMED*, *sqlflow*, *SuperDuperDB*, *mindsdb*, *lotus*, *Dataherald*, *Chat2DB*, *Calcite*, *postgresml*, *cube*, *vespa*, *Snowflake Cortex*, *MLFlow*, *ClearML*, *oss-mlops-platform*, *TFX*, *Xorbits*, *Kong AI gateway*, *gateway*, *LiteLLM*, *glide*, *Unify*, *BricksLLM*, *Paddler*, *llm-gateway*, *felafax-gateway*, *Martian*, *Not Diamond*, *RouteLLM*, *axflow*, *DB-GPT*, *LLMStudio*, *languagemodels*, *entropix*, *agenta*, *GPTCache*, *LLMLingua*, *Octopii*, Open Source LLMs (Text: *BERT*, *Llama*, *Reflection*, *DeepSeek*, *LLM360*, *OLMo*, *Gemma*, *R Family*, *Zephyr*, *RWKV-LM*, *Persimmon*, *OLMo*, *TinyLlama*, *MobileLLM*, *Mistral*, *OpenMoE*, *MPT*, *Falcon*, *Falcon Mamba*, *PaLM*, *Vicuna*, *Alpaca*, *Dolly*, *t5x*, *RWKV-LM* (*rwkv.cpp*), *Mamba*, *VMamba*, *zamba*, *Jamba*, *s4*, *LFM*, *LOLA*, *matmulfreellm*, *LWM*, *Extended Mind Transformers*; Code: *Codestral Mamba*; Video: *JEPA*, *InternVideo*; Multimodal: *Vitron*, *MiniCPM*, *Pixtral*, *DBRX*, *X-LLM*, *BakLLaVA*, *CogVLM*, *Qwen-VL*, *MiniGPT-4*, *NExT-GPT*, *PALM-E*, *OpenFlamingo*, *Grok*, *Video-LLaMA*, *Otter*, *Multimodal-GPT*, *AnyGPT*, *Orca*, *Phi*, *Macaw*, *VILA*, *Show-o*), *SentencePiece*, *PyTorch-StudioGAN*, Managed Generative ML/LLM Services (e.g., *OpenAI Assistants API*, *Gemini*, *Midjourney*, *DALL-E*, *SORA*, *Azure Open AI*, *Claude*, *Nova*, *Cohere*, *Suno*, *PLayHT*, *ElevenLabs*, *Resemble.AI*), *ai-artifacts*, *GodMode*, *Jsonformer*, *JuJu*, *Merlin*, *DeepRec*, *NVIDIA NeMO*, *NeMo-Aligner*, *alignment-handbook*, *FastChat*, *Pipecat*, *Rasa Open Source*, *Lobe Chat*, *TensorRT-LLM*, *TransformerEngine*, *ipex-llm*, *ray-llm*, *Superagent*, *Cheshire Cat*, *Botonic*, *Tock*, *wechaty*, *ffcv*, *Horovod*, *Distributed Data Parallel (DDP)*, *Petastorm*, *Colossal AI*, *DeepSpeed*, *training-operator*, *Composer*, *Ray Train*, *pache Singa*, *trlx*, *GPT-NeoX*, *Megatron*, *megablocks*, *Paddle*, *h2o-3*, *Spark MLib*, *Spark NLP*, *Spark GraphX*, *Dask Distributed Training*, *Analytics Zoo*, *Apache Singa*, *Ceph/rook*, *longhorn*, *DAOS*, *openebs*, *HDFS*, *Gluster*, *Lustre*, *moosefs*, *SeaweedFS*, *cubefs*, *juicefs*, *DVC*, *fds*, *xvc*, *LakeFS*, *Quilt*, *Nessie*, *Neo4j*, *JanusGraph*, *NebulaGraph*, *HugeGraph*, *ArangoDB*, *OpenCog AtomSpace*, *minio*, *storj*, *swiftstack*, *drawdb*, *postgres*, *rainfrog*, *Neon*, *pgx*, *pglite*, **graphql-engine*, *pg_graphql*, *graphql*, *Ariadne*, *graphql-js*, *directus*, *tidb*, *Mathesar*, *Dgraph*, *Redis*, *cachecloud*, *DiceDB*, *etcd*, *Vitesse*, *KV*, *foundationdb*, *minikeyvalue*, *olric*, *Ignite*, *tikv*, *Vineyard*, *CouchDB*, *pouchdb*, *MongoDB*, *Cassandra*, *HBase*, *Scilladb*, *YDB*, *CockroachDB*, *TerminusDB*, *EventStore*, *realtime*, *DeltaLake*, *LakeSoul*, *ulid*, *netron*, *modelstore*, *ModelDB*, *optscale*, *aiconfig*, *Tensorboard*, *sacred*, *Aim*, *truelens*, *Humanloop*, *ChainForge*, *Promptflow*, *AgentHub*, *ChainForge*, *mirascope*, *spring-ai*, *benchllm*, *modelbench*, *bigcode-evaluation-harness*, *llm-autoeval*, *SWE-bench*, *hallucination-leaderboard*, *pytorch-fid*, *Purple Llama*, *garak*, *PyRIT*, *VectorDBBench*, *sciml-bench*, *speech-to-text-benchmark*, *WorkArena*, *DISCOVERYWORLD*, *AgentBench*, *agent-arena*, *OSWorld*, *llm-reasoners*, *llmperf*, *prometheus-eval*, *EvalLM*, *evalgen*, *cappy*, *arena-hard-auto*, *JudgeLM*, *PandaLM*, *Auto-J*, *BotChat*, *FastEval*, *ToolTalk*, *rebuff*, *auto-evaluator*, *LLMZoom*, *moonshot*, *Inspect*, *empirical*, *alpaca_eval*, *lone-arena*, *opencompass*, *BIG-bench*, *Eureka ML Insights*, *artkit*, *chat-arena*, *Inspect*, *fasttrackml*, *MLTRAQ*, *Neptune*, *W&B*, *CometML*, *Determined AI*, *Kolena*, *Efemerai*, *Kedro*, *Skypilot*, *substratus*, *lanarky*, *lamini*, *Xorbits Inference*, *khoj*, *UFO*, *ChatGPT Reverse Proxy*, *dialog*, *aphrodite-engine*, *KubeAI*, *leptonai*, *Text Generation Inference (TGI)*, *Text Embeddings Inference*, *vLLM*, *lmdeploy*, *WebLLM*, *PowerInfer*, *FlexFlow*, *mistral.rs*, *lightllm*, *Monte Carlo*, *Qualdo*, *Metaplane*, *Talend Data Quality*, *JSON Schema*, *Great Expectations*, *Dafafold*, *DataBand*, *Soda Core*, *Anomalo*, *BigEye*, *Ataccama*, *Evidently*, *labml*, *Phoenix*, *nannyml*, *AimOS*, *Radicalbit AI Monitoring*, *OpenLLM*, *langkit*, *Openllmetry*, *Helicone*, *llm.report*, *Vibranium Dome*, *Doku*, *LangWatch*, *langtrace*, *openlit*, *agentops*, *tokencost*, *langfuse*, *pezzo*, *bisheng*, *agentsea*, *AGiXT*, *Arize AI*, *Arthur AI*, *Why Labs*, *MonaLabs*, *Aporia*, *Senser*, *Superwise*, *Cencius*, *Verta*, *Truera*, *Hugging Face Libraries (Transformers, Diffusers, PEFT, Accelerate, Optimum, AWS Trainium and Inferentia, Tokenizers, Evaluate, TRL, timm, Safetensors, Autotrain, Autotrain Advanced, Lerobot)*, *big-AGI*, *TorchScale*, *Stable Diffusion*, *stable-dreamfusion*, *Wonder3D*, *ODM*, *stable-audio-tools*, *DiT*, *Fooocus*, *photoguard*, *PaddleGAN*, *PhotoEditor*, *EditAnything*, *ImageToolbox*, *rembg*, *magic-animate*, *SadTalker*, *DynamiCrafter*, *LoRA*, *QLoRA*, *LongLoRA*, *llmware*, *artifacthub*, *HuggingFaceHub*, *Featherless.ai*, *tost.ai*, *Label Studio*, *Labelbox*, *xtreme1*, *docanno*, *cvat*, *Dataloop*, *diffgram*, *cleanvision*, *Universal Data Tool*, *refinery*, *Slicer*, *labelme*,* makesense.ai*, *aubio*, *Praat*, *prodigy*, *v7labs*, *FiftyOne*, *Superb AI*, *Scale AI*, *aquarium*, *Kili*, *SuperAnnotate*, *Encord Annotate*, *labelme*, *praat*, *evals*, *Bench*, *checklist*, *lm-evaluation-harness*, *deepchecks*, *datachecks*, *cleanlab*, *TPOT*, *Featuretools*, *tsfresh*, *RasgoQL*, *Armada*, *Joblib*, *Hamilton*, *Burr*, *Argo*, *Hera*, *Airflow*, *Dagster*, *maestro*, *mage*, *kestra*, *Cadence*, *Azkaban*, *Bacalhau*, *Dagger*, *Prefect*, *Nomad*; *zenml*, *CLAIMED*, *kedro*, *kubeflow pipelines*, *mleap*, *flyte*, *sematic*, *covalent*, *Ploomber*, *n8n*, *zapier*, *Pachyderm*, *Mage*, *Cromwell*, *Nextflow*, *Feathr*, *Hopsworks OSS*, *Chronon*, *Michelangelo Palette*, *F3*, *Featureflow*, *Griffin*, *OpenMLDB*, *auto-sklearn*, *mljar-supervised*, *PyCaret*, *SapientML*, *Ludwig*, *higgsfield*, *litgpt*, *dclm*, *openchat*, *LLaMA2-Accessory*, *aicommits*, *opencommit*, *AutoGen*, *MetaGPT*, *AgentVerse*, *swarm*, *Camel*, *ChatDev*, *Langroid*, *crewAI*, *langgraph*, *GPTSwarm*, *livekit/agents*, *ControlFlow*, *agent-zero*, *agentK*, *ADAS*, * openbb-agents*, *AutoAgents*, *LLMStack*, *Magick*, *Rivet*, *Tribe AI*, *PraisonAI*, *AppAgent*, *MobileAgent*, *zep*, *Langchain*, *Langchain.js*, *Dust*, *BondAI*, *MemGPT*, *OpenAgents*, *SuperAGI*, *Agent Pilot*, *griptape*, *phidata*, *LLMCOmpiler*, *ragapp*, *CopilotKit*, *agentkit*, *gptscript*, *Agents*, *Embedchain*, *Verba*, *Neum AI*, *gorilla*, *graphRAG*, *AutoRAG*, *Cohere Toolkit*, *R2R*, *clip-retrieval*, *HippoRAG*, *ragbuilder*, *cognita*, *composio*, *FastGPT*, *haystack*, *Langflow*, *Flowise*, *tuned-lens*, *inspectus*, *llm-viz*, *Semantic Kernel*, *DSPy*, *Zenbase Core*, *AutoPrompt*, *AdalFlow*, *KaibanJS*, *textgrad*, *Agents 2.0*, *prompt-poet*, *guidance*, *YiVal*, *sglang*, *lorax*, *orca*, *ChemCrow*, *alphafold3-pytorch*, *Chai-1*, *private-gpt*, *vanna*, *Sourcegraph*, *XAgent*, *AgentGPT*, *skyvern*, *webllama*, *robotframework*, *devika*, *AI-Scientist*, *Devin*, *Genie*, *Open Hands*, *auto-code-rover*, *Devon*, *JACoB*, *micro-agent*, *GPT Pilot*, *LoopGPT*, *foyle*, *Mentat*, *Sweep*, *aider*, *codel*, *whatsapp-chatgpt*, *JARVIS*, *gptme*, *LlamaFS*, *gpt-engineer*, *Robbie G2*, *Agentless*, *merlinn*, *smol developer*, *vectorlm*, *hackingBuddyGPT*, *cover-agent*, *fuzz4all*, *coverup*, *SWE-agent*, *RepoUnderstander*, *DevOpsGPT*, *OpenCopilot*, *DemoGPT*, *GeniA*, *AutoDev*, *Open-Assistant*, *Open Interpreter*, *Self-Operating Computer Framework*, *GPT4All*, *LocalAI*, * danielmiessler/fabric*, *chatbox*, *torchchat*, *TinyLLM*, *Jan*, *leon*, *screenpipe*, *TinyAgent*, *OpenOpenAI*, *speech-to-speech*, *Open-Sora*, *Pyramid Flow SD3*, *VideoCrafter*, *CogVideo*, *open-chat-video-editor*, *imaginAIry*, *VGen*, *Tune-A-Video*, *mmaction2*, *VADER*, *promptify*, *promptfoo*, *SoM*, *AI Test Kitchen*, *AnySolve*, *betterprompt*, *ChatGPT Prompt Generator*, *ClickPrompt*, *DreamStudio*, *Dify*, *TaskingAI*, *Dust*, *IX*, *Dyno*, *EmergentMind*, *EveryPrompt*, *FlowGPT*, *fastRAG*, *GPT Index*, GPTTools, hwchase17/adversarial-prompts, Interactive Composition Explorer, Knit, *LangBear*, *Lexica*, *LMFlow*, *loom*, *Metaprompt*, *OpenAI Playground*, *OpenICL*, *OpenPrompt*, *OpenPlayground*, *OptimusPrompt*, *Outlines*, *Faster-Outlines*, *instructor*, *lm-format-enforcer*, *Artificial Intelligence Controller Interface (AICI)*, *Playground*, *Portkey AI*, *Prodia*, *Prompt Apps*, *PromptAppGPT*, *Prompt Base*, *Prompt Engine*, *prompted.link*, *Prompter*, *PromptInject*, *Prompts.ai*, *Promptmetheus*, *PromptPerfect*, *Promptly*, *PromptSource*, *PromptTools*, *Scale SpellBook*, *sharegpt*, *SmartGPT*, *ThoughtSource*, *Visual Prompt Builder*, *YiVal*, *MLFlow Prompt Engineering UI*, *frugal*, *FLAML*, *NNI*, *AutoGluon*, *AutoTS*, *AI Scientist*, *RD-Agent*, *Vision Agent*, *StatsForecast*, *beta-recsys*, *metarank*, *kusion*, *rig*, *dyrectorio*, *Azure DevOps*, *DevOpsBox*, *OpsLevel*, *cortex*, *telepresence*, *Argonaut*, *Mia Platform*, *Shipa*, *Port*, *Upbound*, *gimlet*, *nautes*, *Crossplane*, *Backstage*, *Spinnaker*, *sheperd*, *Teller*, *Hashicorp Vault*, *Sops*, *Infisical*, *external-secrets*, *keycloak*, *Zitadel*, *ory*, *supertokens*, *OKTA*, *Auth0*, *authelia*, *defguard*, *OPA*, *keto*, *spicedb*, *permify*, *casbin*, *Teleport*, *openfga*, *warrant*, *topaz*, *Ory Kratos*, *Ory Hydra*, *dex*, *OneLogin*, *Ping Identity*, *WSO2 Identity Server*, *Duo Security*, *bytebase*, *cloudnative-pg*, *kubeblocks*, *Gaphor*, *Archi*, *Icepanel*, *Jira*, *Backlog*, *Trello*, *plane*, *Openproject*, *Actionview*, *cloudcustodian*, *xterm.js*, *sshuttle*, *Ayup*, *gliderlabs/ssh*, *sshfs*, *ssh3*, *ssh-audit*, *team-viewer*, *rustdesk*, *Cookiecutter*, *Craft*, *copier*, *Yeoman*, *mlops-python-package*, *AgentStack*, *Self-hosted AI starter kit*, *OPEA*, *llama-stack*, *AgentGenesis*, *Apollo*, *NACOS*, *Salt*, *Configu*, *OmegaConf*, *Hydra, *DynaConf*, *RSS Guard*, *rss-bridge*, *FreshRSS*, *Follow*, *rudder-server*, *next-cloud*, *Meilisearch*, *typesense*, *ElasticSearch*, *MobileAgent*, *transformers.js*, *llama-cpp-python*, *llm.c*, *llama2.c*, *nanoGPT*, *nanoGPT-mup*, *kat*, *nano-llama31*, *MobiLlama*, *Ollama*, *LM studio*, *ExLlamaV2*, *koboldcpp*, *unilm*, *Dalai*, *BigDL-LLM*, *argilla*, *bonito*, *DataDreamer*, *airoboros*, *textbook_quality*, *NeMo-Curator*, *distilabel*, *lilac*, *llm-data-creation*, *pluto*, *LLama-index*, *mem0*, *LLamalab*, *NeMo-Guardrails*, *guardrails*, *Purple Llama*, *LLaMA-Factory*, *JAX*, *FLAX*, *TRAX*, *optax*, *Mctx*, *AXLearn*, *GGML*, *KitOps*, *valor*, *llamafile*, *ctransformers*, *GGUF*, *STAX*, *Objax*, *Haiku*, *Elegy*, *Ray Core*, *Kuberay*, *SciML*, *pysindy*, *neuromancer*, *deepmd-kit*, *PyDMD*, *PyKoopman*, *PyNumDiff*, *PySensors*, *deepxde*, *FluidX3D*, *SU2*, *FreeCAD*, *openscad*, *drake*, *Seldon Core*, *BentoML*, *Ray Serve*, *KServe*, *Cortex*, *Truss*, *MLEM*, *OctoML*, *HippoML*, *NVIDIA DALI, *NVIDIA Triton Inference Server*, *TorchServe*, *exo*, *Vitis*, *ONNX Runtime*, *deepsparse*, *PPLNN*, *NVIDIA TensorRT*, *NVIDIA TensorRT-LLM*, *Intel Extension for Transformers*, *TensorRT Polygraphy*, *TensorRT onnx-graphsurgeon*, *TensorRT*, *pytorch-quantization*, *ao*, *torchao*, *BlindAI*, *Streamlit*, *Gradio*, *text-generation-webui*, *Shiny*, *Dash*, *Voila*, *nextpy*, *PyWebIO*, *mesop*, *reflex*, *appsmith*, *ToolJet*, *illa-builder*, *budibase*, *nocobase*, *Retool*, *Chainlit*, *LMFlow*, *Simple LLM Finetuner*, *Axolotl*, *unsloth*, *xTuring*, *langdrive*, *mistral-finetune*, *xtuner*, *LLM Finetuning Toolkit*, *OpenPipe*, *xTuring*, *LLaMA-Adapter*, *multimodal-maestro*, *ms-swift*, *torchtune*, *HF trl*, *trlx*, *OpenRLHF*, *DistillKit*, *Marvin*, Marvin, *TypeChat*, *Vercel AI SDK*, *text-generation-web-ui*, *AnythingLLM*, *snipe-it*, *glpi*, *Zendesk*, *ZohoDesk*, *FreshDesk*, *Zammad*, *osTicket*, *FreeScout*, *Peppermint*, *Trudesk*, *Faveo Helpdesk*, *Pagerduty*, *chatwoot*, *papercups*, *chaskiq*, *helpy*, *ort*, *OpenSCA*, *QEMU*, *Firecracker*, *katacontainers*, *Chatbot Arena*, *WindowsAgentArena*, *Radon*, *Bandit*, *Snyk*, *Intruder*, *Splunk SOAR*, *Immuniweb*, *OpenDSR*, *Drata*, *Cymulate*, *SOPHOS*, *Snyk*, *FOSSA*, *What the Diff*, *Github Dependabot*, *super-linter*, *Bump My Version*, *addlicense*, *ModelScan*, *Github Code Scanning*, *AI Reviewer*, *Codeball*, *Codacy*, IaaS (e.g., (1) Open Source: *OpenStack*, *Cloudstack*, *Cozystack*, *clutch*, *cluster-health*; (2) Managed: *NVIDIA DGX Datacenter*, *AWS EC2*, *AWS ECS*, *AWS EKS*, *AWS Fargate*, *Google GKE*, *cluster-toolkit*, *ECR Container Registry*, *genesiscloud*; AI Paas/FaaS (e.g., GPUs: FaaS: (1) they own the machines: *Lambda GPU Cloud*, *Fluidstack*, *mfem*, *CoreWeave*, *HF GPU Spaces*, *Crusoe.ai, *LambdaLabs*, *JarvisLabs*, *SambaNova*, *Juice*, *Super Micro*, *TensorDock*, *Modal*; *Shadeform*; (1.1) GPU FaaS Clients: *Moonglow*; (2) Normal People rent their machines to others: *salad*, *vast.ai*; PaaS: *Anyscale*, *Lepton AI*, *Fireworks AI*, *Groq*, *Paperspace*, *run:ai*, *Runpod*, *Radium*, *AWS bedrock*, *replicate*, *Nebius AI*; IaaS: *San Francisco Compute*; Other: PaaS: *Graphcore*)) Serverless managed offerings (e.g., *AWS Lambda*, *Azure functions*, *Baseten*, *Inferless*, *Banana*, *Pipeline*, *Slai.io*, *inferrd*, *replicate*), IT PaaS: build and deploy easily apps (e.g., Open Source: *Supabase*, *NVIDIA DGX SuperPod*, *AWS EKS*, *GCP GKE*, *fly.io*, *coolify*, *dokploy*, *Qovery Engine*, *langrunner*, *OpenFaaS*, *OpenWhisk*, *KNative*, *Nuclio*, *OpenFunction*, *Fission*, *tsuru*, *CDS: Continuous Delivery Service*, *Encore*, *PipeCD*, *Dokku*, *KubeVela*, *shuttle*, *Porter*, *piku*, *OTOMI*, *rack*, *kubero*, *kopf*, *PaaSTA*, *kapp*, *dyrectorio*, *Rig.dev*; Managed: *Firebase*, *Heroku*, *Netlify*), SaaS offerings (e.g., *AWS SNS*), *Django*, *fasthtml*, *gin*, *Spring*, *dubbo*, *Laralevel*, *Sveltekit*, *xyflow*, *redwood*, *Voyager*, *Wave*, *Laralevel Spark*, *Genesis*, *Nextjs*, *Vite*, *Astro*, *remix*, *perseus*, *caprover*, *FastAPI*, *litestar*, *Gunicorn*, *Uvicorn*, *starlette*, *Caddy*, *NGINX*, *haproxy*, *workerd*, *ModSecurity*, *Bunkerweb*, *Podman*, *Docker*, *Apptainer*, *containerd*, *runc*, *buildah*, *Moby*, *pack*, *ko*, *Porter*, *Kaniko*, *Shipwright*, *contaiNERD CTL*, *Slim*, *cadvisor*, *virtualenv*, *pex*, *Github packages*, *SLURM*, *volcano*, *Nomad*, *Harbor*, *distribution*, *cri-o*, *stackrox*, *strelka*, *trow*, *Kubernetes*, *CRI-O*, *kubectl-cost*, *kubebuilder*, *operator-sdk*, *operator-lifecycle-manager*, *testcontainers-python*, *dockertest*, *Docker-Android*, *ContainerSSH*, *Docker-OSX*, *kubernetes in docker (kind)*, *minikube*, *k3d*, *mirrord*, *vcluster*, *K3S*, *Kubeedge*, *baetyl*, *openyurt*, *shifu*, *EdgeX*, *fledge*, *thingsboard-gateway*, *Microk8s*, *ekuiper*, *Helm*, *Kustomize*, *timoni*, *cdk8s*, *kpt*, *k9s*, *OKD*, *Kubesphere*, *kluctl*, *zadig*, *cds*, *devtron*, *Lens*, *Kubeapps*, *porter-archive*, *capsule*, *Rancher*, *kubermatic*, *Karmada*, *Skaffold*, *werf*, *Flux2*, *ketch*, *sablier*, *dapr*, *Carvel*, *Garden*, *Velero*, *stash*, *OPA*, *Kubearmor*, *cert-manager*, *in-toto*, *kubecost*, *opencost*, *kube-prometheus*, *scope*, *kdash*, *skooner*, *KEDA*, *Karpenter*, *Sealed Secrets*, *Kubernetes Secrets Store CSI Driver*, *External Secrets*, *k8sgpt*, *PentestGPT*, *LaVague*, *Agent-E*, *Huginn*, *Fuji-Web*, *zapier*, *sonobuoy*, *robusta*, *preevy*, *Metacontroller*, *Twisted*, *tornado*, *asyncio*, *Celery*, *RQ*, *kueue*, *fabric*, *plural*, *Run:ai*, *Cloudflare*, *Fastly*, *Quarto*, *Hugo*, *Gatsby*, *Jekyll*, *hexo*, *tailwind-nextjs-starter-blog*, *BigQuery*, *AWS RedShift*, *Doris(, *Hive*, *Snowflake*, *Coalesce*, *Fivetran*, *Stich*, *velox*, *dbt*, *dlt*, *piperider*, *Snowpark*, *Tecton*, *dotData*, *Molecula*, *Hopsworks*, *Feast*, *Apache Arrow*, *TorchArrow*, *substrait*, *spark*, *GraphScope*, *koalas*, *RabbitMQ*, *NATS*, *cloudevents*, *ActiveMQ*, *ZeroMQ*, *Kafka*, *strimzi*, *cruise-control*, *GStreamer*, *NNStreamer*, *Pulsar*, *Redpanda*, *Pravega*, *Kinesis*, *RocketMQ*, *Brooklin*, *fireworq*, *Storm*, *Flink*, *Kafka Streams*, *Spark Structured Streaming*, *Beam*, *Pathway*, *Impala*, *NiFi*, *Benthos*, *Apache NiFi*, *Numaflow*, *KSQL*, *Materialize*, *Decodable*, *quix streams*, *hazelcast*, *community-skeleton*, *danswer*, *gerev*, *mlflow-kernel*, *Sphinx*, *Graphviz*, *pygraphistry*, *MkDocs*, *mkdocstrings*, *Material for MKDocs*, *gitbook*, *Read the Docs*, *github pages*, *lightdash*, *blazer*, *evidence*, *Salesforce*, *Power BI*, *Tableau*, *Google Analytics*, *umami*, *plausible*, *Mixpanel*, *GoReplay*, *rrweb*, *keep*, *Nagios*, *wireshark*, *sniffnet*, *GoodbyeDPI*, *SREWorks*, *zabbix*, *netdata*, *CyberChef*, *Prometheus*, *Thanos*, *VictoriaMetrics*, *Nightingale*, *Ddosify*, *fleet*, *OpenObserve*, *highlight.io*, *hyperdx*, *pixie*, *Retina*, *skywalking*, *OpenTelemetry*, (Grafana Agent*, *OpenMetadata*, *atlas*, *collectd*, *Open metrics*, *Telegraf*, *OpenLineage*, *Marquez*, *Datahub*, *Egeria*, *Aligned*, *Beats*, *coroot*, *Cilium*, *Hubble*, *Grafana Pyroscope*, *envoy*, *Istio*, *Kiali*, *Consul*, *kuma*, *linkerd2*, *flannel*, *kubeshark*, *hertzbeat*, *openclarity*, *Keptn*, *FastNetMon*, *akvorado*, *librenms*, *ThingsBoard*, *web-check*, *entef*, *Datadog*, *Sentry*, *Honeycomb*, *SignalFx*, *CloudWatch*, *New Relic*, *Librato*, *Algolia*, *Splunk*, *BigPanda*, *Sensu*, *Middleware*, *AppDynamics*, *Raygun*, *Splunk Cloud*, *eG Enterprise*, *metabase*, *plotly*, *graphana*, *Perspective*, *superset*, *amundsen*, *redash*, *ckan*, *magda*, *Genie*, *open data discovery (odd)*, *Chat2DB*, *ClickHouse*, *Apache Pinot*, *DuckDB*, *StarRocks*, *Druid*, *Materialize*, *Trino*, *presto*, *Impala*, *Apache Drill*, *EvaDB*, *Alluxio*, *Delta Lake*, *LakeSoul*, *paimon*, *Kylo*, *Apache Hudi*, *Apache Iceberg*, *PyIceberg*, *Apache Parquet*, *h5py*, *Apache Avro*, *Deep Lake*, *AIStore*, *Debezium*, *Zepellin*, *marimo*, *runme*, *Jupyter Lab*, *Jupyterlab-requirements*, *ipyflow*, *Papermill*, *Ploomber*, *jupysql*, *jupyter-ai*, *pyparallel*, *parakeet*, *testbook*, *nbdev*, *nbdime*, *elyra*, *jupytext*, *ReviewNB*, *NBQA*, *QAnything*, *paper-qa*, *lmk*, *lineapy*, *HF Evaluate*, *TF Model Analysis*, *TF Embedding Projector*, *ZenoML*, *AI Verify*, *TF Responsible AI Toolkit*, *Microsoft Responsible AI Toolkit (RAI)*, *Fairlearn*, *AI Fairness 360 (AIF360)*, *Sagemaker Clarify*, *SHAP*, *InterpretML*, *Alibi Explain*, *AI Explainability 360*, *TF What-If*, *interpret*, *Captum*, *TF Lattice*, *nnsight*, *gemma-scope*, *OmniXAI*, *TF language interpretability*, *ecco*, *Pythia*, *maia*, *sae-auto-interp*, *automated-interpretability*, *DeepCausality*, *Causal ML*, *dowhy*, *causalnex*, *Learning Interpretability Tool (LIT)*, *PiML*, *Adversarial Robustness Toolbox (ART)*, *Foolbox*, *Giskard*, *XAI Toolbox*, *Pysift*, *tf federated*, *flower*, *nvidia clara*, *substra*, *OpenFL*, *FATE*, *FedML*, *PipelineDP*, *OpenCV*, *SuperGradients*, *NVIDIA DeepStream SDK*, *Savant*, *espnet*, *Mermaid*, *Icevision*, *detectotron2*, *PaddleDetection*, *Segmentation Models*, *Grounding DINO*, *segment-anything*, *co-tracker*, *mmsegmentation*, *Kornia*, *ocrpy*, *faceswap*, *GPT-SoVITS*, *Behave*, *Lettuce*, *Robot*, *Scalene*, *Pytest*, *Unittest*, *Coverage*, *Codecov*, *JaCoCo*, *Hypothesis*, *JUnit*, *pycodestyle*, *moto*, *ruff*, *pyright*, *yapf*, *pylint*, *sqlfluff*, *Black*, *flake8*, *MyPy*, *returns*, *injector*, *mock*, *faker*, *pyre-check*, *shellcheck*, *MonkeyType*, *beartype*, *Pydantic*, *jaxtyping*, *schema*, *pandera*, *deequ*, *TF Data Validation*, *DVT*, *data-diff*, *cerberus*, *ydata-profiling*, *unstructured*, *unstract*, *docetl*, *DataTrove*, *docling*, *sparrow*, *ryoma*, *orjson*, *marker*, *pycln*, *pyupgrade*, *Bandit*, *cloudpathlib*, *SonarQube*, *Nessus*, *SpotBugs*, *PMD*, *gosec*, *codeql*, *gitleaks*, *nodejsscan*, *pino*, *bearer*, *scorecard*, *criticality_score*, *semgrep*, *webhint*, *BinAbsInspector*, *reko*, *ImHex*, *jpexs-decompiler*, *python-decompile3*, *LLM4Decompile*, *Recaf*, *decompiler-explorer*, *compiler-explorer*, *trivy*, *grype*, *Clair*, *neuvector*, *Anchore Engine*, *Sysdig*, *Dagda*, *isort*, *Airbyte*, *syncthing*, *SeaTunnel*, *skyplane*, *Fivetran*, *Stitch*, *ingestr*, *lineapy*, *cog*, *chassis*, *chitra*, *crane*, *finetuner*, *Notion*, *notion-clone*, *siyuan*, *super-productivity*, *platform*, *papermark*, -*reor*, *Logseq*, *dendron*, *AppFlowy*, *AFFiNE*, *plane*, *TVM*, *iree*, *HF Optimum*, *GPTFast*, *FlexiGen*, *XLA*, *voltaML*, *TF Model Optimization Toolkit*, *TF Lite*, *executorch*, *uTensor*, *Intel Neural Compressor*, *Adlik*, *AITemplate*, *MLC LLM*, *mnn-llm*, *Hidet*, *distiller*, *sparsify*, *OnnxSlim*, *sparseML*, *AIMET*, *TinyNeuralNetwork*, *HAWQ*, *quanto*, *AWQ*, *SmoothQuant*, *VPTQ*, *AutoGPTQ*, *cold-compress*, *Encord Active*, *modAL*, *libact*, *ALiPy*, *Baal*, *TenSEAL*, *google/differential-privacy*, *TF Privacy*, *SEAL*, *HElib*, *tf-encrypted*, *HEFlow*, *concrete-ml* *Presidio*, *molfeat*, *AWS EC2 Inf2*, *botui*, *bottender*, *pandas*, *pingouin*, *pandas-ai*, *WrenAI*, *quivr*, *localGPT*, *kotaemon*, *DocsGPT*, *KnowledgeCanvas/knowledge*, *pdftochat*, *Modin*, *Polars*, *vaex*, *ibis*, *Dask*, *Modin*, *mars*, *cuDF*, *Vaex*, *Daft*, *winglang*, *Odigos*, *Jaeger*, *Zipkin*, *Elastic APM*, *py2cfg*, *gensim*, *PyOD*, *Darts*, *Anomalib*, *Anomstack*, *Snorkel*, *Compose*, *Autolabel*, *Adala*, *vectorflow*, *superlinked*, *Text Embeddings Inference*, *infinity*, *pykeen*, *pgvector*, *Weaviate*, *Chroma*, *Milvus*, *Qdrant*, *HNSWLib*, *NMBSLib*, *Active Loop*, *lancedb*, *Marqo*, *Faiss*, *semantra*, *vearch*, *Vald*, *indexify*, *docarray*, *Argo Rollouts*, *flagger*, *Posthog*, *Growthbook*, *Flipt*, *Flagr*, *Optimizely*, *AB Tasty*, *Apptimize*; *Google Optimize*, *Stumpy*, *InfluxDB*, *TDengine*, *TimescaleDB*, *FiloDB*, *FlashDB*, *opentsdb*, *Prophet*, *Grafana Loki*, *Greylog*, *Logstash*, *spdlog*, *vector*, *monolog*, *FluentD*, *loguru*, *quill*, *structlog*, *flume*, *goaccess*, *Logseq*, *logparser*, *Pyomo*, *openblocks*, *Screenshot-to-code*, *draw-a-ui*, *memray*, *py-spy*, *pprof*, *coz*, *Bytehound*, *Stopwatch*, *async-profiler*, *gradle-profiler*, *line_profiler*, *orbit*, *hotspot*, *easy_profiler*, *puffin*, *Wordpress, Strapi*, *Ghost*, *Decap*, *wagtail*, *Keystone*, *Payload*, *Chromium*, *Publii, Drupal*, *Drupal*, *agentlabs*, *OpenCLIP*, *CLAP*, *ImageBind*, *AnyGPT*, *stable-diffusion*, *mmagic*, *SDXL-Turbo*, *StableStudio*, *Flux*, *DiffSynth-Studio*, *ComfyUI*, *SillyTavern*, *chathub*, *tldraw*, *IOPaint*, *diffusion-forcing*, *servo*, *OnionBrowser*, *Tor*, *ladybird*, *filebrowser*, *Cypress*, *Selenium*, *Playwright*, *Taxy AI Full Browser Automation*, *webdriverio*, *Testcomplete*, *Airtest*, *momentic*, *openv0*, *Langui*, *AI Spend*, *Swagger UI*, *Github Actions*, *Jenkins*, *gocd*, *woodpecker*, *CircleCI*, *Bamboo*, *Travis CI*, *Buddy*, *Buildbot*, *Bitbucket Pipelines*, *AWS CodePipeline*, *Tekton*, *TeamCity*, *ArgoCD*, *ArgoCD*, *Spinnaker*, *Buck*, *pygradle*, *python-fire*, *argparse*, *Typer*, *sqlmodel*, *ELK on Docker*, *OpenSearch*, *Solr*, *Sphinx Search*, *Xapian*, *Nutch*, *bm25s*, *datasketch*, *git-code-debt*, *Sandstorm*, *zuul*, *Kong*, *Traefik*, *higress*, *SafeLine
*, *SafeLine*, *Unkey*, *unicorn*, *capstone*, *reconftw*, *OWASP Nettacker*, *BlackWidow*, *retire.js*, *minisearch*, *WhiteSource Bolt*, *Dependency-Track*, *OSSIndex*, *System Informer*, *BinAbsInspector*, *x64dbg*, *Santa*, *nuclei*, *flan*, *checkov*, *Vuls*, *Tsunami*, *grype*, *VeraCrypt*, *cryptomator*, *gophish*, *URH - Universal Radio Hacker*, *Aircrack-ng*, *WireGuard*, *tailscale*, *openvpn*, *amnezia-client*, *nebula*, *IPsec VPN Server on Docker*, *netmaker*, *MVT - Mobile Verification Toolkit*, *Kubescape*, *Gatekeeper*, *Kyverno*, *kube-score*, *yamllint*, *TOML*, *Carvel*, *kubesec*, *kubeaudit*, *kubeconform*, *kube-linter*, *tflint*, *polaris*, *conftest*, *datree*, *kubevious*, *Datashim*, *OSS-Fuzz*, *ClusterFuzz*, *prowler*, *Docker Bench for Security*, *nmap*, *vulscan*, *IVRE*, *Wazuh*, *OpenVAS*, *ThreatMapper*, *CrowSec*, *ossec-hids*, *Sn1per*, *secretive*, *Falco*, *IntelOwl*, *Watcher*, *metlo*, *hackingtool*, *it-tools*, *securityonion*, *Pacu*, *capa*, *cve-bin-tool*, *cve*, *tfscan*, *terrascan*, *ggshield*, *CloudSploit*, *kube-bench*, *pyWhat*, *Arkime*, *steampipe*, *cloudmapper*, *ntopng*, *systeminformer*, *osquery*, *Zeek*, *Dependency-Check*, *maltrail*, *x11docker*, *authentik*, *Calico*, *Osmedeus Core Engine*, *MISP - Threat Intelligence Sharing Platform*, *Faraday*, *TheHive*, *OWASP DefectDojo*, *OpenCTI*, *Ockam*, *cameradar*, *Suricata*, *Arachni*, *Meshbird*, *ufw-docker*, *CDK - Zero Dependency Container Penetration Toolkit*, *pocsuite3*, *Step CLI*, *Oso*, *Seatbelt*, *drozer*, *DefectDojo*, *SSLyze*, *tracee*, *parca*, *Interactsh.*, *Chef InSpec*, *CHIPSEC*, *Lynis*, *OpenSCAP*, *ScoutSuite*, *node-rate-limiter-flexible*, *OWASP Zed Attack Proxy (ZAP)*, *Nikto*, *Wapiti*, *Grabber*, *mitmproxy*, *Algo VPN*, *IPsec VPN Server Auto Setup Scripts*, *NGINX Config*, *bettercap*, *cutter*, *wifiphisher*, *JavaScript obfuscator*, *DOMPurify*, *RustScan*, *dirsearch*, *Vuls*, *Caldera*, *Shuffle*, *PhoneSploit Pro*, *shodan*, *keeweb*, *Protobuff*, *Prisma*, *pgroll*, *goose*, *gorm*, *typeorm*, *Sequelize*, *mongoose*, *sqitch*, *flyway*, *Pyrseas*, *liquibase*, *Planetscale*, *cibuildwheel*, *Poetry*, *rye*, *pyo3*, *Maturin*, *Twine*, *PySnooper*, *coremltools*, *ncnn*, *ShaderNN*, *mediapipe*, *HyperOpt*, *Optuna*, *SigOpt*, *RayTune*, *Katib*, *Overleaf*, *Stencila*, *novel*, *vnote*, *Notable*, *Joplin*, *memos*, *Laverna*, *Notesnook*, *MarkText*, *Stripe*, *hyperswitch*, *Checkout.com*, *browserify*, *Storybook*, *One*, *htmx*, *solid*, *mitosis*, *Lucide*, *React*, *react-starter-kit*, *openv0*, *open-saas*, *Preact*, *Bulletproof React*, *react-figma*, *intro.js*, *kivy*, *React Native*, *Hippy*, *reactotron*, *lit*, *biome*, *Vue*, *Svelte*, *flipper*, *Ionic*, *capacitor*, *Expo*, *Electron*, *Pake*, *Tauri*, *Acorn*, *OpenUI*, *tiptap*, *Tailwind*, *vegeta*, *locust*, *gatling*, *k6*, *jmeter*, *Chaos Monkey*, *chaos-mesh*, *chaosblade*, *lLitmusChaos*, *kube-monkey*, *gum*, *go-jsonnet*, *CUE*, *LibreTranslate*, *Argos Translate*, *upscayl*, *yake*, *tesseract*, *Umi-OCR*, *EasyOCR*, *OCRmyPDF*, *PaddleOCR*, *surya*, *VERBI*, *tts*, *fish-speech*, *IMS-Toucan*, *ChatTTS*, *Parler-TTS*, *GPT-SoVITS*, *EmotiVoice*, *MARS5*, *MetaVoice*, *Deepspeech*, *riffusion*, *muzic*, *magenta*, *riffusion-hobby*, *riffusion-app-hobby*, *AudioLDM2*, *musegan*, *Suno-API*, *Amphion*, *audiocraft*, *spleeter*, *voice-changer*, *Retrieval-based-Voice-Conversion-WebUI*, *open-musiclm*, *musiclm-pytorch*, *Ultralytics YOLOv8*, *yolov10*, *Github Copilot*, *cody*, *fauxpilot*, *claude-dev*, *continue*, *privy*, *cline*, *cursor*, *void*, *coffee*, *melty*, *zed*, *tabby*, *LSP-AI*, *claude-engineer*, *StarCoder*, *Yi-Coder*, *Toolformer*, *ToolBench*, *AgentCoder*, *OpenCodeInterpreter*, *AlphaCodium*, *pr-agent*, *Sweep*, *magnet*, *unblocked*, *Onboard*, *aider*, *Butterfish*, *Second*, *TFLite*, *MLKit*, *CoreML*, *Pytorch Mobile*, *TF.js*, *ai-jsx*, *WebAssembly*, *WebGL*, *WebGPU*, *Rive*, *WebSocket*, *WebRTC*, *LiveKit*, *Edge Impulse*, *EdgeCloudSim*, *EnvisEdge*, *Auptimizer*, *firesim*, *OpenFPGA*, *apio*, *icestorm*, *yosys*, *nextpnr*, *Logisim-evolution*, *Digital-Logic-Sim*, *Amaranth*, *PlatformIO*, *LiteX*, *LGN*, *whisper.cpp*, *whisper-diarization*, *WhisperKit*, *SpeechGPT*, *bark*, *wavify-sdks*, *gpt-aria*, *TimesFM*, *chronos-forecasting*, *seamless_communication*, *NLLB/OpenNLLB*, *llama.cpp*, *gpt-fast*, *codellama*, *ml-design-doc*, *Youtube Transcript API*, *OutfitAnyone*, *adr-tools*, *postal*, *EasyEdit*, *mergekit*, *FlexGen*, *Medusa*, *LookaheadDecoding*, *einops*, *safetensors*, *Diataxis*, *ROS2*, *home-robot*, *OpenBot*, *curobo*, *IsaacLab*, *jhonny-five*, *gobot*, *Godot*, *GDevelop*, *bevy*, *Babylon.js*, *cocos-engine*, *libgdx*, *MonoGame*, *o3de*, *Unreal Engine*, *MindMaker AI Plugin for Unreal Engine*, *Unity*, *Unity ML-Agents Toolkit*, *Huggy*, *AnimateAnyone*, *Swirl*, *Devv*, *Algolia*, *Coveo*, *searxng*, *SolidGPT*, *youtube-dl*, *Qt*, *Airtable*, *NocoDB*, *teable*, *aframe*, *univer*, *quadratic*, *sheetjs*, *visidata*, *Perplexity*, *Rubik's AI*, *You*, *Capitol AI*, *Perplexica*, *morphic*, *FreeAskInternet*, *MindSearch*, *OpenPlexity-Pages*, *stanford-oval/storm*, *farfalle*, *Noi*, *cvxpy*, *pulp*, *BayesianOptimization*, *ceres-solver*, *timefold-solver*, *osqp*, *nlopt*, *ScrapeGraphAI*, *Lumos*, *firecrawl*, *scrapy*, *Crawlee*, *crawl4ai*, *EasySpider*, *parsera*, *beatifull soup*, *colly*, *cheerio*, *orbitdb*, *ipfs*, *etherium*, *PySR*, *pystruct*, *neurosym*, *alpha-beta-CROWN*, *nnenum*, *VeriNet*, *Lyapunov_Stable_NN_Controllers*, *Verified Software Toolchain*, *pysmt*, *fastsmt*, *Gradient Free Optimizers*, *or-tools*, *sympy*, *langdiversity*, *PyReason*, *LNN*, *Problog*, *DeepProbLog*, *SATNet*, Unique3D*, *Meshroom*, *invesalius3*, *Open3D*, *Online3DViewer*, *nerfstudio*, *yt-dlp*, *convex_adversarial*, *Energy-Languages*, *torch2chip*, *TF-Agents*, *ReAgent*, *OpenSpiel*, *OpenAI Gym*, *Gymnasium*, *poppy-humanoid*, *Isaac Sim*, *humanoid-gym*, *arcle*, *GRUtopia*, *mesa*, *iGibson*, *ai2thor*, *Dopamine*, *dm_control*, *keras-rl*, *ray rllib*, *Coach*, *Huskarl*, *Stirling-PDF*, *MinerU*, *pdfminer.six*, *ivy*, *haxe*, *c2rust*, *Transcrypt*, *c2go*, *SQLGlot*, *sqlalchemy*, *DBeaver*, *sqlx*, *sqlmap*, *Insomnia*, *bruno*, *keploy*, *Scalar*, *hoppscotch*, *Lean4*, *LeanDojo*, *dns0.eu*, *libredirect*, *AirSim*, *simulator*, *carla*, *HighwayEnv*, **, *MotionPlanning*, *UniAD*, *ApolloAuto/apollo*, *autoware*, *ardupilot*, *openpilot*, *End-to-end-Autonomous-Driving*, *SMARTS*, *SensorsCalibration*, *SVGDreamer*, *NoLabs*, *Evo*, *poly*, *REINVENT4*, *sherlock*, *LocalSend*, *drawio-desktop*, *Lean*, *Lean Dojo*, *llmlean*, *slack*, *discord*, *Vencord*, *Baileys*, *Zulip*, *OpenStreetMap*, *spotube*, *roop*, *Frigate*, *python-tuf*, *ERPNext*, *Score Specification*, *PowerToys*, *linutil*, *Microsoft-Activation-Scripts, *immich*, *ente*, *graphcast*, *WRF*, *open-meteo*, *core*, *formbricks*, *twenty*, *latex*, *typst*, *dub*, *fingerprintjs*, etc

*__Note:__ Freelunch can be using purely open-source (with only open-source tool backends), but you might want to consider managed backends for reasons like: simplicity gain, integration with your existing infrastructure, previous experience with the tool, unique capabilities not present in open source and lower costs compared to implementing open source naively. On the other hand, you can use open source to: cut down costs, give you more flexibility, can give higher quality solutions, avoid relying on vendors (not shutting down, lacking documentation/support or raising prices), developing your team (which can lead to solving other problems) and can avoid private data leaving your org. Generally you want to start with many managed tools, and as your team gains experience and expertise, slowly substitute managed for open source, until you feel that open source cannot boost you anymore. Note: managed tools have to develop Freelunch plugins as to maintain a single dashboard for the MLOps platform user (instead of having to look at multiple dashoboards from different managed tools).*
</details>

<details>
    <summary><b>Tool Landscape</b></summary>

#### :hammer: __Tool Landscape__

The MLOps tool landscape is huge. I will just leave here some pointers (without any specific order).

* [Navigating the MLOps tooling landscape by *LJ Miranda*](https://ljvmiranda921.github.io/notebook/2021/05/10/navigating-the-mlops-landscape)
* [AI & Data Foundation Interactive Landscape by *Linux Foundation*](https://landscape.lfai.foundation/)
* [Cloud Native Landscape by Cloud Native Computing Foundation](https://landscape.cncf.io/)
* [The 2024 MAD (ML, AI & Data) Landscape](https://mad.firstmark.com/)
* [Machine Learning Toolbox by *Amit Chaudhary*](https://amitness.com/toolbox/)
* [Machine Learning Tools Landscape v2 (+84 new tools) by *Chip Huyen*](https://huyenchip.com/2020/12/30/mlops-v2.html)
* [Awesome MLOps: a curated list of awesome MLOps tool by *kelvins*](https://github.com/kelvins/awesome-mlops)
* [The Best MLOps Tools and How to Evaluate Them by *Neptune AI*](https://neptune.ai/blog/best-mlops-tools)
* [ML Toys: a curated list of MLOps projects by *Aporia*](https://mlops.toys)

##### __Proprietary (usually managed) MLOps tools__

1. __MLOps platforms__
    1. From Major Cloud Hardware Providers (e.g., *AWS Sagemaker*, *Vertex AI*, *Azure ML*)
    2. Cloud Hardware Provider Agnostic
        1. No-code 
            1. Managed (e.g., *Akkio*, *Datarobot Rata Robot*, *Sagemaker Canvas*, *H20 Driverless AI*, *The AI & Analyitcs Engine*, *Obviously AI*, *ModularMind*, *Tuba.ai*, *Voker AI*)
            2. Self-hosting (e.g., *Modela.ai*)
        2. Low-code 
            1. Managed (e.g., *Datarobot*, *Dataiku*, *H2O*, *Valohai*, *Continual.ai*, *aixplain*, *c3.ai*, *Telepath*, *Domino*, *UbiOps*, *Inlinity*, *Andromeda360AI*, *Striveworks*, *Instill AI*, *InfuseAI*, *HPE Ezmeral MLOps*, *AI Squared*)
            2. Self-hosting (e.g., *Komodo AI*)
        3. Medium-code 
            1. General-purpose (e.g., *Deepchecks*, *Databricks*, *Cloudera machine learning*, *Palantir*, *Paperspace Gradient*, *W&B*, *Comet*, *MindsDB*, *Iguazio*, *Red Hat OpenShift AI, *managed ClearML*, *Neu.ro*, *Outerbounds (managed Metaflow)*, *Seldon*, *BentoML*, *Modular*, *Wallaroo.ai*, *Truefoundry*, *Vessl.ai*, *Lightning.ai*, *cnvrg.io*, *deploifai*, *Grid Dynamics*, *Qwak*, *Shakudo*, *craine.io*, *aiXplain*, *rafay*, *felafax*, *Thread AI*)
            2. Specialized
                1. By requirement
                    1. Real-time (e.g., *TurboML*, *Claypot AI*)
                2. By method
                    1. LLMs (e.g., *Together.ai*, *Langsmith*, *Scale Generative AI Platform*, *Parea*, *HoneyHive*, *Klu.ai*, *Freeplay*, *Giga ML*, *Lamini*, *Vellum*, *Vianai*, *Parea AI*, *Athina AI*, *AilaFlow*, *Airkit.ai*, *Rebyte*, *Keywords AI*, *Helicone*, *GradientJ*, *LastMile AI*, *Adaptive ML*, *Freeplay*, *dkube*, *dynamiq*, *composable*, *VESSL AI*, *Braintrust*, *Toolhouse*)
                        1. By method
                            1. RAG (e.g., *Vectara*)
                            2. Agents (e.g., *Steamship*, *Langsmith*, *crewAI*, *Stack AI*, *Emergence AI*, *Zaia*, *Artisan*)
                                1. Web Agents (e.g., *Lindy.ai*)
                        2. By use case
                            1. Chatbot (e.g., *Sierra*)
                3. By focus
                    1. Data-centric (e.g., *Cleanlab*, *Scale AI*, *mirry.ai*, *markovML*, *co-one*) 
                    2. Federated ML (e.g., *Apheris*)
                    3. Secure ML (e.g., *grayswan*)
                    4. Data (e.g., *Chalk*, *hex*)
                4. By sector
                    1. Defense: (e.g., *Revela*)
                    2. Factory (e.g., MakinaRocks)
                5. By data modality
                    1. CV (e.g., *Dataleon*, *EyeFlow.AI*, *Picsellia*, *Atos*, *Tuba.ai*, *Tenyks*, *crowdai*, *devisionx*)
                    2. Geospatial (e.g., *Deep Block*)
                    3. NLP (e.g., *Dialogflow*, *Wit.ai*, ​​​​​​*​MonkeyLearn*, *Rossum*)
                    4. Video & Audio (e.g., Sieve)
                    
2. __Model Deployment__ (e.g., Modelbit, KeaML, OctoML, Modzy, datatron, HippoML, Preloop, Unify.ai, centml)
    1. __LLM Deployment__ (e.g., Groq, TitanML, predictionGuard, fal, airtrain)
        1. __LLM Agent Deployment__
    2. __Edge Deployment__ (e.g., Sarama)

3. __ML Project Management & Experimentation__
    1. [ __*DagsHub*__](https://dagshub.com/)

4. ___ML Project Management, Model Building and Improvement__ (Experiment Tracking + Model Training + Model Evaluation)
    1. [__*W&B 4 Experiment Tracking*__](https://wandb.ai/site)
    2. [__*CometML*__](https://www.comet.com/site/)
    3. [__*Determined AI*__](https://determined.ai/)
    4. [__*Kolena*__](https://www.kolena.io/)
    5. [__*Efemerai*__](https://www.efemarai.com/)

5. __Metadata Tracking__ (Experiment Tracking + Monitoring)
    1. [__*Aimstack*__](https://aimstack.io/)
    2. [__*Neptune.ai*__](https://neptune.ai/)

6. __Observability__
    1. [ __*Why Labs*__](https://whylabs.ai/)
    2. [ __*MonaLabs*__](https://www.monalabs.io/)
    3. [__*Aporia*__](https://www.aporia.com/)
    4. [__*Superwise*__](https://superwise.ai/)
    5. [__*Cencius*__](https://censius.ai/)
    6. [__*Verta*__](https://superwise.ai/)
    7. [__*Truera*__](https://truera.com/)
    8. [__*Senser*__](https://senser.tech/)

    9. __ML Observability + Something__
        1. __Observability + Model Improvement__
            1. [__*Fiddler*__](https://www.crunchbase.com/organization/fiddler-labs)
            2. [__*Openlayer*__](https://www.openlayer.com/)
            3. [__*Arize AI*__](https://arize.com)
            4. [__*Arthur AI*__](https://www.arthur.ai/)
            6. [__*Autoblocks*__](https://www.autoblocks.ai/)
            7. [__*KLU*__](https://klu.ai/)
            8. [__*Athina AI*__](https://athina.ai/)
        2. __Observability + Data + Model Improvement__
            1. [__*Etiq*__](https://etiq.ai/)
        3. __Observability + Model Improvement + Model Deployment__
            1. [__*Gentry*__](https://www.gantry.io/)
        4. __Observability + Governance, Compliance and Responsible AI + Model Improvement__
            1. [__*Credo AI*__](https://www.credo.ai//)
        5. __Observability + Labelling:__ 
            1. [__*Encord*__](https://encord.com/)
        6. __Observability + Evaluation__
            1. [__*Galileo*__](https://www.rungalileo.io/)
            2. [__*Kolena*__]()
            3. [__*FairlyAI*__](https://www.fairly.ai/)

    10. __Data Observability__
        1. [__*Secoda*__](https://www.secoda.co/)
        2. [__*Monte Carlo*__](https://www.montecarlodata.com/product/data-observability-platform/)
        3. [__*Metaplane*__](https://www.metaplane.dev/)
        4. [__*DataDog*__](https://www.datadoghq.com/)
        5. [__*acceldata*__](https://www.acceldata.io/)
        6. [__*Qualdo*__](https://www.qualdo.ai/)
        
        6. __Data Governance__
            1. [__*Atlan*__](https://atlan.com/home-page/)

8. __Dataset Construction__
    1. __Synthetic Data Generation__
        1. [__*Gretel*__](https://gretel.ai/)
        2. [__*YData*__](https://ydata.ai/)
        3. [__*Mostly AI*__](https://mostly.ai/)
        4. [__*Statice*__](https://www.statice.ai/)
        5. [__*Hazy*__](https://hazy.com/)
        6. [__*Synthesized*__](https://www.synthesized.io/)
        7. [__*Replica Analytics*__](https://replica-analytics.com/)
        8. [__*Anyverse*__](https://anyverse.ai/)
        9. [__*Bifrost*__](https://www.bifrost.ai/)
        10. [__*parallel domain*__](https://paralleldomain.com/)
        11. [__*Synthesis AI*__](https://synthesis.ai/)
        12. [__*Octopize*__](https://www.octopize.io/?locale=en)
    2. __Data labelling__
        1. [__*V7*__](https://www.v7labs.com/)

9. __Others__
    1. __Knowledge management for MLOPs:__ [__*Align AI*__](https://www.getalignai.com/)
    2. __LLM-related__
        1. __LLM Observability:__ 
            1. [__*Orchesta*__](https://orquesta.cloud/)
            2. [__*Portkey*__](https://portkey.ai/)
            3. [__*vectorview*__](https://www.vectorview.ai/)
            4. [__*katanemo*__](https://www.katanemo.com/)
            5. [__*traceloop*__](https://www.traceloop.com/)
            6. [__*Patronus AI*__](https://www.patronus.ai/)
            7. [__*Evidently Cloud*__](https://www.evidentlyai.com/blog/evidently-cloud-ai-observability-platform)
        2. __LLM Observability + Something__
            1. __LLM Observability + Performance Evaluation + Experiment Tracking__
                1. [__*Parea AI*__](https://www.parea.ai/)
            2. __LLM Observability + Evaluation__
                1. [__*Diligently AI*__](https://diligently.ai/)
                2. [__*Inductor*__](https://inductor.ai/)
            3. __LLM Observability + Experiment Tracking__
                1. [__*HumanLoop*__](https://humanloop.com/)
        3. __Agent Observability__
            1. [__*Agency AI*__](https://www.agentops.ai/)
        4. __LLM APIs__
            1. __Vendor-dependent:__
                1. [__*OpenAI API*__](https://openai.com/gpt-4)
            2. __Vendor-agnostic (LLM gateway)__
                1. [__*Monster API*__](https://monsterapi.ai/)
                2. [__*Javelin*__](https://www.getjavelin.io/)
                3. __LLM Routers__ (instead of you specifying which LLM, the LLM Router is intelligent to chose the best one for your prompt)
                    1. [__*Martian*__](https://withmartian.com/products/model-router)
                    2. [__*Mot Diamond*__](https://www.notdiamond.ai/)
        5. __LLM Fine-tuning__
            1. [__*Graphlit*__](https://www.graphlit.com/)
            2. [__*Helix*__](https://tryhelix.ai/)
            3. Mosaic
            4. Together
            5. Predibase
            6. Deci
            7. Tune AI
        6. __Prompt Engineering__
            1. [__*PromptLayer*__](https://promptlayer.com/)
            2. [__*Prompthero*__](https://prompthero.com/)
        7. __LLM Security__
            1. [__*Lakera*__](https://www.lakera.ai/)
            2. [__*Guardrails AI*__](https://www.guardrailsai.com/)
            3. [__*Acuvity*__](https://acuvity.ai/)
        8. __Knowledge-base Marketplace__
            1. [__*Boostio*__](https://www.boostio.ai/)
        9. __Synthetic Data for Testing/Evaluation__
            1. [__*Tonic AI*__](https://www.tonic.ai/)
        10. __RAG Building__
            1. [__*ragieai*__]
    3. __Data Versioning__
        1. [__*XetHub*__](https://about.xethub.com/)
    4. __Governance, Compliance and Responsible MLOps__
        1. [__*Saidot*__](https://www.saidot.ai/)
        2. [__*Holistic AI*__](https://www.holisticai.com/)
        3. [__*ValidateML*__](https://www.validateml.de/)
        4. [__*Monitaur*__](https://www.monitaur.ai/)
    5. __Model Building__
        1. [__*Iterative*__](https://iterative.ai/)
        2. [__*Lightning*__](https://lightning.ai/)
        3. [__*Contextual.ai*__](https://contextual.ai/())
    6. __3rd Party Data QA__
        1. [__*Integrate.ai*__](https://www.integrate.ai/)
    7. __Model Interpretability/Explainability/Analysis__
        1. [__*Goodfire*__](https://goodfire.ai/)
    8. __Governance, Compliance and Responsible AI__
        1. [__*2021AI*__](https://2021.ai/)

##### __DevOps tools for Security, Compliance, Code Quality & Cost Management__

1. __While Developing:__
    1. __VS Code Extensions__
        1. __Static Security Testing, Code Quality Analysis & IP/Licence Scanning__
            1. [__*Snyk*__](https://github.com/snyk/vscode-extension): gives a great first impression, based on heir website.

    2. __API listeners__
        1. __Cloud/premise cost management tools__

2. __After Developing: CI/CD Workflows__
    1. __Automated PR Reviews (Offline testing, meaning that the system was not deployed yet)__
        1. __Code Reviews__
            1. __Static Security Testing & Code Quality Analysis__
                1. [__*Codacy*__](https://www.codacy.com/): does static security & code quality analysis & offers simple suggestions. Gives a great first impression, based on their website. Branded as an AI-powered tool.
                2. [__*Codeball*__](https://codeball.ai/): does risky code indentification (they dont go into specifics on their website). Gives a decent first impression, based on their website. Branded as an AI-powered tool.
                3. [__*AI Reviewer*__](https://www.aireviewer.com/): does code quality assesment. Gives a bad first impression, based on their website. Branded as an AI-powered tool.
                4. __Static Security Testing__
                    1. __Software Composition Analysis (SCA) tools__: scans code for license compliance & security vulnerabilities in open source dependencies
                        1. [__*Github's Dependabot*__](https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuring-dependabot-version-updates)
                    2. __Static Application Security Testing (SAST) tools__: scan for proprietary insecure code without running the code
                        1. [__*Github's Code Scanning*__](https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/about-code-scanning)
        2. __PR Explanation__
            1. [__*What The Diff*__](https://whatthediff.ai/): generates pull request summaries in behalf of developers. Branded as an AI-powered tool.

    2. __CD staging cluster Testing (Offline testing, meaning that the system was not deployed to production yet) OR Background Analysis (Online testing (proactive & reactive), meaning that the system was already deployed to production, but a copy of it is being tested in another environment)__
        1. __IP/Licence Scanning & Static Security Testing__
            1. [__*FOSSA*__](https://fossa.com/): scans code for license compliance and security vulnerabilities both in proprietary code code as in open source dependencies. Gives a great first impression, based on their website.
            2. [__*Snyk*__](https://snyk.io/): gives a great first impression, based on their website.
            3. __Static Security Testing__
                1. [__*Deepcode*__](https://www.deepcode.ai/): does insecure code identification. Gives a bad first impression, based on their website. Branded as an AI-powered tool.
        2. __Dynamic Security (Cybersecurity-as-a-Service): Vulnerability Scanning & Security Monitoring & Response (Dynamic Application Security Testing (DAST) tools & Breach, Attack Simulation (BAS) tools, Managed Detection & Response (MDR) tools):__ detects security flaws by simulating grey-box or black-box attacks, can identify ongoing attacks & can also initiate reponse to them.
            1. [__*Intruder*__](https://www.intruder.io/): finds weaknesses of deployed systems. Gives a great first impression, based on their website.
            2. [__*SOPHOS*__](https://www.sophos.com/). Gives a great first impression, based on their website.
            3. [__*Cymulate*__](https://cymulate.com/). Gives a great first impression, based on their website.
        3. __GDPR-like Compliance tools__
            1. [__*Drata*__]((https://drata.com/))
            2. [__*OpenDSR*__](https://opendsr.org/)
        4. __Dynamic Security (Cybersecurity-as-a-Service)__ and __GDPR-like Compliance tools__
            1. [__*Immuniweb*__](https://www.immuniweb.com/). Gives a great first impression, based on their website

</details>

</details>

<details>
    <summary><b><span style="color:purple">Elephant</span> in the room</b></summary>

### <b><span style="color:purple">Elephant</span> in the room</b>

__Is it worth having a evaluation logical cluster for evaluation and observability, considering the cost it imposes?__

Yes, because of the following reasons:

1. The evaluation logical cluster doesnt have to be running all the time. You can proviion it on the fly, only when you need it or you can schedule it to run on a % of total time, only run on low-demand periods and you can also <tear the cluster down/set it back up (starting from the state it left)> when you please.

2. The system/platform is deployed to the background cluster at the smallest scale possible to avoid unnecessary costs. But then how do you test and observe correctly? Well, we downscale the simulated requests in the same manner. We know how much is the "same manner" by trying different scales and fiting a function that maps downscaled system/platform metrics --> full blown system/platform metrics.

3. If you have internal heavy services (e.g., internal LLM API or Data lake) that would be too costly to replicate in the background cluster and/or you don't care much about experimenting with their code and intra-service tracing, you can just emulate it via a service that simply calls the actual service's API, where you can still experiment with different configurations. Of course, this heavy service is then scaled according to the increase in load.
    
4. You don't need a large physical cluster. You can use a small physical cluster, an existing one (e.g., using vcluster to create a cluster in a k8s namespace) or even choose a virtual cluster running on a single pysical machine (e.g., using kind (kubernetes in docker (kind), minikube, k3d)). This of course will impact the speed at which you get evaluation results and observability analysis. However, you are safe since the MLOps platform itself is deployed in your fast production cluster.

5. The evaluation and observability benefits you gain from having a sandbox environment for your whole MLOps platform are huge. You can do production-grade: debugging, system improvements, engineer training and responsible deployments; without breaking your production system and directly impacting users; or overloading your CI/CD. As MLOps platforms get more and more complex, this becomes more and more important.
</details>

<details>
    <summary><b>In-depth capabilities</b></summary>

### __In-depth capabilities__

#### 1. Capabilities of Freelunch

* __Managers/Leads__ can:

    1. __Set goals: align MLOps platform and system KPI goals with Business KPI goals__ (change evaluation function that works on top of benchmarks) based on data (by connecting with your business analytics tool & Ml System/MLOps platform evaluation tool). The goal here is to find a single metric that if the MLOps platform team or applied ML team maximizes it, it maximizes the most the business' KPIs. (e.g., this could work by doing the causal estimation P(BUSINEES_METRIC == business_metric | do(EVALUATIONS == evaluations) where each evaluation metric is first normalized to be within [0,1] and then all benchamrks are layer normalized to sum to 1 (softmax applied to input)) and then doing optimization on top of it to find argmax of the expected value of (P(BUSINEES_METRIC == business_metric | do(EVALUATIONS == evaluations))), to get the ideal weights for each evaluation metric on a linear evaluation function.)

* __ML Engineers & Managers/Leads__ get:

    1. __ML System Design (Architecture)__ (engineers can sample ml design patterns, use our drawing tool to make awesome diagrams & collaborate on a same diagram with multiple colleagues, versions your diagrams and formally verifiy them agaist specifications.)
    2. __Hot-start: Declarative API for building MLOps platforms__ (Models/Prompts + Runtimes + Infrastructure + Data + Config) with a lower-level layer (plugins and/or edit repo directly) for hacking. Allows you to leverage domain & ML expertise knowledge while making use of Freelunch to do most of the work of setting up the architecture and implementing it, according to your requirements & preferences. It auto-generates documentation and is interactive, allowing you to edit the ML Sytem automatically built by Freelunch. If you want to implement custom integrations and optimizations you can use the lower-level layer (plugins and/or edit repo directly) to hack it from the ground up.
    3. __System-wide Experimentation__ (e.g., an ml team wants to experiment with federated learning)
        1. __Experiment Management__ (You can do experiments on: codebase, production configuration and/or artifacts (code, config, data, schema (backards-compatible), html, prompts, compute artifacts (e.g. functions, elt, pipelines, models, ensembles, pre/post processors (input, prediction and/or data filters))). Experiments also carry along more data that enables you to reproduce it (e.g., branch/commit of the experiment and command to run). And of course, experiments have metrics attached to them: evaluation results (e.g., ML metrics, performance metrics, statistical tests) and specifications (e.g specific system config)). 
        
        Besides storing & visualizing experiments, you also have: experiment ranking with custom evaluation function, experiment suggestions (use of intelligent algorithms under the hood (e.g., bayesian optimization, causal inference & genetic algorithms) to make you get great results with less experiments by not following the rule of changing one thing at a time), Playground for doing (notebook and/or no-code) system experiments in the background cluster, estimating large-scale experiment results, providing stat significance tresholds).
        
        Integration with git & dvc/fds for repo-based experiments & can take actions that take them (experiments being commits) to production such as: (1) reconsitute any repo experiment, (2) generate a PR for it, (3) and aprove the PR (only leads can do this). _Note:_ PR is not to merge with master, but to merge with Experimentation master (which then need another PR to be merged with master).
        
        Experiment ideas (that were not tried yet) are organized, stored and can be annotated in the experiment manager.
        2. __Comprehensive ML System Evaluation in background__
            1. __Requirements__
                1. __Constrained environments__ (evaluating on real constrained environments (e.g., rasberry pi) or using Environment mimicking tools (e.g., Selenium or Cypress for browsers, EdgeCloudSim, EnvisEdge or Auptimizer for edge devices) to simulate hardware (computing & networking) (if necessary), OS (if any) & more constraints (if any; e.g., browser constraints such as no fs access). Uses cases: deployments on: mobile, embedded devices (microcontrollers with OS, microcontrollers without OS, FPGAs and ASICs), browsers, wearables (e.g., glasses, watches) and other devices (e.g., TVs, interactive panels, etc))
                    1. __Inference__ (memory usage, power consumption, inference time) (contraints: discontonuos function, only need to be below treshold (memory usage in single-task microcontrollers (no OS)); goals: continuous function, need to optimize (e.g., power usage after treshold is reached)) (small models: Microcontrollers (memory, power, processing speed); medium models: Mobile (memory, processing speed); big models: any cloud instance (memory) (on a fixed high-performant instance: assuming same <machine, OS, processor used, cgroup>)). (_Note:_ has to do various runs and compute <mean, std deviation> because of stochasticity of hardware implementation)
                    2. __Training__ (memory usage, power consumption, training time) (on a fixed high-performant instance: assuming same <machine, OS, processor used, cgroup>) (_Note:_ has to do various runs and compute <mean, std deviation> because of stochasticity of hardware implementation)
                2. __Security__
                    1. __ML-agnostic: Dynamic Application Security Testing (DAST)__ (test your ML System for known cybersecurity attacks with the help of our AI-powered white-hat hacker, generate reports & alerts/notifications)
                        1. __By Atacker Knowledge-level__
                            1. __Black/Grey Box attacks__ (see if deployed system is secure to external attacks. Automated solutions are often called Breach & Attack Simulation (BAS) tools.)
                            2. __White-box Attacks__ (see if deployed system is secure (or responds well) to internal attacks.)
                        2. __By Attack Type__
                            1. __Privacy__
                            2. __Integraty__
                            3. __Availability__
                    2. __ML-dependent__
                        1. __General Analysis__
                            1. __Influence Surface of Models__
                            2. __Innapropriate outputs__
                            3. __Misuse of Models__
                        2. __Red teaming__
                            1. __Attack types__
                                1. __Model Stealing Attacks__
                                2. __Data Reconstruction Attacks (We want differentially private pipelines and models)__
                                3. __Adversarial Attacks (Includes Prompt Injection)__
                                    1. __Integraty attacks__
                                    2. __Privacy attacks__
                                4. __Dependency (Data (training data or context data) and/or Model) Poisoning Attacks__
                                    1. __Direct Attacks__ (dependency is modified to cause the production of bad responses)
                                    2. __Indirect Attacks__ (dependecy is modified to teach the model to misbehave upon a specific trigger word. Then the attacker can use the trigger word when he wants.)
                            2. __Stages__
                                1. __In Integration Testing Workspace__
                                2. __In Staging and Production Workspaces__
                        3. __Formal Verification of Security Properties__
                            1. __Microservices interact with each other in a precise manner__
                            2. __Models that are provably secure against certain kinds of attacks__
                3. __Compliance__
                    1. __Functional Compliance__ (test for functional compliance (e.g., data privacy: role-based access control, GDPR, sensitive data handling; or cybersecurity: encryted data at rest, in transit & maybe even in-memory, and defense & remdiation automated protocols), manage regulations, certificates and agreements, configure & schedule report generation; manage & share reports)
                    2. __IP/Licence Compliance__
                        1. __Dangerous Code Copy/Paste__ (Our VSCode extension works together with our Chrome Extension to detect risky code copy/paste. If risky code is detected, *Freelunch* generates alternative free-to-use code that does the same thing as the original copyleft code. This way, Engineers can be free to do what they like most: copy/paste code :joy:.)
                        2. __IP/Licence Scanning__ Explicit declaration of third-party dependencies & their licences (a simple file where you organize from where you got your third-party code, data & models that we use to verify if you are adhering to their licences. We help you build this file: we scan your repo in an attempt to find dependencies you missed mentioning there or detect licences that got updated; automatically generate a PR with updated dependency declaration file). Plus, automatic scanning of the dependency tree of these dependencies to try to find violations.
                        3. __Compliance support 4 AI generated code__ is checking with the training code data to see any matches, if there is very similar code (of singificant complexity), the licence of that code is linked to it; also: you can ask for the AI Assistant to generate free-to-use code that does the same thing as the original copyleft code.
                4. __Non-crucial unit and integration tests__ (because when they are crucial, they need to be inside CI/CD)
                    1. __Traditional tests__
                        1. __Backwards-compatibility tests__
                    2. __ML-related tests__
                        1. __Training-serving skew__
                        2. __Model-Service-System same inference results sanity check__
                        3. __Reproducibility__
                5. __Backwards-compatibility__
            2. __Benchmarks__ (soft metrics, which you want to optimize for)
                1. __Model-related__
                    1. Predictive power loss (loss of predictive power wrt offline model)
                    2. Explainability/Interpretability
                    3. Uncertainty Estimation (includes OOD Detection)
                    4. Intervention Analysis (using Causal Inference)
                    5. Performance: model profiling under some deployment context
                    6. Fairness
                    7. Model scalability (scaling laws)
                    8. Robustness (training time: to outliers in training data; inference time: tail data, noisy data, invariants)
                2. __System-related__
                    1. Only for RAG:
                        1. Embedding Power (how well does the embedding distance reflect the actual semantic distance between documents)
                        2. Document Relevance (how usefull are the documents for the tasks that need to be done)
                        3. Retrieval Power (how well the retriever gets the adequate set of documents for a user input)
                        4. Query Rewriting Power (how well does query rewriting actually reflect what the user is querying)
                    2. Degenerate Feedback Loop Handling
                    3. Replayability
                    4. CI/CD speed & efficiency
                    5. Cost (Total Cost and Cost Profiling)
                    6. Scale and Scalability
                    7. Performance: profiling of services, pipeliens and the whole system
                    8. Wise training data selection
                    9. Robustness
                    10. Technical Debt/Maintanability (e.g., based on Google's ml test score or on comments with technical debt hints)
                    11. Carbon Footprint
                    12. User as Data Owner
                        1. Data Deletion
                        2. Data Compensatation
                    13. Evaluation Coverage (problems identified in read teaming that are not covered in current evaluations)
        3. __Working Environment__ (packaged as a container dev environment)
            1. __Data-related__
                1. __Dataset Construction__
                    1. __Data Annotation (can be more than just labelling)__ (its job is to get datapoints from a storage location, show them in a frontend to labellers and store them back in the sotrage location with annotations. Supports explanation annotation, prioritization of datapoint/datapoint regions (received from the "Active Learning" system), detection of mislabelled examples, anomaly labeller detection, bayes optimal erros estimation based on different labelller annotation for the same input. Monitoring of latency & throughput of labellers. Priortizes datapoints to double-check labels based on model predictive power & labeller-disagreement. Also, experiments with different labelling guidelines to see if labeller disagreement lowers.) Note: we provide integrations with third-party labelling services.
                        1. By automation level
                            1. Manual
                                1. Internal
                                2. Outsourced
                                    1. Curated team
                                    2. Crowsourced
                                        1. Non-gamified
                                        2. Gamified (e.g., Chatbot Arena, Open Parti Prompt)
                            2. AI-assisted (e.g., for drawing boundaries in images)
                            3. Automatic
                                1. Using proxies
                                    1. Weak signals
                                    2. Heuristic functions
                                2. Fully automatic (e.g., LLM does it)
                        2. By data modality
                            1. Tabular
                            2. Graphs
                            3. Text
                            4. Audio
                            5. Computer Vision
                                1. Image
                                2. Video without Audio
                                3. Video
                            6. Signals/Time-series
                            7. Spatial-temporal
                                1. 2D-time (e.g., geospatial)
                                3. 3D-time (e.g., point clouds or molecules)
                        3. By type
                            1. For training and Evaluation
                                1. Labelling
                                2. Scoring (used for annotating generative model outputs)
                            2. Just for training
                                1. Ranking (used for annotating generative model outputs, e.g., RLHF)
                    2. __Synthetic Data Generation__
                        1. Data Augmentation (makes it easy for you to produce new X' data, where the model needs it, from existing X data (e.g., for CV: new views of a scene)). Note: we provide integrations with third-party data augmentation services.
                            1. Automatic Labelling
                                1. For tabular data
                                    1. ML models
                                    2. Heuristic Functions
                                2. For non-tabular data (files) (often called semi-structured (e.g., pdf) & unstructured (e.g., media))
                                    1. ML models
                                        1. LLM-generated labels
                            2. New X's from old Xs
                                1. ML Models
                                    1. Foundation Models
                                        1. LLM generated text
                                        2. Img2Img Diffusion-generated images
                            3. New (X',Y')s Datapoints from old (X, Y)s
                                1. For tabular data: learn P(X=x,=y) and sample from it (But if we could model P(X=x,Y=y) wouldnt we already have the final mode? Well, technically yes, but in paractice the P(X=x,=y) here is a bit low-quality, but is sufficient do generate a lot of data with low bias. Additionally, we can model P(X=x,Y=y) using different approaches as to average out the biases of each data model. Think of it like this: our data models can be wrong, but if their errors everage out, then the expectation becomes similar to the real model.)
                                2. For non-tabular data (files) (pften called semi-structured (e.g., pdf) & unstructured (e.g., media)): output-invariant tranformations: given a spacific model, get datapoins from Feature Store, apply output-invariant transformations to generate new datapoints and store them back in the Feature Store
                        2. Pure Synthetic Data Generation: new (X,Y) datapoints out-of-the-blue
                            1. Using Foundation Models
                                1. CV
                                    1. Text2Image (Image Generation)
                                    2. Text&Image2Image (Image Editing)
                                    3. Tex2Video (Video Generation)
                                    4. TexVideo2Video (Video Editing)
                                2. NLP
                                    1. Text2Text (LLM Generated Data)
                                        1. NL2SQL (Tools: PremSQL)
                                        2. NL2Latex
                                3. Audio
                                    1. Text2Audio (Audio Generation)
                                    2. Text&Audio2Audio (Audio Editing)
                                    3. Speech2Speech
                                4. Application-specific
                                    1. TimeSeries 
                            2. Physical Simulator Data
                    3. __Dataset Storage with Versioning__ (you can upload ready datasets from whatever location (e.g., local, google drive, s3, postgres, etc )to your prefered data storage location (e.g., data lake or feature store) or work on your datasets via the platform, and store them after)
                2. __Data Plumbing__
                    1. __Sync data across locations__
                    2. __On-demand data builds__
                        1. __On-demand ELT__
                        2. __On-demand ETL__
                    3. __Interaction with Database Systems__
            2. __Data & Model-related__
                1. __R&D Playground:__ Experimentation environment for researchers developing new MLsys methods, model architectures and better ML understanding. Notebook-based, with previosuly setup workflow orchestration in a ephemeral workloads cluster. Contains GUI + Automatic Document Writer + Integrations with ML services

                Additional Features:
                    1. Wizard-of-Oz studies. This is where you emulate your ML system with a demo mock app that actually has a human under the hood, not an ML model. The goal here is just to see if the model would bring value to the user in a non-latency bound scenario.
                    2. Estimation of PoC MVMs (Minimum Viable Metrics): these are the metric tresholds of the PoC that indicate that you should take the PoC model to production. But setting these tresholds is hard because you need to know how much metric improvement you can get via iteration on data and models. Our approach is to help with this via a data-driven methods: we track your historic PoC to production models and their metrics, which allows us to estimate the curve of metric improvement vs time, which lets you setup better PoC MVMs. 
                
                How is it different from applied ML Engineer experimentation working environment? 
                    1. This one is has more focus on researchers and data scientistis, it is easier to use but has less engineering rigor.
                    2. Comes with out-of-the box functionality and integrations for: PoCs, in-depth visualizations and SOTA; which is not necessary for experimentation.

                Having a separate environnment for PoCs and Experimentation avoids that PoC workloads make Experimentation slower. Your team can also choose to tear done the PoC cluster if on a budget, since it is not crucial.
                
                Main use cases:
                    1. Develop and version proof-of-concepts
                    2. Understand why and when the method is better/worse suited. 
                    3. Develop their own ideas for new methods or tweaks to existing methods.

                2. __Artifact Expert: Retrieval/Storage, Evaluation, Visualization, Analysis, Editing, Merging, Optimization, Compilation, Scaling Law Estimation, Stamping and Mocking__ of artifacts that are not available in production (e.g., in a repo or HuggingFaceHub), that are available in production (e.g., in Model/Prompt Registry) & that are deployed (e.g., dataset feeding data collection system or model being use din a Inference Service). Be able to: (1) fetch from anywhere or upload artifacts; (2) evaluate (e.g predictive power on some curated data, innaproprateness of outputs under prompt injection, bias for some protected class), (3) visualize & analyze (e.g., explain a specific prediction of the model by using feature importance and showing step-by-step computations performed by the model in a GUI or by distilling the model into a simpler interpretable model (e.g., decision tree); or analyze the scalability of the predictive power wrt to dataset size and/or model size) & (4) optimize (e.g make model more accurate, smaller, add custom rules to model file) (like W&B does) these artifacts (using our offline artifact evaluation/optimization suite, doing a custom one with a container image (like AWS lambda) or using notebook environment); (5) store them any place (including custom places defined by letting you write API plugins) along with metadata; & download them locally. Also, (6) compile/transpile models (e.g., from TorchScript to ONNX (standard format that is like a bit-higher-level computational graph where nodes can be data structures and edges built-in operators with multiple outputs); from ONNX to Substrait, GGML, GGUF or TensorRT; or from pure (variables & basic operations) computational graph to ONNX; or from GGML to llamafile; or from ONNX + dataset + training script + training config + dependencies to ModelKit) & data pipelines (e.g., from scikitlearn to spark/koalas or airflow), and compile them to native libraries or executables (with support for edge deployments). Finally, (7) Edit models (e.g., editing LLM factual knowledge using ROME or manually adding operators to the architecture for fine-tuning), (8) Merge models (model soups): mixing the weights of multiple models via a merge operation (e.g., interpolation) (9) estimate their scaling laws; (10) stamp your models: sign them and bind them to their training data with a computational proof; (11) get artifact mocks for testing.
                    1. __Data-related__ (DataOps)
                        1. __Private Dataset Analysis__ (we let you connect to sensitive data & to processing on top of it; without having access to the data itself, and even, not being able to reconstruct the data if you put all your effort into it. This preserve the privacy of the data owner.)
                        2. __Automatic Dataset Insights (EDA) & Enhancements__
                            1. __Insights__ (e.g., outlier detection (e.g., isolation forest), dimensionalty reduction potential; data question-aswering; data quality report (identifying how many missing values, duplicates); feature-feature dependency plot, features with high & features with low dependency with target; causal inference capabilities: given causal graph, estimate causal effects & counterfactuals; causal graph sensitivity analysis, causal discovery)
                            2. __Enhancements__
                                1. __Basic preprocessing__ (Do basic data processing to solve problems identified in "Give Insights" based on user text instruction or problem selection (e.g., do feature inputation, remove duplicates, remove outliers/anomalies))
                                2. __Data Quality enforcement__ (enforce data from multiple locations be consistent with each other, to guarantee formats, avoid duplicates, avoid redundant computation, avoid unused data storage) (Tools: )
                                3. __Active Learning__ (given a mode, its feature store dataset and time range of datapoints, estimate the inputs regions you should focus on getting more datapoints or just labels so that your model get the biggest delta in predictive power, by analzying: (1) decision boundaries, (2) error on slices of featured space + possibly input metadata (input enrichment), (3) knowledge about how the model and learning algorithm work. (4) knowledge about the real-world process.
                                    1. __LLM-powered__
                                        1. __Enhancements Suggestions via RAG LLM__
                                            1. __Types of training data that would most improve the LLM__ (by finding regions, in input text embedding space, with low data and/or low predictive power)
                                        2. __Generation of training data for fine-tuning LLMs__
                    2. __Model-related__
                        1. __Lifeboat Models:__ for every model in production you have an equivalent lifeboat model stored. Lifboat models are models you deploy a model of your is doing something very wrong and needs to be swapped immediately (_Note:_ they are retrained together with you prod model, so tey are fresh). The first though would be to rollback your model to tha last version, and this is a valid approach many times, however, sometimes you are late to encounter the problem, and the previous models also contain the same issue and/or would need to be retrained. ANother sceneraio is if you are using a third-party model and you dont have access to previosu versions. A second though would be to take down the whole model in production, but then you downstream service will have to rely on their fallbacks which are worse than a lifeboat model.
                        
                        This is where the value of lifeboat models appear, they are simpler models (dont even need to be ML models, can be just a heuristic) that are safe due to carefull timely evaluation of them and focus on interpretability. When you encounter a major problem, you can just deploy them without a doubt, until you can solve the issue & redeploy your main model that gives you higher predictive power. 
                        
                        These lifeboat models are also used to compute baselines for your production models, so that, we can evaluate prodictive power relative to the baseline (I call it corrected predictive power, e.g., "reduction in error" is a metric for this), which is what really matters.

                        2. __Compilation Suite__ (compiles models to packages, libraries or executables in any target environment (e.g., cloud (CPU instance, GPU instance), desktop, mobile, microcontroller (rasberry pi, arduino, pic, etc), browser, ASICs. Also supports distributed inference: compiles a big model into multiple libraries or executables, each running on a different logical node, that interact with each other in order to produce a prediction for the master node. Does hardware-dependent optimizations in the compilation process. Plus, can decompile model files (e.g. ONNX. PMML, PFA), libraries (e.g. .so) or executables to source code (e.g., pytorch code))

                        3. __Baseline generator__ (can generate baselines for your models, using the data they were trained on. Supports lower bound baselines like: random model, p(y) model, heuristics, open-source implementations, automl, using FMs; and upper bounds such as: human performance, SOTA for the task & Bayes Optimal Error estimation.)
                        4. __Analysis & Evaluation suites__
                            1. __For Models and Prompts__
                                1. __Evaluation of a single model/prompt__
                                    1. __Overall predictive power__ (how to do for prompts? You can proxy error as distance between the mebeddings of your target output (y) and model output (y_hat) if you having a good embedding model which was trained with comprehensive downstream tasks and input variations)
                                    2. __Error analysis__ (finds slices of data that model is getting most errors)
                                    3. __Model Robustness__
                                        1. __Training-time__
                                            1. __Robustness to Outliers__
                                        2. __Inference-time__
                                            1. __Natural__
                                                1. __Smoke tests/Behavioural tests with curated test data__ (simple inference smoke tests that the model should pass (e.g., absurd cases, directional expectation tests, invariance (random and non-random perturbations) tests, template tests, feature importance tests))
                                                2. __Sensititivity Analysis__ (How model output changes with small variations in input)
                                            2. __Adversarial attacks & trolling__ (How input can me modified in order to make model mistake in any or a cartian way)
                                                1. __Black-box__
                                                2. __Grey-box__
                                                3. __White-box__
                                    4. __Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s__
                                        1. __Detection of out-of-distribution deployments__ (this is for models not deployed yet, compare their training distribution to the recent production distribution)
                                        2. __Robustness to Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s__
                                    5. __Responsible ML__
                                        1. __Explainability/Interpretability__ (explain model's behaviour globally, locally, or for a single datapoint)
                                        2. __Fairness__ (3 types mainly: 1. having diparate corrected predictive power on different input regions; 2. Having different prediction if change ProtectedFeature == protectedValueA and protectedValueB not sustained by reality (thus incurring consistent error); 3. Having different prediction if change protected feature == protectedValueA and protectedValueB, altough its sustained by reality (thus not incurring consistent error))
                                        3. __Inapropriate outputs__ (e.g., spam, incentivizing going agaist law, slurs, pornography, etc)
                                    6. __Uncertainty Estimation (includes OOD Detection)__ (checking if the model knows what he doesnt know by seeing if its outputed uncertainty actuallt reflects the amount of error he makes)
                                2. __For multiple prompts:__
                                    1. __Output Comparisons__
                                    2. __Evaluation of Chains__
                            2. __Just for Prompts__
                                1. __For a single prompt:__
                                    1. __Validation against templates & regex__
                                    2. __Evaluation of prompt innapropriateness__ (e.g toxicity & trolling)
                                    3. __Sensitivity Analysis__ (sensitivity of output quality & price)
                                        1. __Prompt changes__ (how output changes if prompt changes a little)
                                        2. __LLM changes__ (How output changes if LLM is changed or the same LLM but with different config)
                                    4. __Hallucinations__ (how much hallucinatios do we get using this prompt & hallunication-inducing inputs)
                                    5. __Output size/cost__
                                    6. __Steerability__ (evalauting prompt importance models, aka feature importance for LLMs, the goals is to estimate how the LLM can be steered, which type of prompt make it change the output in a certain way)
                                    7. __Stochasticity__ (make a bunch of llm calls with the same prompt, and see measure output variability)
                                2. __For multiple prompts:__
                                    1. __Prompt Analytics__ (common words, sizes, avg output size, most used LLM, etc)
                                    2. __Prompt Anatomy Comparisons__
                                        1. Literal Similarity (token by token comparison)
                                        2. Semantic Similarity (embedding comparison)
                            3. __Just for Models__
                                1. __Responsible ML__
                                    1. __Differential Privacy__
                                    2. __Formal Security Guarantees__
                                2. __Analysis__
                                    1. __Input and Output data strctures__
                                    2. __Viualization of the model atdifferent levels: (1) achitecture, (2) raw computational graph and (3)runtime kernel execution on optimized computational graph__
                        5. __Optimizations__
                            1. __Higher Predictive Power__ (general, weighting slices or in curated datapoints)
                                1. __For models__ (e.g., autoML using current model as starting point: tries different hyperparameters, architectures (Model-conditioned NAS) & batch ordering)
                                2. __For prompts__ (e.g., autoPrompEngineering using templating, learning & LLM generated prompts)
                            2. __Lower Cost, Latency and Higher Throughput__
                                1. __Hardware-agnostic Compression__
                                    1. __For models__ (e.g., pruning/sparsification, quantization, knowledge distillation & Model-contioned NAS)
                                    2. __For prompts__ (e.g., autoPrompEngineering using templating, learning & LLM generated prompts)
                                        1. __Input Size__
                                        2. __Output Avg Size__
                                2. __Hardware-dependent Optimizations__
                            3. __Higher Robustness__
                                1. __To Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s__
                                    1. __For models__ (e.g., causal models, on-the-fly cephs3adaptiveness)
                                    2. __For prompts__ (e.g., lowering prompt variance)
                                2. __To Adversarial Attacks__ (e.g., robust learning, aplying filters, applying rate limiters, attack monitoring, data validation)
                                    1. __For models__ 
                                    2. __For prompts__
                            4. __Higher Fairness__
                                1. __For models__ (e.g., constrained optimization, changing loss function, applying filters)
                                2. __For prompts__ (e.g., prompt engineering, applying filters)
                            5. __Higher Uncertainty Estimation (includes OOD Detection): Calibration__
                                1. __For models__ (e.g., fine-tune model as to optimze calibration, inference dropout, multiple calls, in training, give bigger loss for when the model is confident and wrong, etc)
                                2. __For prompts__ (e.g., one method is to do multiple calls & evaluate the distribution of output embeddings)
                    3. __Data & Model-related__
                        1. __Discovery Platform__ (data source connectors + UI + custom artifact assembly support)
                            1. __Data Discovery & Retrieval__ (Helps clients get the data they need from multiple data sources (e.g., Ceph, S3 files, repos, self-managed DBs, Data Lake, Data Wharehouse, LakeHouse, Distributed Filesystem, Streaming Data, etc). For complex non-supported query pipelines it supports giving you the query instead of the data, so that you can edit it. ALso gives you memory footprint of keeping all the data in-memory. Also, helps clients by getting directly fragments of their data related to a topic and answering questions about their data.)
                            2. __Services Discovery__ (A frontend that lets you acess all internal services in a single-interface. Helps you discover (self-hosted & managed) APIs that can be helpfull for production systems to use or offline by engineers. Should give you all API specifications along with metadata such as: self-hosted (externally accesible, within cluster or within VPN) or managed; authorization procedure; link to discuss with who maintains the API. Also can contain example client code for the most used languages or even make calls itself (e.g., LDAP protocol).)
                            3. __Artifact Discovery__ (find them, visualize & understand (e.g., with cards))
                                1. __Your models__ stored in a Model/Prompt Registry or some other storage location
                                2. __Your pipelines__ stored in a pipeline/graph registry or some other storage location
                                3. __Open source Artifacts (Pretrained Models & also data)__ (e.g., connect to Hugging Face Hub, ONNX Model Zoo, github/gitlab/gitea (self-hosted), etc)
                            4. __Package Discovery__ (your internal PyPI (if using gitlab its just gitlabs' package registry))
                        2. __Profiling__ (fine-grained breakdown of metrics)
                            1. __What to profile__
                                1. __Computational Artifacts__ (e.g, models, pre/post processors, serverless functions)
                                2. __Services__
                                3. __Pipelines__
                                4. __Systems__
                            2. __Profiling Metrics__
                                1. __Performance__
                                    1. __Latency__
                                    2. __Throughput__
                                    3. __Memory usage__
                                    4. __Disk usage__
                                    5. __Networking usage__
                                    6. __Energy usage & % of battery (if any) usage__
                                    7. __User mobile data usage__
                                    8. __Effective TFOps__
                                    9. __Effective IOPs__
                                    10. __Netwroking speed__
                                2. __Cost__
                            3. __Profiling Context__
                                1. __Avarage Load__
                                2. __Peak Load__
                                3. __Type of Profiling__
                            4. __Profiling Compute Hardware__
                                1. __Local Machine__ (can be usefull for memory profiling (assuming you are running the deployables on same type of processor in production (e.g., using GPU in local machine & GPU in prod))
                                2. __Production-like Machine__ (recommended for latency & throughput profiling) if its compute power is very different from your local one)
                                    1. __Cloud__ (e.g., Linux 16 core x86 machine with 1 NVIDIA Volta GPU)
                                    2. __Edge (simulated edge environment running on the cloud)__ (e.g., Raspberry Pi 4)
                        3. __Container Image Security Scanning__ (scan images to make sure they dont have malicious software)
                3. __Offline Notebook Features & Model Experimentation Environment__
                    (1) Experimentation Services:
                        (1) All-in-one
                            (1) Platform: Kubeflow, MLFlow, ClearML, Polyaxon, SuperDuperDB
                            (2) Framework: ZenML, CLAIMED, Kedro, Dstack, FuseML
                        (2) Piecewise
                            (1) Experiment Tracking: (1.1) open source: sacred, Tensorboard, Aim, MLflow tracking, dvc/fds + CML, Pachyderm, truelens, fasttrackml, MLTRAQ; (1.2) paid: W&B, DagsHub
                                (1) LLM Experiment Tracking: (1) open source: trulens
                            (2) Working with Notebooks: JupyterHub/Marimo + Jupyterlab-requirements + Papermill/Ploomber + pyparallel + parakeet + testbook + nbdev + elyra + jupytext + ReviewNB + NBQA + lmk + lineapy + nbdime + jupysql
                            (3) Pipeline builder/Virtual Workflow Orchestrators: just helps build workflow-orchestrator-agnostic pipeline, you then need a backend workflow orchestrator to execute it (Tools: Hamilton, Kedro, ZenML, CLAIMED, Couler, Metaflow, sqlflow). Note: not strictly necessary, since workflor orchestrator already help you with this aswell. Note 2: optionally you can compile it to a single task istead of a pipeline (i.e. pipeline as a task of other pipeline)
                            (4) State Machine builder: pipelines are a special case of state machines (Tools: Burr)
                            (5) Workflow Orchestrator/Pipeline Executor (_Note:_ every fully fledge exeprimentatio tool offers itw own inside) (very important to decouple experimentation stages in an organized manner. 
                                (1) If you have small data you can do computation on the the same machine you code: dvc/fds pipelines, ploomber; true workflow orchestrators (using them locally) such as: General workflow Orchestrators: Argo/Hera, Prefect, Airflow, Dagster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau, dagger; (2) Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro; (3) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo/Hera), mleap, flyte, sematic; (4) Data-driven Workflow Orchestrators: Pachyderm; (5) Notebook-based Wrkflow orchestrators: Ploomber
                                (2) If you have bug data/heavy workloads then you need to decouple the place of computation from the place of writing code and do computation in a dedicated cluster, and for that, you will need a true worflow orchestrator (metioned above))
                            (6) Additional setups: 
                                (1) Private datasets: Pysift
                    (2) Distributed Processing Engines: Ray, Spark
                    (3) Data Storage Abstraction (to make pipeline independent of actual storage implementations that may vary betwen ci/cd stages): Kedro
                    (4) Preprocessing Libraries:
                        (1) Low-level: unstructured, sparrow
                            (1) Numerical workloads: (1) CPU: Numpy; (2) GPU: cuda-python, cupy, Numba, MatX
                            (2) Dataframe: 
                                (1) Engines: Pandas/Modin/Polars/Dask/cuDF(CUDA's GPU dataframe library)/Daft(non-tabular data (files) distributed dataframe library)
                                    (1) Zero-copy data formats (with libraries to use them): Arrow, Iceberg, safetensors, avro, parquet
                                (2) Single Interfaces to multiple Engines: ibis
                            (3) Data Validation: JSON Schema, Great Expectations, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation, DVT, data-diff, cerberus
                            (4) Profiling: ydata-profiling
                        (2) High-level: (1) General: optimus, scrub, dataprep; (2) CV-specific: DataGradients
                            (1) EDA
                                (1) General: Rath, ydata-profiling, sweetviz, spotlight, TaskWeaver
                                (2) CV: Kangas, fastdup, Voxel51, DendroMap
                                (3) Visualization: matplotlib, seaborn, facets, altair, bokeh, folium, holoviz, nomic, pygwalker
 
                            (2) Data Wrangling
                                (1) General
                                    (1) Data Augmentation: YData Synthetic, Gretel Synthetics, AugLy, nlpaug, TextAttack, audiomentations, albumentations, inflect, snorkel, textaugment, kornia, batchgenerators, synthea
                                (2) CV: CV-CUDA
                                    (1) Data Management: supervision
                                    (3) 3D Data: cupoch
                            (3) Application-specific
                                (1) Time-series: temporian
                            (4) Feature Engineering: anovos, ppscore, featuretools, Feature Engine
                    (5) Framework Libraries:
                        (1) Simple Models: Scikit learn
                        (2) Complex Models
                            (1) Powerhorses:
                                (1) Single-machine focused
                                    (1) Low-level: Pytorch, TF, JAX, TRAX, AXLearn, MXNet, MNN, Volwpal Wabbit, Burn, candle (rust), MindSpore, DL4J, dlj, BeyondML, mlpack, flashlight, Paddle Paddle, Oneflow, mmengine, jittor
                                        (1) Add-ons:
                                            (1) Distributed Computing
                                                (1) Only Distributed Training (Note: single machine with multiple GPUs already requires it): 
                                                    (1) Single-machine: 
                                                        (1) Framework-specific: TF Distributed Training, Pytorch Distributed Data Parallel (DDP)
                                                    (1) Cluster:
                                                        (1) Your cluster:
                                                            (1) Any model
                                                                (1) Framework-agnostic: Horovod, Analytics Zoo, MosaiCML Composer, spark/koalas MLib, Dask Distributed Training, Petastorm, FedML, training-operator
                                                            (1) LLMs: trlx, Megatron, megablocks
                                                        (2) P2P: Hivemind
                                                (2) Distributed Training and Inference: Colossal AI, DeepSpeed, Paddle, Ray, Apache Singa, FlagAI
                                                    (1) LLMs: BigDL
                                                (3) Only Distributed Inference (Note: single machine with multiple GPUs already requires it)
                                                    (1) Your Cluster: NVIDIA Triton, TorchServe, exo
                                                    (2) P2P: Petals 
                                            (2) Privacy: 
                                                (1) federated Learning: Pysift, TF federated, flower, nvidia clara, substra, OpenFL, FATE, FedML
                                                (2) Differential Privacy: TF Privacy, google/differential-privacy, Pytorch Opacus
                                                (3) Homomorphic Encryption: SEAL, HELib, tf-encrypted
                                            (3) Fairness: TF Model Remediation, TF Constrained Optimization
                                    (2) High-level:
                                        (1) General
                                            (1) Pytorch-based: HuggingFace, Fastai, Catalyst, Pytorch Lightning, Pytorch Ignite, Ludwig
                                            (2) Tensorflow-based: Keras
                                            (3) JAX-based: FLAX
                                        (2) NLP: TextBlob, Spacy, opennlp, flair, Spark NLP, CoreNLP, PaddleNLP, NLTK
                                            (1) Topic Modelling: pyLDAvis, scattertext
                                    (3) Higher-level: AutoML frameworks that you just mess with config
                                        (1) Just Feature Engineering
                                            (1) TPOT
                                            (2) Featuretools
                                            (3) CleanLab
                                            (4) tsfresh
                                        (2) Just Models (NAAS)
                                            (1) ENAS-pytorch
                                            (2) HF Autotrain
                                            (3) HF Autotrain Advanced
                                        (3) Everything
                                            (1) Traditional ML-focused
                                                (1) auto-sklearn
                                                (2) mljar-supervised
                                                (3) PyCaret
                                                (4) SapientML
                                            (2) DL-focused
                                                (1) Ludwig
                                                (2) AutoKeras
                                                (3) NNI
                                                (4) AutoGluon
                                            (3) Time-series
                                                (1) AutoTS
                                        (4) AutoLLM:
                                            (1) autollm
                                            (2) H20 LLMstudio
                            (2) High-level Libraries
                                (1) General: Hugging Face
                                    (1) Add-ons:
                                        (1) Distributed Training & Inference: Accelerate, Optimum
                                (2) Specific 
                                    (1) Domain-specific
                                        (1) NLP
                                            (1) Chatbot: NVIDIA NeMo, FastChat, Rasa Open Source, Lobe Chat, Superagent, Cheshire Cat, Botonic, Tock, wechaty
                                        (2) CV: OpenCV, SuperGradients, NVIDIA DeepStream SDK, Savant, Icevision, Kornia, FiftyOne, detectron2, Lightly SSL, timm, LAVIS, supervision, ImageAI
                                        (3) Time-series: tsai, darts, kats, Time-Series-Library, gluonts, skforecast, pydlm
                                            (1) Hierarchical Forecasting: hierarchicalforecast
                                        (4) Survival Analysis: lifelines
                                        (5) Audio: audiocraft
                                    (2) System-specific
                                        (1) Recommender Systems: NVIDIA Merlin SDK
                        (3) FM-specific Libraries
                            (1) LLM-specific Libraries
                                (1) LLMs (single calls)
                                    (1) Data Management & Dataset Construction: RedPajama-Data
                                    (2) Model Building: HF Transformers, ml-4m, Ludwig, higgsfield, litgpt, dclm
                                        (1) Pre-trained models: openchat  
                                        (2) Pre-training: unilm
                                        (3) Finetuning: HF TRL, LLaMA-Factory, LMFlow, Simple LLM Finetuner, Axolotl, unsloth, langdrive, xtuner, LLM Finetuning Toolkit, OpenPipe, xTuring, LLaMA-Adapter, xTuring, felafax, torchtune
                                            (1) Alignment: NeMo-Aligner, alignment-handbook, 
                                                (1) RLHF: HF trl, trlx, OpenRLHF
                                                (2) DPO: direct-preference-optimization
                                            (2) Knowledge Distillation: DistillKit
                                            (3) AutoFinetuning: LLMstudio
                                            (4) Instruction Tuning: open-instruct, LLaMA-Adapter
                                            (5) Multimodal finetuning: multimodal-maestro, ms-swift
                                            (6) 4 Voice conversion: Retrieval-based-Voice-Conversion-WebUI
                                            (7) Model-specific tools
                                                (1) Llama: llama-recipes
                                                (2) Mistral: mistral-finetune
                                        (4) Scaling: TorchScale
                                        (5) AutoLLM: autollm, LLMstudio
                                        (6) Exploration: languagemodels
                                        (7) Output decoding: entropix
                                    (3) Guardrails (Structure & Evaluation of LLM IO): PurpleLlama
                                        (1) Server-side Enforcement: Jsonformer
                                        (2) Client-side Enforcement: Guidance, promptify, lmql 
                                            (1) Prompt Enforcement: 
                                            (2) Output Enforcement: guardrails, outlines, Faster-Outlines, instructor, lm-format-enforcer, Artificial Intelligence Controller Interface (AICI)
                                    (4) Prompt Engineering: Promptflow, AgentHub, ChainForge, mirascope, Agenta, AI Test Kitchen, AnySolve, AnythingLLM, betterprompt, Chainlit, ChatGPT Prompt Generato, ClickPrompt, DreamStudio, Dify, DUST, Dyno, EmergentMind, EveryPrompt, FlowGPT, fastRAG, Guardrails, Guidance, GPT Index, GPTTools, hwchase17/adversarial-prompts, Interactive Composition Explorer, Knit, LangBear, LangChain, LangSmith, Lexica, LMFlow, loom, Metaprompt, OpenAI Playground, OpenICL, OpenPrompt, OpenPlayground, OptimusPrompt, Outlines, Playground, Portkey AI, Prodia, Prompt Apps, PromptAppGPT, Prompt Base, Prompt Engine, prompted.link, Prompter, PromptInject, Prompts.ai, Promptmetheus, PromptPerfect, Promptly, PromptSource, PromptTools, Scale SpellBook, sharegpt, SmartGPT, ThoughtSource, Visual Prompt Builder, MLFlow Prompt Engineering UI
                                        (1) Automatic Prompt Engineering: YiVal, gpt-prompt-engineer, DSPy, Zenbase Core, textgrad, Agents 2.0, prompt-poet
                                    (5) Local Hosting: 
                                        (1) Just LLM: Open Interpreter, transformers.js, llama-cpp-python, GPT4All, LocalAI, Ollama, LM studio, ExLlamaV2, Dalai, BigDL-LLM, ExLlamaV2, koboldcpp, Xorbits Inference, danielmiessler/fabric, torchchat, TinyLLM
                                        (2) RAG/Agents: TinyAgent, Jan, khoj, UFO, llama-agent, leon, gptme, JARVIS
                                            (1) Can take screenrecord as input: screenpipe
                                (2) RAG:
                                    (1) Imperative: Verba, Anything LLM, Neum AI, gorilla, graphRAG, Cohere Toolkit, R2R, clip-retrieval, HippoRAG, ragbuilder, cognita, mem0
                                        (1) RAG for Tool Use: composio
                                    (2) Declarative (AutoRAG): AutoRAG
                                (3) LLM Orchestrators (mUltiple calls orchestrated)
                                    (1) Types
                                        (1) Agent-less Orchestrators: DSPy, Guidance, mirascope, spring-ai
                                        (2) Agent-full Orchestrators: AutoGen, Llama-index, zep, Langchain, haystack, Semantic Kernel, Dust, IX, MemGPT, BondAI, OpenAgents, SuperAGI, Agent Pilot, griptape, phidata, LLMCompiler, ragapp, CopilotKit, agentkit, gptscript, TaskGen, AdalFlow, KaibanJS
                                            (1) Multi-Agent Orchestrators: MetaGPT, ChatDev, Langroid, crewAI, langgraph, ControlFlow, agent-zero, AgentVerse, swarm
                                            (2) No-code Agent-full Orchestrators: LLMStack, Magick, Rivet, Tribe AI, PraisonAI
                                                (1) Automatic Agent Building: AgentK, ADAS, AutoAgents
                                            (3) Mobile Agent-full Orchestrators: AppAgent
                                            (4) Web Agent-full Orchestrators: Huginn, LaVague, Agent-E, AgentGPT, skyvern, webllama, robotframework
                                            (5) Data Enginering Agent-full Orchestrators: Sparrow, ryoma
                                            (6) Agents as Graphs: langgraph, GPTSwarm
                                            (7) Streaming Agents: livekit/agents
                                    (2) Experimentation: langflow, flowise
                                (4) LLM Interpretability
                                    (1) Token importance: 
                                    (2) Prompt Engineering for explainability:
                                    (3) LLM internal analysis: tuned-lens, TransformerLens
                                    (4) Visualization: inspectus
                        (4) Distributed Hyperparameter Tuning: NNI, HyperOpt, Optuna, RayTune, Katib
                    (6) Visualization: 
                        (1) Models: tensorboard, netron
                        (2) Prompts: cometLLM 
                    (7) Evaluation: Open Source: Phoenix, HF Evaluate, TF Model Analysis, TF Responsible AI Toolkit, Microsoft Responsible AI Toolkit (RAI), Giskard, Learning Interpretability Tool (LIT), PiML, ZenoML, Evidently, AI Verify; proprietary: W&B, CometML, Kolena, Efemerai, Determined AI, Fiddler, OpenLayer, Arize AI, Etiq, Gentry, Credo AI
                        (1) General
                            (1) predictive power: plain framework
                            (2) Fairness: TF Fairness Indicators 
                            (3) Privacy: TF Privacy Tests
                            (4) Interpretability/Explainability: SHAP, InterpretML, Alibi Explain, AI Explainability 360, TF What-If, Microsoft Responsible AI (RAI), interpret, Learning  Interpretability Tool (LIT), PiML, Captum, TF Lattice, nnsight, gemma-scope, OmniXAI
                                (1) NLP-specific: TF language interpretability, ecco)
                                (2) Transformer-specific: Pythia
                                (3) Automated Interpretability: maia, sae-auto-interp, automated-interpretability
                        (2) Specific
                            (1) NLP: langtest, checklist
                                (1) LLM Evaluation: OpenAI evals, deepeval, Bench, Pheonix, HELM, lm-evaluation-harness, truelens, guardrails, promptfoo, fiddler-auditor, ChainForge, benchllm, evidently, llm-autoeval, auto-evaluator, LLMZoo, moonshot, Inspect, empirical, alpaca_eval, chat-arena, lone-arena, opencompass, BIG-bench, Eureka ML Insights
                                    (1) RAG Evaluation: deepeval, ragas
                                    (2) Image Generation Evaluation: pytorch-fid
                                    (3) Hallucinations: hallucination-leaderboard, selfcheckgpt
                                    (4) Software Engineering: SWE-bench, Spider
                                    (5) Security and Safety: modelbench, Purple Llama, garak, PyRIT
                                        (1) Prompt Injections: rebuff
                                    (6) Code 
                                        (1) Generation: bigcode-evaluation-harness, HumanEval, MBPP, and LeetCode-hard
                                        (2) Vulnerability Detection & Repair: Defects4J, Codeflaws, QuixBugs, Common Vulnerability and Exposure (CVE)
                                    (7) vectorDB: VectorDBBench
                                    (8) Science: sciml-bench
                                    (9) speech-to-text: speech-to-text-benchmark
                                    (11) Agents: AgentBench, HotpotQA, ALFWorld, FEVER, WebShop, and MGSM
                                        (1) Desktop Agents: WindowsAgentArena
                                            (1) Web Agents: WorkArena, WebArena
                                    (12) Reasoning: llm-reasoners
                                    (13) Performance: llmperf
                                    (14) LLM-powered Evaluation: prometheus-eval, EvalLM, evalgen, cappy, arena-hard-auto
                            (2) CV: efemarai
                    1. __FM Experimentation__ (assistance with manual prompt engineering (optimizing for model-related benchmarks) & management, fine-tuning)
                        1. __Fine-tuning without regressing:__ fine-tune for new tasks while evalauting performance on previosu tasks to avoid catostrophic forgetting and regressing your model on these previous tasks.
                    2. __Hardware-flexible Experimentation Environments:__ data scientist can provide hardware requirements on-the-fly (e.g., machine with GPU or large memory) and a experemtation environment adapts acoordingly, while maintaining state.
                2. __Online Experimentation Management: Analyze and modify online experiements__ e.g., alter the traffic distribution in an A/B or bandit setting or promote one of the models.
    4. __Cost Optimization__
        1. __On-premise Cost Optimization__ (aka energy optimization, to lower your energy bill)
        2. __Cloud Cost Optimization__ (we monitor your cloud usage to predict cloud bills, suggest cloud savings & analyze tradeoffs with other ML System metrics)
            1. __Other Clouds__ (e.g., _AWS_ costs)
                1. IaaS (per/time (on demand or pre-scheduled plans))
                    1. Machines
                    2. Networking Infrastructure
                2. PaaS (per/usage, per/time, per/users)
                3. SaaS (per/usage, per/time, per/users)
            2. __Our Cloud__ (measuring _Freelunch's_ costs (most useful when using per/usage plan))
        3. __Build vs Buy Cost Analysis__ (experiments with self-hosted and managed to decide which one is cheaper)
        4. __Engineer Cost Optimization__ (we try to identify cost reduction opportunies in personal, by detecting low production/salary ratios of engineers & opportunities to replace their work with third-party software (e.g., data preparation and/or infra maintanance) that abstracts it away from the team & in the end becomes cheaper than the engineer)

    5. __Cloud Management__
        1. __Cost Optimization__
            1. IaaS (per/time (on demand or pre-scheduled plans))
                1. Machines
                2. Networking Infrastructure
            2. PaaS (per/usage, per/time, per/users)
            3. SaaS (per/usage, per/time, per/users)
        2. __Communication Optimization:__ optimizing workloads to be as close as possible to where they get their input and where they send their output; to minimize latency due to communication.
        3. __Security enforcement policies__
            1. IAM
            2. Vestige resources
            3. IP Whitelisting

#### 2. Capabilities of a Platform built with Freelunch

* __Manager/Leads__ can:

    1. __Operate Engineers__
        1. __Knowledge Management & Collaboration__ (create, store, annotate & manage documents (e.g., internal playbooks, guidelines, knowledge-base, design docs, architectural decision records, data models, etc) and integrate with your Notion workspace; search/talk to your knowledge-base; share: reports, custom dashboards, results & ideas in the platform or through your favourite communication tools like Notion & Slack)
            1. __Code Quality__
                1. __Code Review Space__ (better interface than github's interface for annotating and starting threads on how to improve the quality of a commit)
                2. __Special UI for teaching Code Quality Standards__ (instead of reading these in a doc, we provide nice UI where engineers can see examples of low quality code & high quality code in the MLOps context, search best ptractices guidelines according to their subject of interest (e.g. best practices for pandas))
            2. __Handbook__ (create an MLOps platform handbook which documents everything a new member needs to know when joining the team)
        2. __Role-based Access Control (RBAC)__ (roles/groups per ML project (e.g., can be manager/leads, Data Engineer, ML Engineer, Data Scientist, subject matter expert, Stakeholder, Lawyer) where each role has its own access rules; and we provide our own secrets manager for engineers or integrate with cloud secrets managers (e.g., Azure Key Vault, AWS Secrets Manager, GCP Secret Manager) which are central authentication tools that manage all secrets owned by engineers. Each role or specific engineer has responsabilities, owning some part of the ML lifecycle (these can change over time) so that any bug can have at least 1 engineer held accountable for it at any point in time. Also, each part of the ML Operations can have an admin that sets which types of actions he wants to receive notifications before they are performed (he can choose to reject these actions). Note: people can have more than 1 role and different roles can be assigned to people manually or on a schedule)
        3. __Daily Operations__ (this involves things as organizing which ML engineers will operate which parts of the ML System & be responsible for fixing it & reporting all of this. We provde capabilities for organizing these schedules, enforcing them by sending alerts/notifications to the manager/lead if the engineer is not on-call, automatically generating reports of the operations of specific engineers & keeping track of a production bugs queue where bugs can be added, removed, edited, handled and resolved via a ticketing system. Also, each bug has a priority label. Additionally, letting coworkers B, C, D... etc comment on how good is to work with engineer A; for the sake of engineer A to get feedback and improve and also for the manager to see if engineer A is worth the cost.)
        4. __Checklists__ (can configure checklists to spawn when some event happens (e.g., CI aproves PR, which now needs code review). The action only takes place after the engineer confirms that he/her checked it all.)
        5. __User Feeback__ (a page where engineers can see user product feedback on the produts of the company, so that MLOps decisions are always guided by improving user experience. Under the hood, it integrates with the User product feedback System.)

    2. __Ensure ML is aligned with Business Goals__
        1. __Caluculate ML ROI: revenue increase by change in ML System/total MLOps costs that went into that change (personel + compute)__ 
        2. __ML-Business KPI Aligment: set optimal ML KPIs via Causal Inference__ 2 plots power this analaysis: (1) E[P(REVENUE=revenue| do ML_METRICS=ml_metrics)] where ml metrics are normalized; (2) MLOps_Cost(ML_METRIC). (2) is built by first building: (a) ML_Metric(ML_modifiable_property) for many modifiable properties, (b) Cost(ML_modifiable_property) and estimating Cost(ML_modifiable_properties) and Cost(ML_modifiable_properties) from the previous uni-dimensional functions.
   
    3. __Avoid Overloads on Offline Ephemeral Workloads Cluster (Experimentation cluster)__ show in a nice UI the current use of cluster resources & tell Data Scientists they sshould avoid using the cluster if possible, e.g., they can focus on other things such as documentation, system design, code quality, etc. When a Data Scientists faces this, he puts his puts himself on the queue to use the cluster. When cluster resources are available & the DS is the next in line, he is alerted. After the alert, the DS can use the cluster in x amount of time, before it goes to the next in line; he can also choose not to use the cluster now and give place to the next in line, going further down the line. Note: "cluster resources not available" means that avg time to run a standard program of type == to the type of workload the DS wants to use is already max (according to predefined rules). This heppens when resources (processing & memory %) are already full and a lot of programs are running concurrently.

* __ML Engineers & Managers/Leads__ get:

    1. __CI__
        1. __Pre-CI bug detection (AI-powered)__
        2. __Automatically adding intrumentation to your code__ (based on config files you write)
        3. __Documentation linked to code__ (tie sections of documentation to their correponding pieces of code; then alert if code was changed but the documentation tied to it was not)
        4. __Static Analysis__
            1. __Compliance Enforcemenet__ (e.g., IP/Licence Scanning & GDPR-like compliance)
            2. __Quality Enforcement (Linting)__ (codebase standards, you can pick from popular patterns or make your own)
                1. __General__ (e.g. yaml or CUE linting)
                2. __Domain-specific__ 
                    1. __By environment__ (e.g., web or mobile)
                    2. __By language__ (e.g. python or nodejs)
                    3. __By tool__ (e.g. terraform or kubernetes)
            3. __Static Application Security Testing (SAST)__
                1. __Security Scanning of dependencies (SCA)__
                    1. __For code: vulnearable tools__
                    2. __For data & models: compromised artifacts__ (poisoned data & models learned on poisoned data)
                2. __Detection of security anti-patterns__
                    1. __Hardcoded secrets__
                    2. __Unsecure functions__
                    3. __Vestige Resources__
            4. __Service-level input expectations__ (making sure the input is in the expected format and with expected bounds)
            5. __Technical Debt/Maintanability Calculator__ (e.g., based on Google's ml test score or on comments with technical debt hints)
                1. __Expandability__
                2. __Correctability__
                3. __Testability__
        5. __Dependecy Injection (making functions/classes more testable by putting their dependencies as arguments and modifying tests to adhere to this)__
        6. __Other quality enforcements__
            1. __Commit quality enforcement__ (ensuring commit messages follow a certain pattern)
            2. __Documentation quality enforcement__ (ensuring docs have certain non-empty fields)
        7. __Dynamic Testing__
            1. __Testing Quality Evaluation__
                1. __Testing Coverage__
                2. __Mutation Testing__ (artificially introducing breaks into the code and seeing if tests catch them, tests that catch more errors introduced by the changed said to be of higher quality)
            2. __Testing Itself__
                1. __Functional Testing__
                    1. __Unit Testing__
                    2. __Integration Testing__
                2. __Application Security Testing (DAST)__
                    1. __Unit Testing__
                    2. __Integration Testing__
                3. __Performance Testing__
                    1. __Unit Testing__
                    2. __Integration Testing__
        8. __Visual High-level Diffs__ (visualize diffs at a high-level instead of seeing source code diffs, thi makes it easier to understand changes)
    2. __Human-in-the-loop (HIL) Prediction Reviewing__ (this only applies for offline ML models that are performing some crucial task. In many cases, you want to send the predictions for review to some subject matter experts before using it do some action. The prediction reviewing capability gets predictions from a storage place & allows, through a frontend, ml engineers & subject matter experts to accept or modify predictions. If predictions are modified, the new correct label is stored in the Feature Store togeterh with the metadata of the modification (who modified and when it modified))
    3. __System-wide Monitoring, Observability & Alerts/notifications: sending, analyzing & acting on ml metadata__
        1. __Monitoring: detecting something wrong in real-time__ (
            1. Types of monitoring objects: 
                1. Distrubuted tracing. 
                    1. Reactive and System-wide: Main artifacts chains: IO samples or "runs" of your system (where a run is all IO that happens between components, based on a triggering event, and where the last output is the one that doesnt trigger anything to run or based on a custom condition (e.g., frm user request to delivered prediction, from new raw data to stored featured datapoints in feature store, from configuration change in State System to new config log & new IO handling of some component/subsytem)). Note: distributed tracing requires attaching IDs to applicaiton protocols and propagating the same ID in downstream app protocols caused by the original one. This generally requires support from the networking app layer API tool (e.g. FastAPI) which needs to see which requests/streaming messages trigger other requests/responses/streaming messages;
                    2. Active and component-wide: IO flows fo a specific component interacting with other compinents to analyze it. E.g., analyze behaviour of an LLM Agent. 
                2. Metrics: nice visualizations of metrics, receive monitoring action suggestions & can setup automated monitoring actions based on metrics (e.g., rollback, promotion or deploying lifeboat models, reconfiguring data collection system to get more data of some specific type (slices), or in a major crisis rollback of the entire system (reverting to the last stable commit);banning users based on adversarial attacks coming from them); changing the traffic going to a model in a online experimentation (e.g., bandit or ab testing) pardigm).
                    (1) Ready-metrics (e.g., input data validation checks or performance of a component): metrics calculated in the components/subsystems and sent to the Monitoring System ready
                    (2) Metric data (e.g., or model inference <x, y_hat, y:> tuples used to calculated Data Distribution shift (Note: sudden shifts are often indicative of data bugs) and preditive rolling predictive power): data sent components/subsystems that will be use to calculate a metric in the Monitoring System

            2. Main things to monitor:
                1. Low-level (typically know as more general Devops monitoring or IT monitoring):
                    1. Cluster (cloud/prem/edge) health (resource consumption & traffic);
                    2. Cybersecurity: identifying vulnearabilities and attack attempts & breaches; activation of defense & remediation (aka incident response) protocols.
                2. High-level (typically known as ML monitoring): system workings: data collection (e.g., detection of data poisoning attacks), annotation (can be more than just labelling) (speed & quality), offline experimentation (in-training, speed, % make to prod), models (predictive power, uncertianty estimaiton, degenerate feedback loops, explanations, adversarial attacks), online testing, service health (availability, latency & throuhgput), data pipelines (examples, schema, consistency, size, volume, expectations, distribution shift), online experimentation & CI/CD;
            
            3. From imperative monitoring actions to declarative desired ml system state (state of artifacts + config state) change: ml engineers dont need to know how to do some action to fix the system, they just need to give their desired new state, and the actions are done under the hood for him (like Terraform/Terragrunt/terramate and Kubernetes does).

            4. Equipped with Statefull Monitoring (e.g., sequential calls for a streaming LLM, calls for a RL controller, calls to Statefull ML Services such as LLM services with finetuning and long-term memory)

            5. Components
                1. __Streaming Dashboard__ (visualize streaming data)
        2. __Observability: debugging & finding improvements__
            1. __Metadata-based Observability:__ reconstructing the working of your system with a nice UI, based on timestamped metadata (artifacts + metrics + logs (telemetry)), to visualize cronologicaly PRs, deploys, events, actions, configuration changes, processing & artifact generation. Its very usefull to see if the system is working as expected. ML Engineers start with a high level UI and can zoom in parts of the system, as well as, slow down/speed up/replay it.
                1. __ML Metadata Store__ (stores ml system metadata: runtime checks and tests (or just the raw data, so that the ML Monitoring System does the actual verification), logs, metrics (or just the raw data, so that the ML Monitoring System does the actual computation) and traces and even maybe control flows)
                2. __ML Metadata Transport__ (connects metadata producers (system components) to metadata consumers (Observability platform) via streaming)
                3. __Streaming Dashboard__ (visualize streaming data)
            2. __Simulation-based Observability: running simulations & performance analysis in background__ (where you do more fine grained monitoring & analysis to your system running in a safe cluster (actual cluster or virtual (simulated) cluster), to make it transparent & get more insights into the root problems. We give you a nice emulator UI for visualizing your ML system working, also do performance analysis: (throughput/latency) profiling of the whole system, identifiying bottlenecks and underoptimized components). 
            
            Includes UX emulator, for observing real user experience with the ML System from within the product!

            It is also useful for some engineers in your team to deploy & operate your system in the background safe cluster for a bit before deploying to production, if major chages were made or you need to train them on incident response. This is more important if the major changes were to human-interfaced capabilities of the sytem such as experimentation & monitoring itself, or new engineers enter the team. 
            
            Also used by AI MLOps Assistant do debugging (root cause analysis) of real-time monitoring problems. 
            
            _Note:_ when deploying your Ml System to the background cluster (actual cluster or virtual (simulated) cluster) you might ask, wont my external services (e.g., Storage (data lake, wharehouse, S3, etc) & Processing (any SaaS)), which will be used by both production & background cluster (actual cluster or virtual (simulated) cluster), be overloaded? 

                1. If these services are public services: as part of Freelunch specific configuration you should write scaling config for your external public services and leave it to our autoscaler (that will listen to production use, determine the production performance, then deploy to the background cluster (actual cluster or virtual (simulated) cluster), listen to prod performance again, determine the gap in performance, then scale & verify performance...until the performance matches the origianal prod performance)

                2. If these services are private services: then you need to include a copy of the service inside the ML System, so that it can be deployed together.
            3. __State System__ (makes it possible to recover any past state of the production system, making it possible to do replays)
                1. __State management__
                    1. __System State (config, schema, data) Store: versions & distributes state__ (makes it possible to go back to any state of the system) Stores versioned config (e.g., data imputation method to be used by feature store, error handling protocol to be used by prediciton manager, DAGs to be implemented by Data Pipelines, etc. configs can be changed in your repo or the state store itself (by athrorized clients) and then distributed to systems or can be done independently by the systems and then sent to the state store so that it can be updated.), schema (db migrations and file format changes), data (backups & patches (telemetry)) and deployment. _Note:_ most recent data is stored in memory for fast access.
                        1. Config Store (Tools: Apollo, NACOS, Salt, Configu, OmegaConf Hydra, DynaConf)
                        2. Schema Store: schema itself and schema expectations (Tools: Liquibase, schemahero)
                        3. Data Store (Tools: (1) Event storing: EventStore; (2) Backup versioning: LakeFS + object storage or DB)
                        4. Deployment Store (e.g., k8s deploy state)
                        5. API Store (stores history of APIs for each service)
                    2. __State change message broker/stream transport__ (messages of state changes (config and maybe data changes produced either by the systems themselves (system state store then consumes) or by the system state store due to humans updating it (systems then consume))
                2. __Replay job:__ receives as input <start_time, end_time> to replay the system & deploys the system in a background cluster (actual cluster or virtual (simulated) cluster)
                    1. __Config replay:__ dispatches timely triggers to the state store to update system configs
                        2. __Data replay:__ 
                            1. __Data Backup Shortcut.__ Uses available backup data + patches since that backup to reconstruct data state of services without having to go through the whole lineage chain that start with streaming data in data collection system
                            2. __Event Simulation.__ Simulates data sources by using historical event data (stored in Data Lake), being timely "produced".
                                1. Simulated data collection stream producers and frontends
                                2. Simulated storage changes (e.g., repo pushes, S3 updates or DB dumps)
        3. __Alerts/notifications__ (where you get important alerts/notifications via Slack/Email/Whatsapp/Other that point you to some place at the Monitoring System)
            1. __Engineer time devotion estimation:__ we send alerts/notifications to engineers which is a just a one-click checkbox that the engineers picks that describes what he is working on at the moment. We draw a lot of these samples an estimate a distribution of where engineers are spending most of their time on. This distribution is useful identify workflow bottlenecks & for addressing new guidelines or tool adoption to make engineers do less manual work on tasks that occupy big chunks of their time.

    4. __Dynamic Enforcement of communication rules between services__ (when a service/task asks KMS for a secret to be able to communicate with service B through requests, KMS checks its rules for this specific communication and decides if it will grant service/task A the secret. These rules can be dynamic, meaning, that you can make any rule that depends on the _state of the system_. The rules can be defined via diagrams. The state of the system is <configurations, artifacts> and is modified in response to external events (data ingestion or requests) triggering execution of several components in pipeline fashion. And you can monitor this to see when service/task that got rejected and what was their rejection handling procedure. These rejections can indicate that you havent coded the rules correctly, but the real goal is to identify implementations outside permissions & attackers.)

#### 3. Capabilities present both in Freelunch and the Platform built with Freelunch

* __Managers/Leads__ can:

    1. __Manage the project__
        1. __High Level:__ help with project selection, do project resource estimation, manage project roadmaps/milestones, schedule meetings with Zoom/Meets API, see developer/data scientist/engineer productivity (based on their repo contributions, artifact production, bug detection, config/monitoring actions, integration setup and anonymous coworker comments data), setup developer/data scientist/engineer onboarding steps, get anonymous feedback from your ml engineers (important to address reasons making them unhappy).
        2. __Low Level (like Jira):__ manage state of features (todo/in-progress/done), issue tracking & branch visualizations, highlight release (PR to release branch)s & link to their CI/CD report and check code reviews.

    2. __Track Project History (Activity Logs)__: the history of all Evaluations, metrics reaching tresholds (production bugs), Automated & User actions (actions that modify the ML System (acompanies notes) and internal Freelunch actions. Also contain metadata such as user ID, role, project, IP adress, Authentication method used) can be retrieved, exported, downloaded & stored in external storage. The history of the project is usefull for: plotting the evolution of the project; measuring the amount of manual & automated operation actions; see who did what; undo operation actions (before action & after action have the same commit, so git/github/gitlab/gitea (self-hosted) alone wont be able to solve our problem

* __ML Engineers & Managers/Leads__ get:

    1. __AI-assisted MLOps: *MLOPs Copilot*__
        1. __Development__
            1. __High Level Assistance__ 
                1. Question-aswering. About: mlops in general, your project, your codebase, design patterns or how to use Freelunch.
                2. Help with being up-to-date with new research papers & tools, by searching new rising tools/data/models/papers, filtering and summarizing them for you; via something like a RSS feed. 
            2. __Low-level Assistance__ 
                1. Setups (e.g., "give me a spark/koalas cluster setup with enough computing power to make x featured datapoints/second, given my spark/koalas script" or "give me minimal instance that can solely run my model with x amount of latency"); 
                2. Model/Prompt building (e.g., ML engineer writes tips on how to build the model interactively along with prior knowledge of the process being modelled, then the AI MLOps assistant does the model building, and at the end presents the final model and the experiments done with explanations; or the ML Engineer wants to improve some prompt)
                3. Data Quality assurance
                    1. Detect: mislabelled datapoints or batches, innapropriate data
                    2. Infer: shcme of schemaless data (schema mining)
                4. Codebase Migrations
                    1. Language migrations (e.g., from java to python)
                    2. Tools migrations (e.g., from pandas to ibis)
                    3. Custom migrations (e.g., migrating an existing MLOPs platform codebase to a Freelunch MLOPs platform codebase)
                5. Autonomous pushes to the repo (can push data, config, code, docs) based on:
                    * Following the company's SOPs: explanation given to ML Engineers on how to use the repo
                    * Lerning by example and previous experiments: previous pushes and evaluation suite results (we want to improve the ML system by using better tools, already used tools in a better way, wirint stuff from scratch)
                    * Detecting and fixing (with explanations) bugs: these can be engineer provided bugs, automatically detected bugs or Copilot-caused bugs: monitoring and engineer feedback data is used to help solve these bugs
                    * Learning methods: learning from internet and comapany internal materials (e.g., public: oss tool docs, blogs, tutorials, discord/slack groups, papers, talks, courses, books, etc; private: notion docs, internal tool docs, internal tutorials, etc)
                    * Leveraging feedback to self-improve: debugging for itself using feedback: (1) human feedback; (2) error & failed test traces;
                    
                        The when the output --> feedback --> output iteration process is over and a usefull output is computed: (<<output_i with some error j, feedback on error j>, output with error j fixed and no new errors introduced> pairs for each iteration can be used as training data to improve the LLM on specific issues.

                        Finetuning data for the direct mapping <first output with errors, feedback> --> usefull output can also be obtained; but first the LLM needs to compare the final output with the first output do identify all the errors it had that were fixed (because engineers will often just point out 1 error that invalidates the solution, but usually there are more, and many of these wont appear in automated testing).

                    It will be mainly doing these types of things:

                    1. Assistance to Human changes
                        1. Compliance-free copy/paste (generates code that does the same thing but has enough differences)
                        2. Identifying CI tests that dont need to run (e.g., if changin a text by another, you dont need to run any CI tests, just code review)
                        3. Automated Code Review. 
                            1. Pre-CI bug detection (faster than running integration tests & detects problems you might have forgotten to test for. e.g., after updating one model & evaluatig it on the test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant), we see if its output distribution had a stat significant change, if yes, downstream models will probably suffer from input (covariate) shift. Or when you change training data pipeline & features schema, the model & monitoring for this part should be changed accordingly);
                            2. Pre-CD bug detection.
                        4. Debugging
                        5. Refactoring
                            1. For Code Quality
                            2. For Performance
                        6. Security Testing (complex vulnerability analysis and attack vectors that cannot be automated with traditional software)
                        7. Documentation: detailed explanation (like a teacher would do) of the platform workings and for each change made via beatifull documentation.

                    2. Suggestions (e.g. how to improve a prompt)

                    3. Interactive instruction (interactive) following for ML system changes (e.g., "change my ML System as to enforce schema checks & default imputation" and it might respond (this is a nice visualization of your ML System, please write the chema checks & default imputation that you want in this format... and I will implement them, show you the actions I took (push to codebase and/or prod config actions) show you that is is working in the background cluster (actual cluster or virtual (simulated) cluster)))
                        1. Conversion between patterns: converting your MLOps platform implemented using pattern A to another implementation using pattern B; where A and B can be any pattern (Note: if B needs things A doesnt provide then you will end up with an implementation template
                    
        2. __Operations (AIOps)__
            1. __Monitoring: detecting problems__
                1. __Automatic treshold adjustment for metrics & alert triggers__ (based on ML-Business KPI Alignment and Succesfull & Unsucessfull Alerts/notifications)
                2. __Security__
                    1. __Anomaly detection__
                        1. __Model-related: features and predictions__ (analysing features and predictions to find anomaly situations & investigate)
                        2. __System-related: logs (telemetry)__ (analysing general monitoring & activity log data to find potential bugs not accounted for a priori)
                    2. __Adversarial Attack Detection__ (both instance-based an user-based) (can be trools or real attackers)
                        1. __Attack on Training: Direct Data Poisoning__
                        2. __Attack on Inference: Input Manipulation or Injection__
                            1. __Black-box__
                            2. __Grey-box__
                            3. __White-box__
                    3. __Innapropriate output detection__
                3. __Cloud Cost: cost reduction suggestions__
            2. __Observability: solving problems__
                1. __Debugging: Root Cause Analysis__ (identify problem A in service S --> check upstream services and see if the root problem is actually there and is affecting downstream service S --> do this until root problem is identified. This is both via production system metadata and messing around with the mirror system in the background cluster via simulation-based observability)__
                2. __Monitoring Action Suggestions__
            3. __Security Testing (complex vulnerability analysis and attack vectors that cannot be automated with traditional software)__
            4. __Customer Success__
                1. __Summary of customer feedback to keep ML team grounded on actual end-user problems__ (with pointers to actual data)
            5. __Evaluation__
                1. __Requirements__
                    1. __Security__
                        1. __AI-Powered Cybersecurity Attacks__
                            1. __Black/Grey Box attacks__ (see if deployed system is secure to external attacks. Automated solutions are often called Breach & Attack Simulation (BAS) tools.)
                            2. __White-box Attacks__ (see if deployed system is secure (or responds well) to internal attacks.)
                        2. __Innapropriate outputs detection__
                2. __Benchmarks__
                    1. __Precise Concept Editing/Model Surgery (e.g., changing the parameters to remove some type of bias, more efficiently and reliably than fine-tuning it with data against that bias)__
            6. __Technical Debt__
                1. __Documentation Quizzes__ (based on codebase & docs, generate docs quizzes with questions & answers. This is made to check if the documentation is good enough to enable any other engineer to use, debug & make improvements.)

    2. __Clarity & insights__ Full benchmarking reports, get interactive visulizations regarding the real-time (or point-in-time) state of the system & artifacts of the system, support for building feature catalog & model card docs, build lineage reports of models (a specific & fully descriptive storyof how the model was built & its evaluations)

    3. __Integrations with your favourite tools & languages__ (provide clients libraries for all major languages; connect to your/third-party: artifact stores, monitoring, business analytics tools, model optjmization tools, etc; update Hugging Face Hub with ML Artifacts; get alerts/notifications via multiple channels (e.g Slack & email), update/get company knowledge in Notion)

    4. __Compliance Management__
        1. __External__
            1. External Audit results
            2. Certificates Management
            3. Agreements Management
        2. __Internal__
            1. Periodical Internal Auditing (e.g., data privacy: role-based access control, GDPR, sensitive data handling; prediction explainability) & Report generation
            2. Report Management
            3. CI static code testing for compliance
</details>

<details>
    <summary><b>Evaluation of your MLOps</b></summary>

### :seedling: __Intro: evaluation of your MLOps__

''If you cannot measure it, you cannot improve it''.

#### __Evaluation Approaches__

1. __Automated__
2. __Partially automated (human plays a role)__
3. __Manual: Red teaming__

#### __Evaluation Suite: Requirements and Benchmarks__

Evaluations can be:

1. Automated
    1. Not AI-powered
    2. AI-powered;
2. Manual

#### :no_entry_sign: __*Requirements*__

These differ from benchmarks in the sense that we need to solve them all 100%, there are no trade-offs involved. But they do not call for further optimization beyond what we established to be sufficient (as opposed to benchmarks). They are fairly independent of each other. These requirements are: __Code Quality, Security & Compliance__. Below, in more detail

1. __ML-agnostic Requirements__
    1. __Offline Requirements__ (the system doesnt need to be operating)
        1. __Non-simulated data scenarios__
            1. __System-related__
                1. __Codebase Quality__ (liting standards, project-specific patterns (e.g., dependency & config management), general patterns (reproducibility, code reuse, tests coverage, design patterns, SOLID priciples), simplicity (e.g., fusing files or functions, making custom-scripts into libraries), clarity (e.g., variable names, comments & breaking into multiple files/functions)))
                2. __IP/Licence Compliance:__ identifying & quantifying risky copied code used to make/operate the ML System.
                3. __Security__
                    1. __Secure Systems__
                        1. __Static Security Testing:__ identifying & quantifying insecure code just by looking at the code.
                            1. __Open source Dependency Scanning__
                            2. __Proprietary Code Scanning__
                4. __Backwards-compatibility__
        2. __Simulated data scenarios__ (uses simulated data for evaluation)
            1. __System-related__
                1. __Integration tests__
                    1. __Regression tests__
                    2. __Interface tests__
                    3. __End-to-end tests__
                    4. __Service Backwards-compatiblity tests__
                    5. __Smoke tests__
                    6. __Crucial performance tests:__ load, scalability and soak tests
                2. __Security__
                    1. __Privacy__
                        1. __Compliance__
                            1. __GDPR-like__
                                1. __Users can delete all personal data at any time__ (systems needs to delete all data on user-x upon request)
                                2. __Users can get a copy of all personal data at any time__
                                3. __Users can update personal data at any time__
                                4. __Record of consents given__ (by whom, which preferences were expressed, which legal or privacy notice they were presented with at the time, and which form they were presented with at the time)
                                5. __Be able to detect, report & investigate personal data breaches__
                                6. __Personal Data can ony be accessed by authorized people__
                                7. __Audit Report Generation & Management__
                    2. __Integraty__
                        1. __Testing__ (Automated solutions are often called Dynamic Application Security Testing tools, but they still cant automated a lot of scenarios.)
                            1. __Penetration Testing__ (see if deployed system is secure to external attacks. Automated solutions are often called Breach & Attack Simulation (BAS) tools)
                            2. __Insider Threat Testing__ (see if deployed system is secure (or responds well) to internal attacks.)
                            3. __Fuzz Testing__ (see if deployed system is secure to edge cases)
                        2. __Audit Report Generation & Management__
                3. __Constrained environments__ (using Environment mimicking tools (e.g., Selenium or Cypress for browsers, EdgeCloudSim, EnvisEdge or Auptimizer for edge devices) to simulate hardware (computing & networking), OS (if any) & more constraints (if any; e.g., browser constraints). Uses cases: deployments on: desktop, mobile, embedded devices (microcontrollers with OS, microcontrollers without OS, FPGAs and ASICs), browsers, wearables (e.g., glasses, watches) and other devices (e.g., TVs, interactive panels, etc)
                    1. __Inference__ (memory usage, power consumption, inference time) (contraints: discontonuos function, only need to be below treshold (memory usage in single-task microcontrollers (no OS)); goals: continuous function, need to optimize (e.g., power usage after treshold is reached)) (small models: Microcontrollers (memory, power, processing speed); medium models: Mobile (memory, processing speed); big models: any cloud instance (memory) (on a fixed high-performant instance: assuming same <machine, OS, processor used, cgroup>)). (_Note:_ has to do various runs and compute <mean, std deviation> because of stochasticity of hardware implementation)
                    2. __Training__ (memory usage, power consumption, training time) (on a fixed high-performant instance: assuming same <machine, OS, processor used, cgroup>) (_Note:_ has to do various runs and compute <mean, std deviation> because of stochasticity of hardware implementation)
                        1. __Training from Scratch__
                        2. __Retraining (Hot-start)__

2. __ML-related Requirements__ (_Note:_ more & more things are becoming compliance requirements as time goes by)
    1. __Non-simulated data scenarios__
        1. __System-related__
            1. __Compliance__
                1. __IP/Licence Compliance__ (use of Open source models & data)
            2. __Security__
                1. __Open source Model Dependency Scanning__
    2. __Simulated data scenerios__
        1. __Model-related__
            1. __Privacy__
                1. __For Generative Models: privacy-preserving outputs__
            2. __Compliance__
                1. __Explainability/Interpretability__ (in some industries, e.g.,  Financial & Medical)
                2. __For Generative Models: privacy-preserving outputs__
        2. __System-related__
            1. __Security__
                1. __Integraty__
                    1. __Attacks that exploit AI fragilities__
                        1. __For Generative Models__ 
                            1. __Prompt Injection__
                                1. __Integraty attacks__
                                2. __Privacy attacks__
                                3. __Availability attacks (for agents)__
                            2. __Inapropriate Outputs__ (Model shouldnt give inapropriate outputs (applies to Human-native data: NLP & CV & Audio). (e.g., spam, incentivizing going agaist law, slurs, pornography, etc) (Note: need ML Models doing this also, because we cant harcode for all the innapropriate scenarios)
                        2. __Influence Surface of Models__ (if your model outputs are succesfully manipulated by an attacker/troll: which other systems can get affected by this, how severe the downstream effects can be on downstream components/systems that function based on this output (directly or indirectly). E.g., currently if you are using LLMs, you want dont want to let them mess with very important data & do irreversible things)
                        3. __Dataset poisoning Attacks__ (indentifying and correcting poisoned data and/or malicious users, or being robust to it)
                        4. __Adversarial Robustness__ (robust to integrity attacks, using adverserial inputs that make the model output the wrong thing)
                    2. __Dangerous Misuse__ (cant allow using models to do bad stuff)
                    3. __Unsafe Behaviour__ (in this case, the model is not attacked, but is generally naive and has a poor understanding of human values, leading to dangerous scenarios (e.g., explaining to someone how to make a bomb or giving support to a 12 year old going to the house of a 30 year old stranger))
                2. __Privacy__
                    1. __Compliance__
                        1. __GDPR-like__
                            1. __Users can reject certain types of use of their data at any time:__ the most notable and difficult case being: when data is used to train ML Models.
                        2. __Very sensitive data (e.g., medical data)__
                            1. __Data can only leave organization if differentially private or encrypted__
                            2. __Data can only leave user device if differentially private or encrypted__
                    2. __Dataset Reconstruction Attacks__ (can be white-box (nowledge of the model) or black-box (no knowledge of the model)
                3. __Fairness__
                    1. __Not Only/Only serve certain publics__ (e.g., avoid children or old people)
            2. __ML-related Integration Tests__
                1. __Training-Serving Skew due to:__ (unwanted differences in how the model is trained and how it is doing inference)
                    1. __Data Leakage__ (the model should be trained only with features available at inference time)
                    2. __Data Pipelines__ (training data pipeline & inference data pipeline are logically different)
                2. __Reproducibility__
                    1. __Deterministic jobs__ (same input 2 times, same output needs to hold (e.g, model training))
                    2. __Stochastic jobs__
                        1. __Stochastic in the end jobs__ (same input 2 times, same outputed distribution needs to hold (e.g, model inference))
                        2. __Stochastic in the middle jobs__ (same input n times, aproximate output distribution needs to hold)

#### :checkered_flag: __*Benchmarks*__

The Benchmarking suite for a specific project can have two types of benchmarks in it: Ml-related (aplies only to ML Systems) and ML-agnostic (need humans to evaluate). These can be further diveded into non-simulated and simulated scenarios. These can be further divided roughly into Model-related and System-related benchmrking. Some benchmarks can appear in more than 1 place, these are singaled with an *x before them; where x is the identifier of the repeated benchmark.

_Note:_ __these are all automated benchmarks, no humans involved__. In stages where humans are heavily involved like Experimentation & Monitoring/Debugging we do proxy benchmarks, meaning that if you do better on these proxies, you will probably do better at Experimentation & Monitoring/Debugging with humans involved; because you will know great guidelines & protocols at least.

1. __ML-related Benchmarks__
    1. __Model-related__
        1. __System__
            1. __Corrected predictive power (relative to a baseline: 0 is just like baseline, 1 is perfect model)__ (vanilla and/or custom objectives (e.g., weighting higher bias errors like mistaking a black person for a gorilla); and with possibility to leverage unlabelled data aswell (e.g weak superivision, unsupervised representation learning or using strcutural priors))
            2. __Uncertainty Estimation (includes OOD Detection)__
                1. __Model knows it own uncertainty: model calibration__ (e.g., grouping predictions or selective classification)
                2. __Model's uncertainty reflects ground thruth uncertainty__ (this cant be measured by comparing predictions of the model to ground trhuth y's like we normally do with expectation (deterministic) models. You must compare the distributions: one way is to do n model predictions and compare to n ground thruth y's. You can do statistical tests to see to get the simlarty between the empirical distribution they come from: e.g., using Maximum Mean Discrepancy (MMD) or a multivariate verison of Kolmogorov-Smirnov hypothesis test. You can also run some sort of matching algorithm to match (one prediction to another ground truth) close datapoints, as to minimize the sum of distances between mactches.). _Note:_ ground truth uncertainty is dependent of the features, you can lower ground thruth unncertianty by adding more informative features and increase it by letting go of informative features.
            3. __Explainability/Interpretability__ (maybe it can be done objectively also, e.g., 1. Distilling model into heuristics: the model has to derive heuristics that if used work well (are able to predict with low error on new cases) and these heursitcs are the explanation users get; 2. Treating it as a control problem (my personal favourite): the programmer is required to code a function that changes the behaviour of the model to some desired behaviour which is only made available after the function is already coded. The function has a time limit to execute (which is modulated by the training time of the model, e.g.,  larger training times get more adapt time). Treating it as a control problem makes sense because people use a lot of tools that they dont know how they work under the hood, but nonetheless they are able to model how they work by just looking at inputs: <state_t, control_inputs, process_noise(state_t)> and outputs: <state_{t+1}> (assuming state == observation) and then desinging their controller control_inputs == controller(state_history, setpoint) (E.g., when driving cars).
            4. __Fairness__ (3 types mainly: 1. having diparate corrected predictive power on different input regions; 2. Having different prediction if change ProtectedFeature == protectedValueA and protectedValueB not sustained by reality (thus incurring consistent error); 3. Having different prediction if change protected feature == protectedValueA and protectedValueB, altough its sustained by reality (thus not incurring consistent error).)
            5. __Model scalability (scaling laws):__ corrected predictive power & resource consumption (how model performance scales with more data. E.g., fro predicitive power: deep learning scales predictve power better than tradiditonal ml algos)
            6. __Intervention Analysis (using Causal Inference):__ can the model estimate the causal effect P(Y == y|do(X == x), W == wk) == E_z[P(Y == y|X == x, W == wk, Z == z)], with the purpose of estimating impact of interventions do(delta_X == dx) from state X == x on the distribution of target delta_p(Y). This is hard to evalute and knowing the right amount of randomized controlled experiments to do this evalution is key. Also, you can be interested in the counterfactual P(Y == y'|X' == x', X == xk, Y == yk, W == w, Z == zk) (which is a generalization of causal effect due to: (1) Z == z making it fully determined; (2) X == x, Y == y which can be used to update the statistical estimators for each of the values of Z before doing the inference. Therefore: P(Y == y|do(X == x), W == wk) == E_z[P(Y == y'|X' == x', X == xk, Y == yk, W == w)] which is itself an expectation over possible model update scenarios, which becomes: E_xy[E_z[P(Y == y'|X' == x', X == xk, Y == yk, W == w)]]) for estimating causality between variables in specific real-world scenarios, so that you can plan your next interventions. _Note:_ in most cases where ML is deployed, it is only deploy: (1) to do interventions with respect to some of the features (e.g., price campaign trying to increase revenue) or (2) help user understand how they can do interventions in order to optmize some other target variable. _Note 2:_ causal inference is tied to modelling natural processes that occur in the real world, not artifical processes running in our brain (or if you will, algorithms). E.g., you  can have a doctor tha tuses some medical features to predict if someone has skin cancer. The algorithm he developed/learned is an algorithm made for optimizing corrected predictive power, so he doesent care if the some feature doesnt cause directly skin cancer through biologicla processes, if it is informative of cancer it is sufficient. With this in mind, if you could actually mess with the persons body to actually change this feature, you would also fool the doctor (because biologically, this change would not affect the chances of this person having cancers)!
            7. __Natural Robustness__ (Natural selected hard cases and ood datapoints)
            8. __Model Backwards-compatiblity__ (we want to retrain a model and the new version dont makes mistakes the older version didnt make. Even if the overall acc is higher, this can ruin the user experience, because the user was used to the model behaving in a certain way.)
    2. __System-related__
        1. __System__
            1. __Retrieval Power (For RAG):__ how well the retriver gets the adequate set of documents for a user input.
            2. __Embedding Power (For RAG):__ (from your custom dataset, separate 20-30% texts to be your test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant). Then feed n pairs of 2 texts to a labeller and his job is to rank similarity of pairs (similar to RLHF). The ranking is a label distribution over the most similar output (e.g., for n == 5, a possible distribution is [0.02, 0.08, 0.1, 0.3, 0.5]) However, feeding every possible pair combination in not feasible, so you need an aproximate algorithm for choosing the pairs. The output of this process should be a normalized similarity score for pairs <a,b> that will be used as ground thruth for the next step, where a,b is not necessariliy a fed pair. The algorithms job is produce as much high confidence similarity pair as possible given contraint of number of feeds. These <a,b> pairs are used to train a constrative learning NN that maps: <pair of texts> --> similarity score which will be used as ground thruth for the embedding model evaluation. Finally, we can evaluate the embedding model: draw randomly pairs <c,d> of texts, compute the embeddings for a & b, compute the distance between the embeddings using your production distance method (e.g., euclidean distance a very good one, but slow; cossine similarity ignores magnitude but is faster)), normalize the distance & then compare with the normalized output of the constrative learning model.
            2. __Robustness to Degenerate Feedback Loops__ (how you deal with moel that train on data that is gathered depedent on itrs previous output. IID not valid)
            3. __Efficiency__
                1. __Model-related Latency & Throughput: Avg & Peak__ (Peak is when at a load peak)
                    1. __Inference__ (typically from request to prediction to client)
                        1. __LLM-specific__
                            1. Time to First Token (TTFT): The time it takes for the first token to be generated.
                            2. Time Between Tokens (TBT): The interval between each token generation.
                            3. Tokens Per Second (TPS): The rate at which tokens are generated.
                            4. Time Per Output Token (TPOT): The time it takes to generate each output token.
                    2. __Training__ (from training procedure specification to model ready in storage)
                        1. __Core Training__ (from initalized model to trained model without making use of hyperparameter tuning)
                        2. __Hyperparameter Tuning__ (from trained model without making use of hyperparameter tuning to best model after making use of hyperparameter tuning)
                    3. __Offline Evaluation__ (from not evaluated model in storage to evaluated model in storage)
                    4. __Annotation (can be more than just labelling)__ (for training purposes: from unlabelled datapoint in feature store to labelled datapoint in feature store; for monitoring purposes: from unlablled datapoint in feature store to label in monitoring system)
            4. __System Robustness__
                1. __Stress Testing (Chaos Engineering)__ (breaking machines/services/configs/artifacts on purpose (note: these are things that can occur by mistake, not attacks) & seeing: (1) Error handling: what happens to the system & what is sent to the user; (2) Detection: if Monitoring can detect it automatic version of common DevOps metric Mean-time-to-detect); (3) Action: what action does the system take to automatically fix itself; (4) If the issue is fixed: how long does it take (automatic version of common DevOps metric Mean-time-to-restore)).
                    1. __*3 Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s__ (due to input shift or process shift)
                    2. __Schema changes__ (e.g., feature schema changes can break models during inference & retraining if not dealt correctly)
                        1. __Schema structure change__ (adding/removing/changing feature types)
                        2. __Schema semantic change__  (changing feature definitions/computation) (''e.g., your model was using years for the 'age' feature, but now it uses months, so the range of this feature values has drifted'')
                    2. __Service changes__ (e.g., if ML API gateway changes, will fronten change accordingly or will it break?)
                2. __Observability__ (Monitoring/Versioning Coverage)
                    1. __Level 1: Transparent (component-complete) Observability__ (All outputs, performance metrics of your components and traces are being sent to the Operations Center)
                    2. __Level 2: Lineage-complete Observability__ (traces are used to build strctured lineages for each artifact generate in history, these lineages are stored in a lineage store controlled by the Observability System)
                    3. __Level 3: Replayable Observability__ (System can be replayed from any point in time to produce point-in-time correct artifacts. Too get his you need: (1) store all ci/cd data; (2) store <data backups + data patches> of statefull components; (3) link these data to a mlops platform version (meta pipeline version); (4) you then need a reaply job that is able to deploy the MLOps platform, restore the ML System to an initial state and trigger replay events.)
            5. __System scalability__ (how the system responds to more traffic and larger datasets/models.)
            6. __IP/Licence Compliant Outputs__ (Avoid Outputs that violates IP/Licence Compliance)
            7. __Training-Serving Skew due to:__ (unwanted differences in how the model is trained and how it is doing inference)
                1. __Data Distribution__ (training data distribution should be close to the inference data distribution (which is avaiable through monitoring. The problem can be noticed at deploy time if the trianing set is not representative of production data, but generally, it is noted after some time when the inference data distribution suffer shift))
            8. __Wise training data selection__ (e.g active learning or curriculum learning. Between a set of possible datasets that can be chosen, se if the dataset that improves the most the model is chosen)
        2. __Process__
            1. __Experimentation Efficiency__ (see how how good of a model same user or autoML (usually known as hyperparameter tuning, but can involve more than hyperparameters) can make, in some time t, using data D, starting with model M & varying the experimentation setting E)

2. __ML-agnostic Benchmarks__
    1. __System-related__
        1. __System__
            1. __Efficiency__ (here hardware is not fixed, choosing hardware type & amount is part of the problem for ml engineers)
                1. __Compute Costs__ (This involves infrastrcture, training, tuning and inference costs)
                    1. __Cloud Costs__ (cloud costs related to IaaS, PaaS & SaaS for a given MLOps platform configuration (e.g., retraining weekly on some specific machine + certain experimentation budget/engineer))
                    2. __Premise Costs__ (energy consumption)
                2. __System Performance Profiling__
                    1. __Coarse-grained: Component-level profiling__ (of all components)  (e.g., feature store latency) (can be calculated from IO of components/subsystems)
                    2. __Fine-grained:__ Intra-component profiling (within a component) (e.g., model profiling) (needs to be calculated and sent by the component)
        2. __Process__
            1. __CI/CD__
                1. __Push-2-Production Times__
</details>

<details>
    <summary><b>Problems when working with production ML</b></summary>

### :dizzy_face: __Problems when working with production ML__

1. __High-level Problems__
    1. __Complexity:__ Multiple Components/Tools, Big Data/Models & a bunch of complex new research & tools popping up every week. ML Systems theselves are complex and research around them is also complex. This make it hard for humans to do risk assesment, find improvement opportunities systematically & efficiently and act on those.
    2. __Efficiency:__ High Costs. Maintaining these systems can be very costly. With the wrong practices, companies can end up sinking in cloud & personel costs.
    3. __Compliance:__ Regulations. Data & AI Laws, that enforce certain properties of ML Systems, started popping (e.g., GDPR & Explanability 4 Medicine). Plus, usual IP/Licence Compliance.
    4. __Quality:__ High Bar. Users have low tolerance for errors & want fast systems. There are compeititors that can take your place if they offer higher quality.

2. __Low-level Problems (our scope)__
    1. __Evaluating__
        1. __Lack of Standards__: lack of standards for evaluating ML Systems and ML Platforms.
        2. __Lack of Tools__
            1. __Lack of tools__: some types of evaluations are being implemented systematically & thus dont have any tools.
            2. __Lack of unified tool__: there isnt a central tool that can spin up all evaluations to compare trade-offs & simplify UX.
    2. __Improvements (Experimentation)__
        1. __Lack of Time__: lack of time to do Experimentation: search for system improvement opportunities and good solutions takes a lot of time. Time which is mostly dedicated to maintanance of the ML System.
        2. __Lack of Awareness__
            1. __Methods__: lack of awareness of methods that can be usefull.
            2. __Tools__: lack of awareness of tools that can be usefull.
        3. __Lack of Maturity__: lack of maturity with current tools, not using them in the best way.

</details>

<details>
    <summary><b>Similar tools</b></summary>

## :performing_arts: Similar tools 

<details>
    <summary><b>Tools similar to <i>Freelunch OSS</i></b></summary>

### __Tools similar to *Freelunch OSS*__

1. __MLOps__
    1. ___Proprietary (usually managed) MLOps platforms__
        1. From Major Cloud Providers (e.g., *AWS Sagemaker*, *Vertex AI*)
        2. From other vendors
            1. No-code 
                1. Managed (e.g., *Akkio*, *Datarobot Rata Robot*, *Sagemaker Canvas*, *H20 Driverless AI*, *The AI & Analyitcs Engine*, *Obviously AI*, *Abacus.AI*)
                2. Self-hosting (e.g., *Modela.ai*)
            2. Low-code (e.g., *Datarobot*, *Dataiku*, *H2O*, *Valohai*, *Continual.ai*, *aixplain*, *c3.ai*, *Telepath*, *Arrikto*, *Union.ai*)
            3. Medium-code 
                1. General-purpose (e.g., *Deepchecks*, *Databricks*, *Cloudera machine learning*, *Palantir*, *Paperspace Gradient*, *W&B*, *Comet*, *MindsDB*, *Iguazio*, *Red Hat OpenShift AI, *managed ClearML*, *Neu.ro*, *Outerbounds (managed Metaflow)*, *Seldon*, *BentoML*, *Modular*, *Wallaroo.ai*, *Truefoundry*, *Vessl.ai*, *Lightning.ai*, *cnvrg.io*, *deploifai*, *Grid Dynamics*, *Qwak*, *Shakudo*, *craine.io*, *aiXplain*, *rafay*, *felafax*, *Thread AI*)
                2. Specialized
                    1. By requirement
                        1. Real-time (e.g., *TurboML*, *Claypot AI*)
                    2. By method
                        1. LLMs (e.g., *Together.ai*, *Langsmith*, *Scale Generative AI Platform*, *Parea*, *HoneyHive*, *Klu.ai*, *Freeplay*, *Giga ML*, *Lamini*, *Vellum*, *Vianai*, *Parea AI*, *Athina AI*, *AilaFlow*, *Airkit.ai*, *Rebyte*, *Keywords AI*, *Helicone*, *GradientJ*, *LastMile AI*, *Adaptive ML*, *Freeplay*, *dkube*, *dynamiq*, *composable*, *VESSL AI*, *Braintrust*, *Toolhouse*)
                            1. By method
                                1. RAG (e.g., *Vectara*)
                                2. Agents (e.g., *Steamship*, *Langsmith*, *crewAI*, *Stack AI*)
                                    1. Web Agents (e.g., *Lindy.ai*)
                            2. By use case
                                1. Chatbot (e.g., *Sierra*)
                    3. By focus
                        1. Data-centric (e.g., *Cleanlab*, *Scale AI*, *mirry.ai*, *markovML*, *co-one*) 
                        2. Federated ML (e.g., *Apheris*)
                        3. Secure ML (e.g., *grayswan*)
                        4. Data (e.g., *Chalk*, *hex*)
                    4. By sector
                        1. Government Defense (e.g., *Revela*)
                        2. Factory (e.g., MakinaRocks)
                    5. By data modality
                        1. CV (e.g., *Dataleon*, *EyeFlow.AI*, *Picsellia*, *Atos*, *Tuba.ai*, *Tenyks*, *crowdai*, *devisionx*)
                        2. Geospatial (e.g., *Deep Block*)
                        3. NLP (e.g., *Dialogflow*, *Wit.ai*, ​​​​​​*​MonkeyLearn*, *Rossum*)
                        4. Video & Audio (e.g., Sieve) 

    These products typically help you with all the steps in the ML lifecyle in an easy way. __Problems:__ (1) they lock you in too much, killing your flexibility; (2) they are slow to change because they are not tool integrators; (3) they are expensive; (4) they dont offer improtant capabilities we do (e.g., Platform-level Experiment Tracking & Evaluation, and Simulation-based Observability)

    2. __Open-source MLOPs all-in-ones__ (if you consider them as a whole, they are competitiors. However, Freelunch makes use of components of them as interchangable tool backends (e.g., Kubeflow pipelines, MLFlow tracking and NVIDIA Triton))
        1. General Purpose
            1. Set of Workload Components
                1. *TFX* (all of the components are tasks except for tf serving which is a service)
                2. *Xorbits*
            2. Platforms (Depends on specific tools)
                1. Incomplete Platforms (Weak Quality Assurance = Evalution + Monitoring + Observability)
                    1. *Kubeflow*
                    2. *MLFlow*
                    3. *SystemDS*
                    4. *SuperDuperDB*
                    5. *Determined*
                    6. *OpenMLOps*
                    8. *Jina*
                    9. *PrimeHub*
                    10. *UnionML*
                    11. *ModelScope*
                2. Complete Platforms
                    1. *ClearML*
                    2. *Polyaxon*
                    3. *MLRun*
            3. Frameworks (Largely tool-agnostic)
                1. Incomplete Frameworks
                    1. *CLAIMED*
                    2. *Dstack*
                    3. *FuseML*
                2. Complete Frameworks
                    1. *ZenML*
        2. Specific
            1. By sector
                1. Marketplaces: *Wyvern*
            2. By domain
                1. CV: *NVIDIA DeepStream SDK*, *Savant*
                2. Robotics: *IsaacLab*         
            3. By task
                1. Chatbots: *NVIDIA NeMo*, *Intel NeuralChat*, *FastChat*, *Rasa Open Source*, *Lobe Chat*, *Superagent*, *Cheshire Cat*, *Botonic*, *Tock*, *wechaty*
                2. Recommender Systems: *NVIDIA Merlin SDK*
            4. By property
                1. Privacy-preserving: *heflow*
            5. By method
                1. Generative ML/LLMs: *axflow*, *agenta*, *DB-GPT*, *Dify*, *llmware*, *langfuse*, *pezzo*, *bisheng*
                    1. Agents: *agentsea*, *AGiXT*

    These products typically help you with all the steps in the ML lifecyle in an easy way. __Problems:__ they dont offer improtant capabilities we do (e.g., Platform-level Experiment Tracking & Evaluation, and Simulation-based Observability)

2. __DevOps__

    1. __Internal DevOps Platforms__
        1. __Proprietary:__ *Azure DevOps*, *DevOpsBox*, *OpsLevel*, *cortex*, *telepresence*, *Argonaut*, *Mia Platform*, *Shipa*, *Port*, *Upbound*, *Project IDX*
        2. __Open Source:__ *gimlet*, *nautes*
          
    2. __Internal DevOps Platform fabricators__
        1. __Open source__
            1. Fabricate the entire platform: *kratix*, *idpbuilder*
            2. Fabricate a piece of it
                1. Operations Center & IaC: *Crossplane* 
                2. Portal: *Backstage*, *appsmith*, 
                3. Deployment: *Qovery Engine*, *OpenFaaS*, *OpenWhisk*, *KNative*, *Nuclio*, *OpenFunction*, *Fission*, *tsuru*, *CDS: Continuous Delivery Service, *Encore*, *PipeCD*, *Dokku*, *KubeVela*, *shuttle*, *Porter*, *piku*, *OTOMI*, *rack*, *kubero*, *PaaSTA*, *kapp*, *OKD*, *Kubesphere*, *kluctl*, *kusion*, *rig*, *dyrectorio*, *kargo*, *Skaffold*, *Kubeblocks*, *Flux*, *ketch*, *sablier*, *dapr*, *Carvel*
        2. __Proprietary__
            1. Fabricate the entire platform: *Humanitec*, *Syntasso*, *Port*
            2. Fabricate a piece of it: 
</details>

<details>
    <summary><b>Tools similar to paid <i>Freelunch MLOps Copilot</i></b></summary>

### __Products similar to *Freelunch MLOps Copilot*__

1. __Research Copilots__
    1. *AI-Scientist*

2. __MLOPs Copilots__
    1. Cost optimization
        - *LogSpend* (cost optimization for LLM Systems via MLOps copilot)

3. __General Software Copilots__
    1. Development
        1. Making PRs
            1. Doing everything
                - *Devin*
                - *Genie*
                - *codegen*
                - *GeniA*
                - *gpt-engineer*
                - *DevOpsGPT*
                - *GeniA*
                - *AutoDev*
                - *Qodo*
                    
            2. Writing tests
                - *Nova*
                - *octomind*
                - *carbonate*
                - *Meticulos*
                - *diffblue*
                - *Codium* (*cover-agent* is oss)
                - *Sofy AI*
                
                1. Fuzzing
                    - *fuzz4all*

            3. Dependency upgrades
                - *Infield*

        2. Annotating PRs
            - AutoPR

        3. Code Review
            - *PR-Agent*
            - *coderabbit*
            - *code-review-gpt*

        4. Transpiling Code
            - *Second*

        5. Repo Search & Question-aswering
            - *bloop*
            - *cosine*
            - *SeaGOAT*

        6. Writing commit messages
            - *aicommits*
            - *opencommit*
    
    6. Operations
        1. Everything
            - *Cleric*
            - *OneGrep*
            - *OpsBerry AI*
            - *merlinn*
            - *foyle*

        2. Debugging
            - *corgea*
            - *duckie*

        3. Alert filtering
            - *Keep*

        4. Security Operations
            - *Intezer*
            - *Torq*
        
            1. Red Teaming
            - *PentestGPT*

4. Scope-specific Software Copilots
    1. GUI Generators
        - *v0*
        - *renditioncreate*
        - *Magic Patterns*
        - *Clone UI*
        - *tempolabs*
        - *Kombai*
    2. Documentation Generators
        - *trelent*
        - *docify*
        - *Mintlify*
    3. Tool-specific
        1. Kubernetes
            1. Debugging
                - *k8sgpt*
        2. Selenium/Playwright
            1. Writing code
                - *LaVague*
    4. For AI Systems
        1. For LLM Systems
            1. QA
                - *MAIHEM*



</details>

</details>


</details>

<details>
    <summary><b>Deep dive</b></summary>

## __Deep dive__

<details>
    <summary><b>Our users</b></summary>

### :microscope: __Our users__

Our focus is with __companies with ML maturity (generally big non-tech comapnies, medium-to-big tech companies and AI companies)__. Companies that fit this description want to solve multiple ML production problems, not just predictive power. They also value software engineering best practices. Therefore they tend to build their own MLOps platforms to improve ML infrastructure. __We help them to continuously improve their MLOps platforms and use their platforms to continously improve their ML Systems.__
</details>

<details>
    <summary><b>Looking inside High Maturity MLOps</b></summary>

### :ferris_wheel: __Looking inside High Maturity MLOps: production ML project steps (at least, an attempt)__

#### :warning: __Steps Disclaimer__

1. __Shurely contains errors:__ it's just a draft, still need people to give feedback.
2. __Restricted Scenario:__ considering in-cloud Inference Service deployments (acces via REST API usually) for supervised ML to be used in the a app (web or mobile). This should be the majority of cases nowadays. __Not__ considering:
    1. __Types of Deployments (Patterns)__
        1. __Edge deployments__ (Tools: (1) General: (1.1) Compiled: TVM, HF Optimum, XLA, voltaML, Adlik, AITemplate, Hidet, MLC LLM, mnn-llm, iree; (1.2) Interpreted: TFLite, executorch; (2) Embedded Devices with OS: KubeEdge; (3) Mobile Devices: MLKit, CoreML, Pytorch Mobile; (4) Browser: TF.js;) (_Note:_ Changes a lot experimentation and deployment). When models are deployed as .so libraries or executables direclty (when the device doesnt run an OS). This is used when the client can't suffer latency to get predictions and to offload compute costs from our servers. Updates can be done via new webserver version, new app version, OTA updates or schduled new model retrieval.
            1. __Model baked in__
            2. __Model retrieved from Model/Prompt Registry__
        2. __Paid APIs__ (altough this one is easy, frontend becomes just about giving you the API key pair, and you need to setup payments in your API)
        3. __Robotics: embedded devices as prediction clients__
        4. __Hybrid Deployments:__ deployments with an edge part and a service part. ("There are many possible designs where models are deployed both on servers and user-facing devices (or other infrastructure in a network). A common scenario is to deploy a smaller or special-purpose model on the client side and a larger model on the server, where the smaller model is used in many settings as a first step and the larger server-side model is used for more challenging inputs. For example, voice-activated smart speakers often have a small client-side model to detect an activation sequence such as “Alexa” or “Hey Siri” and send the subsequent audio with the main command to a server for analysis")
            1. Stages Pattern (first stage (lighter) is done in client side and second stage (heavier) is done ins erver-side)
            2. ML Fallback Pattern (simple model is used at client-side when there is no internet or internet error occurs)
            3. Slow-Fast Loops (when there are 2 feedback loops and the inner loop needs to be fast (reactive) and the outer loop can be slower (planning), then you can but the controller for the fast loop at client-side receving setpoints from the server-side outer controller)
            4. Precomputed predictions (when you precompute predictions for a bunch of scenarios and make a table of <x, y_hat> tuples, then put it at client-side. When the client doesnt find the input provided by the user in the table, it requests the prediction server for the prediction)
    2. __ML Paradigms__
        1. __Training-based Paradigms__
            1. __Interactive ML__
                1. __Learning from Feedback__
                2. __RL__ (tools: Hugging Face Simulate, TF-Agents, ReAgent, OpenSpiel, OpenAI's Gym, Dopamine, dm_control, keras-rl
                ray rllib, Coach, Huskarl)
            2. __ML on Graphs__
            3. __Unsupervised Learning__
        2. __Deploy-based Paradigms__
            1. __Federated Learning__ (tools: pysift/kotlinsyft, tf federated, flower, nvidia clara, substra, OpenFL, FATE)
3. __Not ready for production__
    1. __Complexity: not complexity-aware:__ certainly don't (and shouldn't) need all of these to get a great working system (neither in the way presented), but the atempt was for it to be a reference for the _Highest Maturity Level of MLOps_, so that you can decide the places you want to simplify (e.g., by not doing some stuff, unifying things, doing at smaller scale, etc). You certainty shouldn't try do implement a system complex as this. Always start with a simple end-to-end working solution and improve upon it only when needed.
    2. __Computational Performance: jobs are not optimized:__
        1. __Processing:__ how jobs would run async, concurrently, in parallel or distributed is not treated
        2. __Communication:__ Networking and data acess patterns may not be the best
        3. __Unification:__ Some tools/processes might be useful for more than 1 usecase and can/should be shared across jobs instead of having separate tools/processes
        4. __Redundancy:__ Some steps get implemented more than once. This happens when they are suitable of implementation in different places inside the architecture, so I decided to not remove them from neither of these places.
    3. __General efficiency: not resource-efficient__
        1. __Not cost-aware:__ assumes infinite computation & storage (doesnt take into account these costs) for the infrastructure
        2. __Not latency-aware:__ doesnt take into account the tradeoff between latency and complexity of the system (generally means more computation)
        3. __Not talent-aware:__ assumes infinite human talent in the organization
        4. __Not time-aware:__ time do do get the ML model working in production. The more data or complexity you add to the system, the more time it takes to get it to production working well.
    4. __Not necessarily IT compatible:__ typically, an ML System solution is implemented on top of an existing IT infrastructure that is already running a lot of applications. Therefore, ML Systems must be compatible with existing production environemnt & tech stack.
4. __Chosen Style__
    1. __Mostly tool-agnostic:__ however, tools are highlighted along the way (preference is given to open source tools). Tools should be used. They may abstract some of the steps away from the user, integrate multiple tasks together, implement them in a different way under the hood and provide a more pleseant interface (API, GUI, etc).
    2. __Flexible sequence of steps:__ often, you will find some problem in step_{k}, re-do step_{m} to solve it, where m < k, these jumps happen a lot. Also, you might skip some steps, and then, only do them later when they are really needed.  You may even prefer a different order for the same steps sometimes.
    3. __Focuses on well-established methods:__ some methods, such as _Weak Supervision_ are relatively new, and thus are not included. This does not mean they are not worthy, just means that most companies are not using it and it is not very clear when/how to use it effectively.

#### :shoe: __Steps__

1. Project Scoping
    1. Business goals
        1. Problem
            1. Define and get good grasp of the problems and who is having it (persona)
            2. Chose a specific problem. Important factors:
                1. Money savings: how much money can th company save if the problem is solved to an extent e, this is return - investment
                2. Feasability: how feasible it is to solve the problem to an extent using ML
                    1. If implmenting solution for the first time
                        1. Define a solution
                        2. Map a solution to an ML task
                        3. Check feasibility of the ML task
                            1. Literature: Search Literature on the task
                            2. Humans: See if humans are already doing it and wat is their their performance on the task
                            3. Data:
                                1. See if enough data can be gathered
                                2. See if quality data can be gathered: if informative features can be produced
                                    1. Informative data sources (tink about the bayes optimmal error rate of P(X,Y))
                                    2. Low noise in these sources
                    2. If improving a working solution
                        1. Check history of the project (plot metrics wrt time or iterations of the system)
                        2. Check if new informative features could be used
                        3. Check resources used to get to this point
                            1. \# of people working on it
                            2. time working on it
                            3. \# cloud usage
        2. Business KPIs
            1. Which metrics
                1. Final metrics: revenue, new users, retention, etc
                    1. Advantage: reflect exatcly hat the business wants
                    2. Disavantage: can be affected by a lot of moving parts, thus cannot say for shure thata chnge in it was caused by the ML system change
                2. Proxy metrics: user engagement, conversion rate etc. _Note:_ There is a spectrum going from very close to ML metrics to very close to Final business metrics. As you go down the spectrum in the ML metric direction you get see that the advantages and disavantages below get more intense
                    1. Advantage: more close to the ML System, affected less by other systems, thus can say with more confidence that a change in it was caused by a change in the ML System; therefore its easier to optimize it.
                    2. Disavantage: An improvement on it does not necessarily mean improving what the business ultmately wants
            2. Which tresholds for these metrics define success
                1. Often more useful to use a proxy metric in near nearm term and final metric in long term.
                2. It can be very helpful to try to estimate relationship between the metrics, this lets you optimmize a metric near ML, do calculations and get rooughly the effect on a metric near the final business metric.
        3. Deployment Patterns
            1. Offline (e.g., Business Inteligence (BI))
                1. Batch predictions
                2. Streaming predictions
            2. Online
                1. At the cloud (as a service usually) (most cases)
                2. At the edge (embedded in app)
                    1. With OS
                        1. Mobile
                        2. Microcontrollers with OS (e.g., Rasberry Pi)
                        3. ASICs with OS
                    2. Without OS (e.g., Arduino) (e.g., for mobile app that need to operate without latency)
    2. ML task specification
        1. Search Literature on the task with more focus on implementation
        2. See if there are pretrained models or external datasets that can be leveraged
        3. Mathematically formalize the task (often formalizing the task differently can solve a lot of problems you encounnter further, so you probably will come back to this step when hitting some problems down the road. E.g., for reccommendation system: you use time spent on some recommendation as your Y label and you assume it is a proxy for how much they liked it; but this assumptions has some problems: old people and peopel with bad internet connections usually spend more time (time spent is caused by them), so the better label would be removing these (causal) factors from original Y, thus, new Y := time_spent*internet_speed/age is an example (age & internet connection do not cause how much user likes content, but getting user likes content data is difficult, so instead we get data with these factors and then remove them from time spent). If you didnt want to heuristically find these factors, you can always get a bit of Y that is more close to the ideal Y (e.g., auto reporting data of how good the content was) or fit different models to varios suspicious groups separetely and then compare the models)
        4. Define the final ML MVMs (Minimum Viable Metrics): final produciton model (after iterations) metrics that justify the effort behind building it e.g., 95% acc and 0.5s latency. These are usefull only for defining PoC MVMs.
        5. PoC MVMs: these are actually usefull, you used them to verify if a PoC should be taken to production. This are by definition lower than the final MVMs, because successive data and model iterations will improve the metrics.
    3. Wizard-of-Oz studies (objective: asuming the ml system works, would it solve a real user problem?)
        1. Build a prototype where there is actually a human doing the task of the ML system behind the scenes
        2. Bring in people to test it, but tell them its a ML system
        3. See if it brought value to the product
        4. Simulate errors
            1. Are some errors more concerining than others? Do these depend on the input or on just the type of error?
            2. Should some inputs have preference over others?
    4. Data availability
        1. Already have sufficient labelled (when necessary) data
            1. Third Party
            2. User-generated data
                1. Historical
                2. Up-to-date
            3. In-company generated data
        2. How to setup data collection and/or annotation (can be more than just labelling)
            1. Time-frame to start doing ML
            2. How to get edge cases
            3. How to avoid noise
    5. Proof-of-Concept (PoC) (tools: HuggingFace Spaces, budgetml)
        1. Dataset:
            1. If data needs to be gathered: gather any feature that might me useful and make a small dataset (less tha 1000 examples)
            2. Study the dataset (refer to "EDA" section inside "Experimentation") to find patterns yourself
        2. Model:
            1. Method: Do EDA, Feature Engineering & Build a heurisitic model, and if you think you have enough data, train a baseline ML model (that tends to work well on this task)
            2. Results: Good enough corrected predictive power score (that already brings value)
        3. Code: in an within-the-company acessible repo
        4. Demo: (Web Server + Application Server) (Tools: Streamlit, Gradio, Shiny, Dash, Voila, PyWebIO, mesop, text-generation-webui, reflex)
            1. Level 1 - Just Showing Model works in the test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) 
                1. Inference Service: Application server acessible within the company via HTTP API
                2. Minimal FrontEnd
            2. Level 2 - Showing Model brings value to the product
                1. Inference Service: Application server acessible within the company via HTTP API
                2. Mockup:
                    1. UI/UX: copy the frontend of the product you want to power by ML to just be a minimal version of the real one
                    2. Versions: can enable ML version or non-ML version
                    3. Hook up analytics to it
                3. Get people to use the the 2 versions (with and without ML)
                4. Show application is delivering more value
                    1. Plot proxy business metrics
                    2. Do statistical tests between the two versions
        5. Doc: Explanation of why improving ML score (ML metrics) will improve Company scores (Business metrics)
    6. Baselines (preferably, divide data into slices and get baseline for each of these slices)
        1. Lower Bounds (From lowest lower bound and simplest to highest lower bound and complex)
            1. Random model
            2. Instrinsic baseline (for classification: most common class; for regression: y mean)
            3. Heuristics (e.g., tree rules or for time series: last value)
            4. Simple ML models
            5. ML models without/with minimal feature engineering and hyperparameter tuning
            6. AutoML Models (Tools: HF AutoTrain)
            7. Open source implementations
            8. Using FMs
        2. Upper Bounds (From lowest upper bound to highest upper bound)
            1. SOTA (State-of-the-art) for the problem
            2. Human-level performance
            3. Optimal Bayes Error-Rate (Need to estimate it. Estimating it is a very hard task, because our objecctive with ML is to actuallylearn p(x,y) so how would we estimate something that depends on p(x,y)?. The most common estimate is actually using humans. For small datasets you can ask labellers to label same datapoint and do this to different datapoints, but it is unscalable (especially if annotation (can be more than just labelling) manpower is your bottleneck). Also common to the estimate be actually Human-level performance, but in cases humans are bad (especially for tabular data) it may be a very pessimistic estimate)
    7. Design Patterns for Human-AI Interaction
        1. Before Deploying ML System
            1. Data collection: can or cannot aks humans for data (active learning)
            2. Training model
                1. Domain Knowledge: level of dependence on subject matter experts
                2. Model building: level of dependence on programmers
            3. Evaluating Model: where human evaluation is needed and where it can be fully autommated
        2. After Deploying ML System (model in production)
            1. By Decision Making
                1. Algorithm is decision-heavy: automating tasks/behaviours
                    1. Full automation (can be deployed without humans having to interfere with its decisions)
                    2. Partial automation 
                        1. Using humans to check output
                        2. Right to Appeal (In case the original model is not confident or user disagrees with model's output)
                            1. Better modelling
                                1. Model with higher predictive power (but slower and/or costlier)
                                2. Humans
                            2. Better data: user is asked to give more detailed data that helps the model make a better prediction
                2. Algorithm is decision-light (gos trough human review)
                    1. Being a second-opnion to humans (e.g., chest-x ray analysis, where doctor checks ML to see if he missed something)
                    2. Screening for humans (discarding easy/non relevant cases to make better use of human time, e.g drug discovery screening for good candidates)
                    3. Informing Humans
                        1. Data (e.g., domain specific chatbot, ride time estimation)
                        2. Methods (e.g discovering science or new methods for doctors to detect some desease)
                    4. Creating candidates for humans
                    5. Debating with humans
                    6. Entertaining Humans (e.g., open dialog chatbots or video-game AI)
            2. By Programmer Interventions: Adaptability of the System. More adaptable systems require less Monitoring. Level to which system can improve itself while in production modulated by the level of human intervention necessary for it (from dealing with distribution shifts to handling entirely new datasets)
            3. By User Interface
                1. By User reliance
                    1. Optimizing lack of underreliance of humans wrt to the system (e.g., showing evidence that backs up how good the system is)
                    2. Optimizing lack of overreliance of humans wrt to the system (e.g., making limitations clear)
                2. By Failure Modes
                    1. Humble to Error (accepts feedback, gives other options, etc)
                    2. Silent to Error (ingnores the errors)
                3. By Transparency
                    1. Explanations to decisions
                    2. COnfidence level of a decision
    8. ML KPIs
        1. Hard KPIs: contraints or requirements. These need only to be reached, not optimized afterwards. Could be: minimum accuracy on whole data or special subsets, integration with existing tools, run o edge device with limited compute, for some industries explanability is required, operate as a service or as a dependy at the edge, threshold on # of low confidence predictions that require human review, inference coverage (% of requests model needs to return a predition), usefull less of recommended content etc
        2. Soft KPIs: what is it optimizing optimizing for. Could be low latency, very acurate, privacy preserving, robust etc
            1. Which KPIs
            2. Tresholds for these KPIs that define success
    9. Management
        1. Viability: define is the net value is larger than the opportunity costs
            1. Opportunity cost (Maybe not doing ML and doing other important things is better)
            2. Budget estimation
                1. People
                    1. Your team
                    2. Help from other teams
                2. Cloud resources
                3. Others (e.g., annotation (can be more than just labelling))
            3. Value Estimation. Due to:
                1. Lower costs: Saving costs with human labor
                2. Increase value:
                    1. Better user experience
                    2. Better product
                3. Increase productivity: Optimizing worker's job
        2. Meetings (How many meeting/week, are they going to be written, etc)
        3. Schedule: milestones to be acomplished (e.g., time to get a minimal version working)
        4. Setup Management tools: 
            1. Communication Tools: 
                1. Async: employees can store, share & sync docs (Tools: (1) open source: nextcloud, novel; (2) managed: Slack, Notion, Google Drive, etc)
                2. Sync (Tools: Zoom, etc)
    10. ML System Design
        1. Write initial Design Docs for the ML System
        2. Write inital Diagram for ML System Architecture (Tools: Diagrams as Code, Draw.io, Miro Board, excalidraw, mermaid, obsidian)
        3. Choose inital tech stack MLOps Stack to use
            1. Steps
                1. Screening of tools
                2. side-by-side comparisons
            2. Evaluation metrics
                1. Community
                2. Ease of use
                3. Integrations it provides
                4. Capabilities it provides
                5. Performance
                6. Scalability
                7. Next 5 years projection
    11. Golden Path Setup (infrastrcture setup that lets you start way ahead with awesome defaults so you dont have to setup same things manually over and over). 
        1. Repo: Start the repo with template (Tools: Cookiecutter, Craft, copier, Yeoman, mlops-python-package, Self-hosted AI starter kit, AgentStack)
            1. MLOps project template
                1. Setup Automatic web documentation built & deployed from repo (Tools: OpenAPI-Specification, redoc, pdoc, devdocs, sphinx, mkdocs, read the docs, gitbook, Quarto)
                2. CI/CD
            2. Sample MLOps project
        2. Services: setup telemetry, register service with discovery catalog, container image, security scans, helm chart, etc  
            1. Stateless 
            2. Statefull (+ mounts, backups + patches)
2. MLOps
    1. IT/DevSecOps
        1. ML-agnostic Sybsystems
            1. Role-based Access Control (within-company permissions, who, or which process, can access what. Use principle of least privilege == people or processes only get the minimum amount of privilege that they need, and for the minum amount of time)
                1. Goals
                    1. Control Access to Codebase
                    2. Control Access to Machines
                    3. Control Access to Private Networks
                    4. Control Access to Services
                    5. Control Access to Data
                2. Clients
                    1. People
                    2. Systems
                3. Components
                    1. Key management service (KMS) (Tools: (1) Open-source: Hashicorp Vault, Sops, Infisical, aws-secretsmanager-agent, ; (2) Managed: All major clouds have their own KMS, e.g., AWS KMS) (manages (generates, stores, distributes, renews, destroys) keys of all services, usually managed solution provided by cloud providers. _Note:_ Cloud KMS usually use Hardware Secuirty modules or extra security OS (Trusted Execution Environment) under the hood. But why use master and data key, instead of the system just getting the data key from KMS? Because if only one of them gets exposed, the other one will stop the attacker; and also enables working with few Master Keys instead of millions of data keys.
                        1. Configuration
                            1. Admin Configuration. Which people can:
                                1. Create/edit/disable/delete which master keys
                                2. Change which Service Configurations
                            2. Service Configuration
                                1. Inter-system networking rules
                                2. System ID --> master key mapping
                                3. Which systems can use KMS
                        2. Monitoring. Sends configuration data and operational data to Monitoring Service. Can see:
                            1. Which person changed some configuration
                            2. Which system accessed a certain set of data
                                1. Operational metric such as requests/system or /master key, number of renewals
                    2. For engineers: Central Access Management (usually the mangement part done by the admin is the identity access management (IAM) (Tools: (1) Open source: keycloak, ory, supertokens, authelia, defguard; Managed: OKTA, Auth0, OneLogin, Ping Identity, WSO2 Identity Server, Duo Security AWS SSO) and the actual user part used by engineers is the Secrets Manager) (Tools: (1) Clients: Teller; (2) Servers: Hashicorp Vault, AWS KMS). Basically a frontend coupled (coupled with a backend behind it) where engineers authenticate themselves with a password/phone SMS, and get a home page with all the data they need: access to credentials, secrets, etc. The backend will be equipped with a KMS secret (so that it can get what it want from KMS). But isnt it dangerous to put a secret hardcoded together with the software that uses it? Yes, thats why the KMS secret is not hardcoded into the CAM backend, it gets there at runtime through the processs described below at "Secrets Management & Service-Service Authentication."
            2. Security
                1. Proactive Security: Security Patterns & Vulnerability Identification
                    1. Secrets Management & Service-Service Authentication. A good pattern:
                        1. SSH Keys of Logical (usually VMs) Machines Management & Kubernetes Cluster Setup. An external service that provides SSH keys at infra CD time (Tools: github/gitlab/gitea (self-hosted) secrets), only when the CD tool needs it. These keys will be used by the CD tool to setup the k8s cluster, installing other necessary packages in the machines aswell. This is better done (from within the CD tool) via an IaC tool like Ansible or Terrraform.
                        2. Kubernates Secret Management. An (internal or external) service that provides kubernetes credetials at app CD time (Tools: github secrets, external-secrets), sending the secrets when the CD tool needs it.
                        3. Kubernetes KMS secret ditribution to pods. Kubernetes Secrets Setup with your pods id secrets, to give specific secrets to specific pods as environment variables or volumes. Important to have Kubernetes Secrets encrypt the secrets in etcd at rest, by having Kubernetes request KMS for an encryption key and using it to encrpyt these secrets.
                        4. Pods actual secret retrieval from KMS using their KMS secret. Pods that get pod id secrets, as volumes or environment variables, from Kubernetes, when they are launched and requests external Key/Secrets Management Service for the actual keys/secrets it needs to use other services. Secrets can be for signing output artifacts or authenticating itself with some service. _Note:_ Pod ID secrets must be registered before-hand with KMS, where KMS will know the details of each pod (which services it needs access to)
                        5. KMS provides pods with actual secret they need to talk to other services (internal and external services), by:
                            1. If using internal service: generating a public/private key pair and giving the pair + client ID + client Roles to the client and the public + client ID + client Roles to the service pod (along with details & permissions of the client). _Note_ both pods need to have logic to support these authentication interactions.
                            2. If using external service: just giving the pair to the client
                        6. More details:
                            External Key/Secrets Management Service (with multi-factor authentication) that is fully authenticated with all the services (by having the keys hard-placed on both ends (but hard-placed on services only at start-time!), just like SSH keys) and provides pods with keys/secrets they need, according to predefined rules. These rules are commonly fetched from the Cloud IAM (Identity & Access Management) configuration. They also change the keys/secrets they pass to client services on schedule (to minimize impact o breaches) and revoke keys/secrets in case of breaches, and also alert developers of some weird secret-requesting behaviour (that might lead developers request KMS to revoke them. Additionally, they are configured to only give secrets/keys to requests that come with origin IP inside [Public IP's of your Phsyical Cluster(s)] (e.g., we know that a servie is only deployed in physical Cluster A, then requests for secrets for that service must only come from public IP's of that cluster (either a router or a load balancer (which is the same thing because it actually has a router behind him forwarding all packets of specific ports to it)).)

                    2. Encryption Patterns
                        1. In-transit encryption (tools: Istio/Kiali/linkerd2/Cilium/Consul is the gold standard for this, specifcally Citadel functionality) (dynamic SSL/mTLS certificates) (to make shure inter-system networking is done only as defined in the our minimal access rules): Can emit SSL/mTLS certificates to servers and (optionally) to clients. However these certificates are not static and independent like in typical web SSL/mTLS between browser and webserver. The certificates will be valid only if the clisnt/server is talking to the right server/client and at a defined state of the whole IT system. The systems certificates will be their IDs, the other system will check with KMS if it is supposed to communicate with that system ID at this state of the IT system (which they dont know, but KMS will know). The KMS either responds that the communication is ok or not according to its predefined inter-system networking rules (made to apply principle of least privilege).
                        2. At-rest encryption (data key used for encrypting/decrypting data stored in the same machine): encrypted data (by system's data key) is stored and has an attached master-encrypted data key. To decrypt master-encrypted data key the system has to get master key from KMS. To get master key, system needs to authenticate with its signature.
                            1. How services stroe encryted data: The signature is JWT-like. System will send authentication claims (system ID) together with password, KMS will verify if this System exists in its DB with that password (hashing password of course) and give back a signature (<identification claims, server-encrypted identification claims>) to be used in the next requests. The password, used for system authentication is provided by a programmer when the system starts as an environment variable (or if frontend: by user) (thus, will be kept in memory) and is deleted after the signature is received.). Periodically System sends signature over secure SSL/mTLS connection to KMS, there a and data key is generated and a master key is fecthed from KMS storage. The data key is master-encrypted. The data key and master-encrypted data key is returned to both the system, where they are kept in-memory (_Note:_ if its a mobile app: Android & Apple even offer a special secure place to put the master-encrypted data key after you get it (instead of plain app memory) which is their Keystore Systems (have dedicated hardware and/or extra security OS which have APIs). Data is encrypted with the data key and then the data key is deleted.
                            2. How services read encrypted data: system sends master-encrypted data key, together with its signature, over a secure SSL/mTLS connection, to KMS. KMS will see the master-encrypted data key and master decrypt it using the right master key (knows which master because of the ID claim in the signature). Its analgous to JWT. After getting the data key, the serivce uses it to decrypy & encrypt data, then deletes it.
                        3. (For very sensitive data) At-usage encryption: plain data only inside the processsor or secure memory, but this should only be used if there is a very strong need for confidentialy: because needs needs special processors that do decryption inside them, lowers processing speed and ands complexity).
                    3. Vulnerability Identification
                        1. Static Application Security Testing: identify vulnearability just by looking at the code (Tools: Snyk is a great paid SaaS)
                            1. When its done
                                1. At Developing Phase (recommended)
                                2. At CI/CD
                                3. After release
                        2. Dynamic Application Security Testing: scan assets & simulate attacks with mirror environment (Tools: Immuniweb and Intruder are great paid SaaS)
                            1. Vulnerability Scanning (Simple Exploits) (Tools: Vulnerability Scanning/DAST tools)
                                1. When its done
                                    1. At CI/CD (but can slow down a lot CI/CD)
                                    2. After release
                            2. Penetration Testing (Complex Attacks) (Tools: Vulnerability Scanning/DAST tools + Cybersecurity Experts)
                                1. When its done
                                    1. After release
                2. Reactive Security: Cybersecurity Observability, aka Cyber Threat Intelligence (CTI) (Identification, Internal Communication and Incident Response (Damage Control, Defense, Public Communication and Remediation)) (Tools: ThreatMapper, CrowSec, ossec-hids)
                    1. Components
                        1. Database (all activity that might indicate threats are stored here)
                        2. Data pipeline that extracts cybersecurity data from System Monitoring Database and sends to Cybersecurity backend
                        3. Backend that coomunicate with Cybersecurity data pipeline, frontend and Cybersecurity Database (Tools: 
                            (1) Python: 
                                (1.1) HTTP API libraries/frameworks (build them as a library for Dynamic HTTP Server Library to use): FastAPI; (1.1.1) REST (Old messy way very tied to DB principles): plain FastAPI; (1.1.2) RPC (New functional way that treats paths as functions, payloads as parameters, compresses data and fully embraces http 2.0)): gRPC + FastAPI; (1.1.3) GraphQL (New DB way that ignores vestige HTTP paths, treat payloads as lossened DB queries (not just reading, but also updating & adding data) payloads as parameters, compresses data and fully embraces http 2.0): Ariadne + FastAPI, pg_graphql; 
                                
                                (1.2) Dynamic HTTP Servers (handles parallel socket connections, calling HTTP API libraries, error returns): Uvicorn (ASGI) using Guvicorn (WSGI) worker; 

                                Note: the job of the HTTP API library/framework is to make it easy for you to specify an HTTP API and build functions to be used by the Dynamic HTTP Server so that it can do the processing of requests. Important to note that the HTTP Server itself knows nothing about your API specification a priori, it will only know about it via the functions that the HTTP library/framework provided to it. This decoupling of having tools that handle HTTP communication and tools that provide processing and specification functions is not necessary (e.g., nodejs land doesnt have it), but is very helpfull because you separate specification from implementation, letting HTTP library/frameworks focus on ease of use and expressability, while letting Web Servers focus on bdoing efficient and scalable HTTP communication. Two main standards standardize the interface between these tools: WSGI and ASGI. WSGI is the oldest and is sync, ASGI is the newest async is backwards compatible with WSGI, meaning it can leverage WSGI tools within it (e.g., Uvicorn (ASGI) leveragaing Gunicorn (WSGI)).
                            
                                (1.3) HTTP Reverse Proxies: NGINX;

                            (2) Nodejs: 
                                (2.1) HTTP API libraries/frameoworks: dont know any
                                
                                (2.2) HTTP Servers: HTTP Module; HTTP Module; 
                                
                                (2.3) Single Dynamic HTTP Framework (analogous to Dynamic HTTP Server + HTTP API library/framework in python land): Express, Nestjs; 
                                
                                (2.4) HTTP Reverse Proxies: NGINX
                        )
                        4. FrontEnd Dashboard (Tools:
                            (1) From scratch
                                (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                (2) Automatic template: screenshot-to-code
                                (3) Implementation
                                    (1) Content Management System (CMS, aka config/artifact-based Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                        (1) Code:
                                            (1) Template Libraries: Grapesjs
                                            (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                            (3) Supporting Libraries: 
                                                (1) State management: Redux; 
                                                (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                (3) Plots & Dashboards: deck.gl, d3, tremor 
                                            (4) Telemetry
                                                (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                (2) Session Replays: OpenReplay
                                            (5) UI Dev Environment: Storybook, OneOne
                                        (2) Build: Webpack; 
                                        (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                        (4) Build & Web Serve: 
                                            (1) SSR: Nextjs, Astro, remix; 
                                        (5) Code & Build: 
                                            (1) Web Frameworks: Django, fasthtml, Larelevel, SpringBoot, Sveltekit, redwood;
                                            (2) Static Site Generators: Hugo, Gatsby, Quartz
                            (2) Using task-specific high level tools
                                (1) Dashboards: metabase, plotly, graphana)
                    2. Goals
                        1. Defend: ongoing attack (defend against attacks once they are identified, try to avoid or minimize damage)
                        2. Remediate: after attack (minimize damage after exploit happened & fix the system)
                    3. Steps
                        1. Assesment of Breaches
                        2. Report
                        3. Fix
            3. Version Control (Can be intergrated with Experiment Tracking)
                1. Development --> human does add/comit/push commands (Note: important to have multi-factor uthentication for pushes)
                    1. Small files (tools: Git & github/gitlab/gitea (self-hosted)): programmer's job, the code will deploy the system. Each system is programmed to function under a space of configurations (can also be hardcoded for simplicity sometimmes) and states. Configuration is data that defines behaviour/computation. State is data that defines the current situation of the System, state changes are done by code.
                    2. Big files (acts on top of small files version control) (tools: dvc/fds, Dud, Pachyderm) (Needs to enable plug-in of external storage (e.g., S3, Ceph). Is used for reflecting the changes that occur in important Artifacts (main I/O files) of system, that are not code. The system will have a local repo, the humans makes a config file where it will say which files to track and waht are these types of files. When ma commit is made (snapshot), intead of storing the patch, it stores the entire new files in external storage (because calculating a patch for small code files is feasable, but for big data files it is not & it doesnt know a priori the changes that were made) and gets a path to acess this patch which it stores in its DB or a simple file (tracked by small file version control).
                2. Production --> system stream logs state changes (config and/or data changes (e.g., DB logs store/update/delete queries) along with backups to be stored in persistent storage). Anys state of the system can be reproduced by starting with a backup and following the state changes. _Note:_ In theory, you dont need to verison data state, becuase you can reproduce the data (point-in-time-correctness) by knowing the config states & historical event data produce by the data sources. However, this requires that your system is fully replayable (which is hard) & has great computational cost to do the replay (backfilling) and fabricate point-in-time correct data. Therefore, it is usefull do have production data state versioning (storing logs of data changes) on the systems that are most important and/or you tend to need to replay more. each system sends its config and/or data changes to a message broker (e.g., RabbitMQ) or stream transport (e.g Kafka, Pulsar, Pravega, etc) that in turn will be consumed by a backend that will update a central data wharehouse (system state store) holding all system states (config and maybe data changes).
                    1. DB Versioning
                        1. Traditional DBs with event sourcing
                        2. Versioned DBs (Tools: TerminusDB, Dolt, Delta Lake)

            4. Cluster setup (_Note:_ just a default setup, because often you will want to change the cluster dependeing on the workloads at deploy time using IaC tool) (Tools: (1) Very low-level: Cloud SDKs (e.g., AWS CDK or AWS CloudFormation); (2) Low-level: libcloud; (3) Medium-level: Ansible, Terraform/Terragrunt/terramate, Pulumi, Crossplane, nitric; (4) High-level: kubesurvival; (5) ML-specific: mlstacks, mlinfra)
                1. Types of Logical Clusters (each one has its own specific requirements)
                    1. Offline Workloads Physical Cluster
                        1. Always up Clusters
                            1. Dev Logical Cluster (Where people will be coding are run things locally fast). _Note: this cluster should always be up but should scale (downscale or upscale) according to usage patterns_
                            2. Background Logical Cluster (Similar to Integration Testing & Staging Clusters (mimicks production cluster & does a lot of tests), but used when: (1) doing risky experiments, in which case you need a big suite of evaluations (which can be long-running), but if these were done in Integration Testing Logical Cluster it would slow your CI/CD too much; (2) proactively detecting production bugs/failure modes before they appear; (3) Undertanding better your MLOps platform and Improvement Opportunities; (4) Penetration Testing.
                            
                            All Ml System changes (debugging/solid changes or experiments) go through integrates testing & staging, and then: (1) if debugging/solid changes: deployed to production & backgorund clusters; (2) elsif experiments: deploed only to background cluster) 
                            
                            _Note: ideally, this cluster should always be up. But this isnt this unrealistic for most companies, since they would double their cloud/premise costs? Not exactly, because this cluster would be much smaller and can even be a virtual cluster. Of course, this makes it impossible to evaluate some metrics like: performance & cloud/premise cost. However, setup (expand the cluster) on-demand production-scale-like cluster when this type of evaluation is needed, and then tare it down to go back to small scale background cluster. Another option is to not upscale at all and use an estimation algorithm to map from scale-dependent metrics in small-scale background cluster to estimated production-scale metrics. Wrt to external services (SaaS) used within the cluster, there is not much way around it and you will probably have to double costs. A long shot is to provide deploy equivalent open-source tools as internal services that would substitute these external services, but for a loss in production mimicking quality_
                        2. Dynamic Setup/Tare down Clusters (_Note:_ you shoudlnt setup up the cluster all over again if you have set it up before. You should just pause the instance (cloud IaaS providers offer this by storing your VM Image that captures the state of your VM whe it was paused), resume it, and maybe make some new setup changes if you want on top of it)
                            1. Manual Offline Ephemeral workloads (Experimentation) Logical Cluster (Tools: (1) FaaS: GPU Clusters: *TensorDock*, *Modal*, *Paperspace*, *Lambda GPU Cloud*, *Fluidstack*, *CoreWeave*, *salad*, *HF GPU Spaces*, *Crusoe.ai, *LambdaLabs*, *JarvisLabs*, *SambaNova*, *Juice*, *Super Micro*; Other: *Graphcore*; (2) Distributed Processing Engines on top of k8s: Ray, Dask, Celery) (aka Experimentation Cluster) (commonly described as a pipelines of tasks) (e.g., model training is a task) (Tools: worflow orchestrators such as kubeflow pipelines (_Note:_ built on top of Argo/Hera), Argo/Hera, prefect, pachyderm, etc). Important: if your workloads are fault-tolerant you can have preemptible (spot) instances (instances that you are not shure when you will have them (you bid for them) and that can terminate any time without your control (if the cloud is under a lot of usage and needs that VM for normal usage)) which are 3x cheaper. _Note: if your CI/CD is super fast then this cluster should be always up; else: you should get it up and down each time you use it to avoid unnecessaty cloud/premise costs._
                            2. CI/CD-specific clusters
                                1. Integration Testing Cluster. Should be in small scale (and adapt load accordingly). _Note: if your CI/CD is super fast then this cluster should be always up; else: you should get it up and down each time you use it to avoid unnecessaty cloud/premise costs._
                                    1. Services Cluster
                                    2. Ephemeral workloads Cluster
                                2. Staging Cluster. Should be aas close as possible to production._Note: if your CI/CD is super fast then this cluster should be always up; else: you should get it up and down each time you use it to avoid unnecessaty cloud/premise costs._
                                    1. Services Cluster
                                    2. Ephemeral workloads Cluster
                            3. Security Testing clusters
                                1. Fuzz & Penetration Testing cluster for recond, build and attack
                                2. Penetration Testing>DDoS distributed virtual cluster (needs multiple cluster in different regions, but logically forms a single virtual cluster via VPN)
                    2. Online Workloads Physical Clusters (ALways up)
                        1. Production Services Virtual Cluster. Virtual Cluster because you want multiple physical clusters all araoud the world for fast serving and you want to manage them as a single cluster. Ideally you should have a IT namespace and a ML namespace and use node selectors to make each namspace run on separate node subsets (to avoid noisy neghbors problem). _Note: this cluster is always up & needs to be fast. Also should scale (downscale or upscale) according to usage patterns_
                            1. CDN Virtual Cluster: small physical clusters spread around the world that host web server cache proxies near the user.
                        2. Production Online Ephemeral workloads Cluster (commonly described as a pipelines of tasks) (e.g., model training is a task) (Tools: workflow orchestrators: (1) General workflow Orchestrators: Argo/Hera, Prefect, Airflow, Dagster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau, dagger; (2) Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro, Couler, metaflow, sqlflow; (3) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo/Hera), mleap, flyte, sematic; (4) Data-driven Workflow Orchestrators: Pachyderm; (5) Notebook-based Wrkflow orchestrators: Ploomber; (6) Distributed Task queues: celery, rq, kueue; (7) distributed processing engines: spark, ray, dask). _Note: if your CI/CD is super fast then this cluster should be always up; else: you should get it up and down each time you use it to avoid unnecessaty cloud/premise costs._
                2. Steps
                    1. For an actual cluster (not a virtual (simulated) one)
                        1. Install locally IaC (e.g., (1) tools that just set up: Terraform/Terragrunt/terramate, Ansible, Pulumi, Crossplane; (2) tools that also try to find the machines with minmum cost for you: kubesurvival) tool on your client machine
                            1. Dependencies
                            2. The tool itself
                        2. Use IaC tool (through cli client from client machine) to install core software on machines
                            1. Only if working with physical machines
                                1. Only if you need a new physical cloud cluster
                                    1. Rent (or Provision using an internal IaaS API provided by your internal IaaS team) Physical Cluster:
                                        1. Multiregion Subclusters (is basically carving up your cluster to be all over the world so that yourservices are closer to the clients, making latency lower)
                                            1. Amount
                                            2. Geographic Locations (Note: for simulated edge deployments, need to simulate the location of the edge device)
                                        2. Machines:
                                            1. Processors: 
                                                1. Parallel processors (e.g., GPUs or TPUs) if you need them (Tools: TensorDock, NVIDIA DGX).
                                                    1. Physical Single GPU
                                                    2. Physical Multi-GPU
                                                    3. Virtual Single-GPU (vGPU)
                                                    4. Virtual Multi-GPU (MIG)
                                                2. Private Processor (e.g., Intel SGX Hardware ENclave) if you need them
                                                3. Embedded Processor (for simulated edge deployments)
                                            2. Storage & memory: 
                                                1. Enough storage and memory
                                                    1. Standard storage
                                                        1. Physical machine storage
                                                        2. VM % of physical machine storage
                                                    2. Virtual Storage (Tools: AWS EBS (which is tied to AWS EC2 instances so that applications running on AWS VMs feel like they have "infinite" storage. The EC2 VM under the hood uses a storage service, openebs))
                                                2. Restricted storage and memory (for simulated edge deployments)
                                            3. Network Adapters:
                                                1. Inter-node (e.g. ethernet)
                                                    1. High transmission speed and parallel network physical adapters (requires parallel switch)
                                                    2. Low speed virtual adapters (for simulated edge deployments)
                                            4. Accelerator-specific Communication
                                                1. Intra-node multi GPU communication (e.g., NVLink or gpudirect):
                                        3. Networking:
                                            1. Intra-subcluster: 
                                                1. High speed/throughput & parallel {cables, switches, routers}, hardware reverse proxies (includes load balancers)
                                                2. Low speed ((for simulated edge deployments))
                                            2. Outer-subcluster: 
                                                1. High internet speed, necessary (cluster entrypoint) public IPs, inter-subcluster VPN.
                                                2. Low speed ((for simulated edge deployments))
                                    2. Physical Machines setup
                                        1. Networking config
                                        2. Install Cluster Management Software (Software that makes it easy to deploy software on the cluster, detect hardware failures, monitor usage, monitor cooling, RBAC, insurance etc) (Tools: (1) Vendor-agostic: OpenStack, Cloudstack, Cozystack, clutch, cluster-health; Vendor-dependent: cluster-toolkit)
                                            1. Installs VM software (e.g., Virtual Box for Core Virtual Machine functionality + Vagrant on top of it for cluster VM provisioning)
                                    3. Virtual Machines setup (Tools: (1) on baremeteal: Vagrant; (2) on kubernetes: KubeVirt, Kata Containers, Harvester)
                                        1. Configuration
                                            1. Types of VMs
                                                1. Operating System (generally some Linux distribution. There are application-specific distributions (e.g., for just storage or just compute))
                                                2. Resource Allocation (storage, memory, etc)
                                                3. Networking configuration (e.g., firewall)
                                            2. VM Networking
                                                1. Virtual Networks
                                                2. External Traffic (VM-Host Interaction) (e.g., port forwarding)
                                        2. Spin VMs on Cluster
                                    4. If On-prem cluster: same things as renting a cluster but + previous setup steps (that the cloud would already do for you):
                                        1. Networking: ensure your phsyical machines are on the same LAN and that you have fast & parallel communication hardware (cables, switches, routers)
                                        2. SSH: ensure at least your master physical machine is running a SSH server
                                        3. SHH Server Discovery: setup DNS for your SSH server OR setup an internal VPN
                                2. If you can leverage an existing setup cluster (has enough spare resources): just create a separate kubernetes logical cluster and allocate the amount of resources you need for it. _Note:_ this option requires you to have the same installed software as you had in the other cluster (because we are working on kubernetes level, all the software that was installed in the k8s hosts will continue there)
                            2. Core setup (can start here if working with you already have a logical machine available frm the beggining)
                                1. If you already have a logical machine available frm the beggining: configure IaC tool with your
                                    1. SSH keys
                                    2. IPs of existing clusters
                                        1. Public IPs (IP that leads to machine where k8s master runs on)
                                        2. Private IPs (machines where workloads run on)
                                2. Machine OS Configuration
                                    1. Security configs
                                        1. Enable Automatic Updates
                                        2. Create a Non-root User
                                        3. Setup Firewall, only enabling the ports you are using
                                3. Core Software Installation/Update: Install/Update core (low-level) software on VMs (k8s hosts) (_Note:_ does not apply if you create the cluster by creating a new logical kubernetes cluster)
                                    1. Always
                                        1. For security:
                                            1. fail2ban (to prevent brute-force authentication attacks)
                                        2. For deployments:
                                            1. For managing services: kubernetes (using Shipwright, Skaffold/Flux/ketch/kubevela/kubeblocks/, Lens/Kubeapps/porter-archive, capsule, Rancher/karmada, Istio/Kiali/linkerd2/Cilium/Consul, Helm/Kustomize/timoni/JuJu/cdk8s/kpt, yamllint/Carvel/kubesec/kubeaudit/kubeconform/kube-linter/polaris/conftest/Kubescape/Kyverno/OPA/Datree/Kubevious, SchemaHero/Datashim, Karpenter, Sealed Secrets/Kubernetes Secrets Store CSI Driver/External Secrets, Metacontroller, KubeEdge/K3S/Microk8s/baetyl/openyurt/shifu, velero/stash, OPA/Kubearmor, cert-manager, kubecost/opencost, kube-prometheus/scope/kdash/skooner)/OKD
                                                1. For managing operators (Tools: kubebuilder/operator-sdk)
                                            2. For deploying tasks: (Tools: worflow orchestrators such as: General workflow Orchestrators: Argo/Hera, Prefect, Airflow, Dagster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau, dagger; (2) Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro, Couler, metaflow, sqlflow; (3) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo/Hera), mleap, flyte, sematic; (4) Data-driven Workflow Orchestrators: Pachyderm; (5) Notebook-based Wrkflow orchestrators: Ploomber); (6) low-code workflow orchestrators: open source: n8n; proprietary: zapier
                                    2. Maybe
                                        1. IP Packet Observability tool
                                        2. VPN
                                        3. Dev CLI tools (e.g., Vim, Curl, Unzip, etc)
                                        4. Distributed Computing tools
                                            1. Generalized Memory (Memory & Storage)
                                                1. Distributed File System (e.g., HDFS, Gluster, moosefs)
                                                2. Distributed DB systems
                                                    1. Data Wharehouse (e.g., Hive)
                                                    2. Distributed key-value Store (e.g., etcd)
                                                    3. Distributed DBs
                                                        1. In-storage (cassandra or scilladb, YDB, cockroach)
                                                        2. In-memory (e.g., Open source: Ignite, Redis,  DiceDB; managed: AWS DynamoDB)
                                                3. Distributed shared memory (e.g., ArgoDSM)
                                                4. Other more specific distributed generalized memories (e.g., Zookeeper)
                                            2. Processing
                                                1. Workflow Orchestrators (Tools: worflow orchestrators such as: General workflow Orchestrators: Argo/Hera, Prefect, Airflow, Dagster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau, dagger; (2) Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro, Couler, metaflow, sqlflow; (3) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo/Hera), mleap, flyte, sematic; (4) Data-driven Workflow Orchestrators: Pachyderm; (5) Notebook-based Wrkflow orchestrators: Ploomber); (6) low-code workflow orchestrators: open source: n8n; proprietary: zapier
                                                2. Distributed Processing tool (e.g., spark/koalas)
                                4. Core Software Configuration: configure core software according to requirements and/or preferences
                                5. Data Sync: setup data sync to copy data from production state store
                                    1. Continuos sync
                                    2. Only sync upon request
                            3. Using Kubernetes/OKD: deploy necessary software infrastructure (e.g., dev environment pod)
                        3. Cluster Autoscaling setup (Tools: k8s autoscaler, keda): cluster should upscale if low on resources (reactive) or based on a demand model (proactive via forecast) and downscale if high on resources (reactive) or based on demand model (proactive via forecast). Important: should upscale by taking into account node proximity, added nodes should be network-wise close to current nodes (ideal situation: all node sin the same LAN, utopic situation: all nodes in the same physical machine).
                    2. For a virtual (simulated) cluster (_Note:_ of course you would never deploy production workloads to a virtual cluster, because they need to be high performance)
                        1. Install locally IaC tool
                        2. Using IaC
                            1. Rent a Physical machine
                            2. Spin Up VMs
                            3. Install Core Software (can start here if working with you already have a logical machine available frm the beggining)
                                1. Always: Cloud Simulation Software (Tools: cloudsimplus, edgecloudsim)
                                2. Maybe
                                    1. VPN
                                    2. Dev CLI tools Vim, Curl & Unzip
                            4. Configure the cluster using the cloud simulation tool's cli
            5. Analytics/BI System (Tools: (1) All in one: (1.1) Lightweight: (1.1.1) Open Source: umami, plausible; (1.1.1.1) AI-powered: DeepBI; (1.1.2) Managed: Google Analytics; (1.2) Fully Fledged: (1.2.1) Open source: Snowplow, DataHog; (1.2.2) Managed: Tableau. (2) You use to build your own Analysitcs System: dbt + piperider (data build tool); Excelize; OLAP dbs)
                1. SQL Database (generally Data whareshouse (could be the same used for ML)) (Tools: OLAP DB (e.g., ElasticSearch, DuckDB, Druid, Materialize) + In-wharehouse transform tool: dbt + piperider):
                    1. User metrics
                        1. Churn
                        2. Click through funels
                        3. Number of users
                        4. User growth
                        5. FrontEnd metrics (aka measuring everything user does)
                            1. Number of [important user ctivity] per [unit of something] (specific to business)
                            2. User overall stats
                            3. User journey (session activities)
                    2. DevOps metrics
                        1. Time from writig code to deployed service
                        2. Time to repair bug
                        3. Deploys/day (ideal, but hard) or deploys/month
                    3. Business metrics
                        1. Costs
                        2. Revenue
                        3. Growth
                        4. Cost/acqusition
                    4. Decision-making metrics: actions that we take, and would like to assess if they were good or not (e.g., how would demand be if we hadnt given discounts?)
                2. Data pipelines
                    1. From Business Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb): that extracts analystics data from Business Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb), structures it, and stores into Analystics Database
                        1. Batch (scheduled jobs, requests data) (tools: e.g., spark/koalas, Dask, Fugue, Ray)
                        2. Streaming (executes when event occurs (receives stremed message from Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb))) (tools: (e.g., Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, Beam (higher level, built on top of batch and stream processing tools), Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming), hazelcast))
                    2. From Manual Decision-Making Repo (where we organize manually the actions we took along with some maanually curated business metrics)
                3. Backend (Tools: (1) Python: (1.1) HTTP API libraries (build them as a library for Dynamic HTTP Server Library to use): FastAPI; (1.1.1) REST (Old messy way very tied to DB principles): plain FastAPI; (1.1.2) RPC (New functional way that treats paths as functions, payloads as parameters, compresses data and fully embraces http 2.0)): gRPC + FastAPI; (1.1.3) GraphQL (New DB way that ignores vestige HTTP paths, treat payloads as lossened DB queries (not just reading, but also updating & adding data) payloads as parameters, compresses data and fully embraces http 2.0): add Ariadne + FastAPI; (1.2) Dynamic HTTP Servers (handles parallel socket connections, calling HTTP API libraries, error returns): Gunicorn (WSGI) with Uvicorn (ASGI) interface; (1.3) HTTP Reverse Proxies: NGINX; (2) Nodejs: (2.1) HTTP API libraries: Dont know any; (2.1.1) REST: plain FastAPI; (2.1.2) RPC: gRPC; (2.1.3) GraphQL: GraphQLjs; (2.2) HTTP Servers: HTTP Module; HTTP Module; (2.3) Single Dynamic HTTP Framework: Express, Nestjs; (1.3) HTTP Reverse Proxies: NGINX)
                    1. Communicates with frontend and Analytics Database
                    2. Get payments data from Payments System API '' which user bought what and when? ''
                4. FrontEnd (Dashboard) (Tools:
                    (1) From scratch
                        (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                        (2) Automatic template: screenshot-to-code
                        (3) Implementation
                            (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                ((1) Code:
                                    (1) Template Libraries: Grapesjs
                                    (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                    (3) Supporting Libraries: 
                                        (1) State management: Redux; 
                                        (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                        (3) Plots & Dashboards: deck.gl, d3, tremor 
                                    (4) Telemetry
                                        (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                        (2) Session Replays: OpenReplay
                                    (5) UI Dev Environment: Storybook, One
                                (2) Build: Webpack; 
                                (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                (4) Build & Web Serve: 
                                    (1) SSR: Nextjs, Astro, remix; 
                                (5) Code & Build: 
                                    (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                    (2) Static Site Generators: Hugo, Gatsby, Quartz
                    (2) Using task-specific high level tools
                        (1) Dashboards: metabase, plotly, graphana)
                    1. Overall stats
                        1. KPIs vs {time, online experiments (often called online testing)}
                        2. Click-through funels
                    2. User stats
                        1. Overall User stats vs time
                        2. Session User activity
                    3. Product stats
                        1. Product KPIs vs {time, user groups}
                        2. Click through funels
                5. Automatic Alerts/notifications and Notifications (via slack, email or dedicated alerts manager like: keep)
            6. Networking setup
                1. VPNs
                    1. Router-based (site-to-site) (Implemented in LAN routers, every packet that arrives to a router, with some specific IP destination (public IP of the other router), gets IPSec applied on it to build a new secure packet which is then sent. This new packet is a secure wrapper arounf the old packet and can be pealed off by the other router) (But why do we need IPSec it if we have good old SSL/mTLS for the app protocol? TL;DR: because its the native security solution for settip up VPNs; More detailed explanation: (1) To avoid worrying about opening ports in the router's firewall, when router peals off an IPSec packet, he knows it is secure and can be forwareded to the right physical machine (2) To add another layer of security (e.g., in case you forget to set up mTLS on the apps); (3) You might not want to worry about setting up mTLS on all your internal apps (browser makes it easy for you, but in your apps you have to  set it up) or want to speed them up). Semantic distinctions:
                        1. Onpremise2Cloud (machines on premise acessing cloud machines and the opposite)
                        2. Onpremise2Onpremise (machines on premise acessing premise server in other location)
                        3. Cloud2Cloud (machines on cloud acessing cloud server in other location)
                    2. Client-Server-based (Implemented in client machine. Every requests is redirected to VPN server where it will go from there to webserver with VPN Server IP as source IP address) (SSL/mTLS on TCP sockets like webserver-browser connections).
                        1. Machine2Site. Semantic distinctions:
                            1. Machine2Onpremise: VPN server runnig inside a machine in the comapnie's on-premise cluster (e.g., worker using company laptop at home accessing company on premise machines)
                            2. Machine2Cloud: VPN server runnig inside a machine in the comapnie's cloud cluster (e.g., worker using company laptop at home accessing cloud machines)
                        2. Machine2Internet: VPN server is hosted by the VPN provider (e.g., worker using company laptop at home accessing internet)
                2. Proxies 
                    1. CDNs (sit in front of client machine) (Tools: Cloudflare, Fastly): distributed geographically to lower latency. They cash common requests by doing async periodic requests to the web server. You can think of it as give a bit of freshnes away for lower latency. But the thing is: frontends dont change a lot, so high freshness is not necessary.
                    2. Reverse Proxies (sits in front of services, inside the LAN)  (Tools: NGINX)
                        1. Capabilities
                            1. Load Balancing
                                1. Avoid server overloading
                                2. Take advantage of multiple servers
                                3. If server goes down, does not send packets to it anymore
                            2. Web acceleration
                                1. Filter/preprocess packets
                                2. Cash requests (usually static files)
                                3. Compress packets
                                4. Do SSL on behalf of servers
                            3. Security and anonymity
                                1. Hides servers IP
                                2. Can add extra security layers
                                3. Avoid malicous users attacks to overload the servers
                        2. Patterns (not necessarily mutually exclusive)
                            1. Backend-for-Frontend (BFF)/API gateway (Tools: (1) Without Authentication: NGINX with Redis, Kong, Traefik, higress, Zuul; With Authentication: Supabase, Firebase, Unkey)
                                1. Manages Frontend auth, is free to access any service
                                    1. Receives (username,password) and returns encrypted JWT (Jason-Web-Token) to Frontend. 
                                    2. Has a DB storeing user information: username, email, password, etc
                                2. Implements rate limiters to avoid overloading our services
                                3. Simplifies API for Frontend
                                    1. Instead of having to make a bunch of requests for different microservices, just to one
                                    2. When you modify/add/remove internal APIs, frontends dont need to be preocupied about it
                                4. Sends (re, res) tuples to Monitoring System
                                5. Makes it faster for the client to get the data it needs, because communications within clsuter are much faster
                            2. Reverse-proxy (Tools: NGINX) (yes, the same name, it is indeed confusing): provides security (SSL termination, rate limiting, and IP filtering), scaling (load balancing), web acceleration (caching, compressing)
                                1. Load Balancer: distributed traffic to different service instances for better utilization of them and for bandit/ab testing purposes
                3. DNS
                    1. Reserve a domain and chose subdomain
                    2. Put your entry point IP(s) on some DNS Server (therre a re a bunch of services for this)
                    3. Do load balancing with dynamic DNS (distributed traffic to servers dynamically)
                4. SSL Certificate
                    1. Minimal SSL Certificate (tools: (1) Certificates Mangers: lemur; (2) Certificate Services: Let's Encrypt (client tool is called Certbot)): only need to demonstrate control over the domain. This means that the certificates only pretects the browser from Man-in-the-middle attacks (where attacker modifies packets by hacking/being routers between you and the webserver so that they are sent to him (pretending to be webserver to browser), sent back, and then sent normally to webserver (pretending to be browser to webserver)). This does not protect attacks were attacker has access to webserver machine & DNS spoofing attacks.
                        1. Generate <public, private> key pair for your webserver
                        2. Generate a certifacte with your prefered SSL Certifacte Service, using your public webserver key & its domain
                        3. Store certificate in the webserver environment
                        4. Config/code web server to handle HTTPs using the certificate
                    2. Extended Verification SSL Certificate (proves the identity of the webserver to the browser. (1) You prove that the website is legimate (not serving virus & not impersonating other person/company) website and that you maintain it; also need to give your personal/company info to the CA (which will be stored and linked to your domain); (2) Certifacate authority signs (encrypts) webserver's domain; (3) Browser receives <claim: domain, webserver public key; proof: CA-signed domain, CA-signed webserver public key, encryption algorithm, certifacte authority (CA)>; (4) Browser decrypts CA-signed domain & public key using built-in public key of CA; (5) If result is <domain, webserver public key> then you know CA authorized that domain and public key. This protects against certificate or domain borrowing. However, this does not protect against a website that once was ok, but decided to go malicous (while the certificate is still valid). Lets suppose you get your certifate for your website my-domain.com and then you change try to take attack users by now serving malicous files, the browser will authenticate normally your website & get those files. However, virus reports will pop up fast & your domain will be blacklisted (browsers get blacklisted domains from time to time from blacklist DNS) and you/your company will get into major trouble because they have your info (unless that info was fake 7 unverified by the CA)
            7. Online Payments System (Tools: hyperswitch)
                1. Built around third-party service payment gateway
                2. Exposes HTTP API to all other microservices
                3. Not shure, but I think third party service is not equipped to handle too much traffic, therefore an internal infra should be
                storing and serving payments data, which they get calling peridically the third-party API.
            8. Private registries (Tools: Harbor, distribution, cri-o)
                1. Image registry (Tools: trow, Glance) (_Note_ Private image registry can also be implemented inside the Artifact Store, since images are artifacts. Its is not like this here because images are more general, they are not ml-specific. So maybe its usefull to keep Aritfact Store an ML-subsystem and Image Registry Separated.)
                    1. Types of images
                        1. Container Images
                        2. VM Images
                            1. Lightweight VMs
                            2. Full-blown VMs
                    1. Stores:
                        1. Image itself
                        2. Image manifest (e.g., for docker is a dockerfile)
                        3. Image metadata
                            1. Image version
                            2. Runtime Requirements
                                1. Host OS
                                2. Hardware: storage, memory, processors, networking, peripherals
                            3. Runtime Constraints (For edge deployments)
                                1. Host OS
                                2. Hardware: storage, memory, processors, networking, peripherals
                            5. Relavant metadata (who pushed, when was pushed, name of service)
                            6. Base image
                            7. Status:
                                1. Pushed
                                2. Staged
                                    1. Failed staging: problematic image
                                    2. Passed staging: production-ready
                    2. Additional Capabilities
                        1. Security: scan images for vulnerabilties (Tools: trivy, grype, Clair, neuvector)
                2. Pod Registry
                    1. Pod manifest
                    2. Pod metadata
                        1. Pod version
                        2. Runtime Requirements
                            1. OS
                            2. Hardware: storage, memory, processors, networking, peripherals
                        3. Runtime Constraints (For edge deployments)
                            1. OS
                            2. Hardware: storage, memory, processors, networking, peripherals
                        5. Relavant metadata (who pushed, when was pushed, name of service)
                        6. Helm/Kustomize/timoni/JuJu/cdk8s/kpt charts dependent on this pod manifest
                        7. Pod Dependencies
                        8. Status:
                            1. Pushed
                            2. Staged
                                1. Failed staging: problematic Pod
                                2. Passed staging: production-ready
                    
                3. Package Registry (Tools: Github packages). Stores versioned packages. Used when working with monorepos.
                    1. Package itself
                        1. Source
                        2. Build
                    2. Package metadata
                    3. Alternative package distributions (e.g., previous versions)
            9. Recycle Bin System (analogous to recycle bin in pc, mobile or cloud)
                1. Capabilities
                    1. Receiving Trash data (that has Trash metadata: type of trash, specification of the client that is deleting it)
                        1. Receives data trash data
                        2. Stores in Trash Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb) with time stamp
                    2. Responding Trash data queries
                    3. Restoring Trash data
                        1. Respond Trash data query with thatspecific trash data
                        2. Delete Trash from Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb)
                    4. Periodically Deleting Trash
                        1. Automatically: as a function of time window and clients interacting with it
                        2. Human-based:
                            1. Human can delete Trash data at any time through FrontEnd given he has access credentials
                            2. After Automatic identification of data it wants to delete, ask human to give ok
                2. Components
                    1. Core Database functionality
                        1. Option 1: Backend + OLTP DB: backend that Receives Trash Disposals and Queries and communicates with DB
                        2. Option 2: OLAP DB (e.g., ELasticSearch, DuckDB, Druid, materialize) that can be managed or not.
                    2. FrontEnd to allow Visual Inspection of Trash currently stored
            10. IT Monitoring & Observability (Out of ML System Scope Jobs. Applies to other systems with the IT of the commpany aswell) (Tools: (1) Open source: e.g., SREWorks, zabbix, netdata, Nightingale, Ddosify, OpenObserve, Nagios, highlight.io, hyperdx, pixie, Retina, skywalking, OpenTelemetry, Beats, Elastic Stack, coroot, tetragon, Cilium, Hubble, Prometheus/Thanos/Cortex, VictoriaMetrics, Grafana Pyroscope, Istio/Kiali/linkerd2/Cilium/Consul, kubeshark, hertzbeat, openclarity, Keptn; (1.1) Networking: ntopng, FastNetMon, akvorado, librenms; (1.2) IoT: ThingsBoard; (1.3) Web: web-check; (1.4) Security: falco, IntelOwl, Watcher, opencti, caldera, Sn1per, faraday, wazuh, Shuffle, metlo, hackingtool, securityonion, prowler; (1.5) Helpers: CyberChef; (1.6) OS: sysdig, eBPF, ecapture; (2) Managed: Datadog, Sentry, Honeycomb, SignalFx, CloudWatch, New Relic, Librato, Algolia, Splunk, BigPanda, Sensu, Middleware, AppDynamics, Raygun, Splunk Cloud, eG Enterprise) Traffic Routing and Observability (Tools: e.g., Istio/Kiali/linkerd2/Cilium/Consul) for microservices health in general. _Note:_ highly recommended to follow [Open Telemetry](https://opentelemetry.io/) standard
                1. Data injection
                    1. Streaming (passive): Subscribe to streams of messages
                    2. Requests (active): make requests peridically or based on some event
                2. Logging System (Tools: ELK stack, Loki) (''Log everything that moves or might move'') (technically, logging is part of Monitoring, but it is good to differentiate the two systems, because eventtracingmonitoring gets massive amount of unfiltered data that will be checked only in debug situations, while the Monitoring System deals with high quality, meaningfull data and is used all the time)
                    1. De-identification of logs (Tools: Presidio)
                    2. Transport logs (Tools: vector, FluentD, monolog, loguru, structlog, flume)
                        1. Batch (logs are sent as request payloads, in a request to the Data Lake, requesting them to be stored)
                        2. Streaming (logs are produced as messages to a message queue/broker that stores them and make them available in order for consumers)
                    3. Process logs (Tools: Logstash)
                        1. Processing Modes
                            1. Batch processing (''Many companies process logs in batch processes. In this scenario, you collect a large amount of logs, then periodically query over them looking for specific events using SQL or process them using a batch process like in a spark/koalas or Hadoop or Hive cluster. This makes the processing of logs efficient because you can leverage distributed and mapreduce processes to increase your processing throughput. However, because you process your logs periodically, you can only discover problems periodically'') (Logstash is a tool)
                            2. Stream Processing (''To discover anomalies in your logs as soon as they happen, you want to process your events as soon as they are logged. This makes log processing a stream processing problem. You can use real-time big data stream transport tools (e.g., Kafka, Pulsar, Kinesis, Pravega) or gain more customizability with message broker builder (e.g., RabbitMQ) to transport events as they are logged. To search for events with specific characteristics in real time, you can leverage a streaming SQL engine like KSQL or Flink SQL'')
                        2. Processing Techniques
                            1. ML (''An example use case of ML in log analysis is anomaly/outlier detection (e.g., isolation forest): to detect abnormal events in your system. A more sophisticated model might even classify each event in terms of its priorities such as usual, abnormal, exception, error, and fatal. Another use case of ML in log analysis is that when a service fails, it might be helpful to know the probability of related services being affected. This could be especially useful when the system is under cyberattack.'')
                    4. Store logs (Tools: (1) search-focused OLAP DB/Engine like: ElasticSearch, OpenSearch, Solr, Sphinx Search, Xapian, and Nutch; AI-powered higher level search engines: Swirl, Devv, danswer, perplexica)
                    5. Visualize logs (Tools: Grafana, Kibana): Web Frontend to visualize the results of the search engine (Kibana (Frontend) combines very weel with Elastic Search (text-search-focused Data Lake).)
                3. Live Performance Informer: a service that informs services within the system about live operational metrics of services within the system, so that they can be fully aware of the surrounding services state without each of them having to do this work separetely
                    1. Liveness: which services are responsive and which are not (heartbeat pattern). It is relevant because many services will have to activate defaults or fallbacks if a dependent service is not responsive. It is also faster and more standardized than each service verifying liveness of each depedent service on its own.
                    2. Load: the load and resource usage each service is under.
                4. What to monitor (Note: metric smust have metadata such as time, version of the service, specific pod>container, cluster servic namespace it belongs to, phyisical cluster the namespace belongs to)
                    1. Dependency Updates
                        1. Data metadata
                        2. APIs
                        3. Libraries (important to tag as backwards compatible or not)
                    2. Documentation: monitor changes in documentation of the ML System and artifacts. These can reveal the problems diffcult to detect via other means (e.g., if unit in some data schema changes from kg to punds)
                    3. Microservices (tools: Prometheus/Thanos is widely used, in cobination with Grafana for web UI dashboard also) (is recommended to use service mesh tool like Istio/Kiali/linkerd2/Cilium/Consul) (very manual)
                        1. Sources of data
                            1. Directly
                                1. Microservice core application code (Container Image builder (Tools: Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) + Telemetrer (Tools: cadvisor) + Processor Acceleration (Tools: NVIDIA Container Toolkit))
                                2. Background jobs (side container in the same pod)
                            2. Indirectly: Logging System
                        2. Data Analysis
                            1. Vanilla Monitoring (is something wrong? where?)
                                1. Monitoring data analysis
                                    1. Attack/Troll attempts
                                    2. Operational metrics (health)
                                        1. Approaches
                                            1. Passive: detecting problem (metric reaches certain treshold)
                                            2. Active: Forecasting problem (when metric will reach certain treshold if we continue trend)
                                        2. Metrics (Tools: OpenMetrics, Telegraf)
                                            1. Computing Environment Liveness
                                                1. Physical Machines up
                                                2. VMs up
                                                3. Services up
                                                4. Specific Pods up
                                            2. Hardware Resource Usage (Per Machine)
                                                1. Processor usage
                                                2. Memory usage
                                                3. Energy consumption & carbon footprint (Tools: codecarbon, scaphandre, mlco2)
                                            3. Errors: whcih error where encountered, where, which error handling polciy was used, status of the error (pending resolution or resolved)
                                            4. Application Performance (Per service)
                                                1. Basic
                                                    1. Availability
                                                    2. Latency (response or task latency)
                                                    3. Throughput (response or task throughput)
                                                    4. Traffic Volume   
                                                2. Advanced (SLOs)
                                                    1. % succesfull networking/communication for each service or broker-consumer pair
                                                        1. Requests dropped or with error codes
                                                        2. Dropped streaming data without being processes
                                                    2. Uptime (''The conditions to determine whether a system is up are defined in the service level objectives (SLOs) or service level agreements (SLAs). SLAs are "a description of consequences and penalties if the SLOs are not met". For example, an SLO may specify that the service is considered to be up if it has a median latency of less than 200 ms and a 99th percentile under 2s.'')
                                                    3. Storage Capacity and Durability
                                                    4. Goodput (% of time training is improving the model)
                                        3. Links: link machines to service currently running on them 
                                    3. Queue lengths
                                    4. Unexpected exception count
                                2. Monitoring actions
                                    1. Automatic alerts/notifications if some metrics reach a treshold (thorugh e.g., whatsapp, email, slack or dedicated alerts manager like: keep). _Note:_ alerts should go off actually before the metric reaches the treshold, it should go off when IT monitoring forecasts that the treshold will be reached in the near future, so that engineers can take appropriate actions before the problem occurs. 
                                    2. Operational metrics (system health) report generation
                                    3. Autoscaling (up and downscaling)
                                        1. Software scaling. Can be triggered by: (1) traffic rate above a treshold; (2) Resource usage above certain threshold; (3) SLOs below certain treshold.
                                            1. Deploy more/less replicas of services (usually more Kubernetes/OKD pods) when needed and then downscale when not needed anymore. 
                                                1. Stateless services: easy, just provision more replicas (e.g., scaling Inference Services)
                                                2. Stateful services: hard, need a k8s operator specifically for this. (e.g., scaling databases or message brokers)
                                            2. Deploy more/less external entrypoints (node port, ingress, etc)
                                        2. Hardware scaling (only if software scaling is not sufficient)
                                            1. Allocate more/less hardware resources to the cluster namespace
                                            2. Provision more hardware resources
                                                1. Vertical Scaling (machines with more/less resources: storage, memory, processors, networking bandwidth)
                                                2. Horizontal Scaling (more/less machines of some specific type, more/less load balancers or masters)           
                            2. Observing (Fine-grained Monitoring for Debuggabulity) (what are the roots of this wrong thing? what other parts of the system it affects?)
                                1. Approaches
                                    1. Manual Analysis
                                    2. Automated Analysis
                                        1. Anomaly detection
                                        2. Cycle detection
                                        3. Window Size Optimization
                                1. What to observe
                                    1. Detailed Logs (''When we log an event, we want to make it as easy as possible for us to find it later. This practice with microservice architecture is called distributed tracing. We want to give each process a unique ID so that, when something goes wrong, the error message will (hopefully) contain that ID. This allows us to search for the log messages associated with it. We also want to record with each event all the metadata necessary: the time when it happens, the service where it happens, the function that is called, the user associated with the process, if any, etc'') (Tools: goaccess)
                                    2. Tracing (Tools: Odigos, Jaeger, Zipkin, Elastic APM)
                                    3. Traffic routing info
                                    4. Possible vulnearabilities info
                    4. Cloud management (tools: cloudcustodian)
                        1. Cost (Tools: infracost, opencost) (This involves infrastrcture, training, tuning and inference costs)
                            1. IaaS (per/time (on demand or pre-scheduled plans))
                                1. Machines
                                2. Networking Infrastructure
                            2. PaaS (per/usage, per/time, per/users)
                            3. SaaS (per/usage, per/time, per/users)
                        2. Security. Policies for:
                            1. Role-based Access control
                            2. Vestige resources
                            3. Unused resources
                            4. Secrets Management
                    5. Generalized CI/CD Monitoring (Tools: Backstage)
                        1. What to Monitor
                            1. Traces (Update (code, config, data source and/or artifact)-> CI --> CD --> Effect)
                            2. Metrics: (Tools: fourkeys) ("When calculating metrics, you can choose between spot checks and exhaustive checks. Spot checks involve sampling a subset of data to quickly identify issues, while exhaustive checks evaluate every request for a comprehensive performance view")
                                1. Deployment Frequency. Requirement baseline for this should offset by model decay metric an new data availability
                                2. Commit2Deployed time. Requirement baseline for this should offset by the time working with the ML system and how often users expect the system to get better.
                                3. Mean time to restore (when something crashes, how long to fix it (can include manual debugging, deployment and possibly retraining)).
                                    1. Fixing Service bugs. Requirement baseline for this should offset by how critical a wrong prediction is to the client apps (how long can a bug last in prod and not causa a massive impact. And this is already optimistic because you would have to include the time to find the bug also)
                                    2. Fixing Non-availability of the Service. Requirement baseline for this should offset by how critical unavailability of the service is to the client apps
                                4. % of changes leading to a worse system
                                    1. Level 2 - MLOps platforms: worse system is measured by the above metrics, operational metrics, model decay relative relative to no retraining, etc
                                    2. Level 3 - Inference Services (Models): is measured by corrected predictive power metric os models in production data (often proxied with by soft labels) and subsequent user actions/time spent in the app
                                5. % of tests that passed. (Should be around 60-70%)
                        3. Sources of data:
                            1. Genralized CI/CD tool (aka Workflow Orchestrator)
                            2. Repo (e.g., github/gitlab/gitea (self-hosted))
                            3. Analytics System
                4. IT Monitoring actions
                    1. Change A/B testing traffic rules
                    2. Rollback release
                    3. Scale (Upscale or Downscale) Infrastrcture
                    4. Modify IT Monitoring configuration
                    5. Modify low-level KPI goals
                    6. Modify codebase
                    7. Modify ci/cd
                    8. Modify people processes
            11. Business Monitoring: Business KPIs (if these arent growing than ML does not justify itself) (Use Analystics System client library  for this)
                1. Sources of data
                    1. Microservices
                    2. Analytics System
                2. KPI types
                    1. Online: these need to be monitored in real-time and strange values should be investigate asap
                    2. Offline: these can be monitored async, no urgency
                3. Data Analysis: KPIs
                    1. Which metrics
                        1. Final metrics: revenue, new users, retention, etc
                            1. Advantage: reflect exatcly hat the business wants
                            2. Disavantage: can be affected by a lot of moving parts, thus cannot say for shure thata chnge in it was caused by the ML system change
                        2. Proxy metrics: user engagement, conversion rate etc. _Note:_ There is a spectrum going from very close to ML metrics to very close to Final business metrics. As you go down the spectrum in the ML metric direction you get see that the advantages and disavantages below get more intense
                            1. Advantage: more close to the ML System, affected less by other systems, thus can say with more confidence that a change in it was caused by a change in the ML System; therefore its easier to optimize it.
                            2. Disavantage: An improvement on it does not necessarily mean improving what the business ultmately wants
                    2. Which tresholds for these metrics define success
                        1. Often more useful to use a proxy metric in near nearm term and final metric in long term.
                        2. It can be very helpful to try to estimate relationship between the metrics, this lets you optimmize a metric near ML, do calculations and get rooughly the effect on a metric near the final business metric.
                    3. Causal Analysis:
                        1. KPIs are going up/down because of ML model (or decisions made because of our model) or they would go up/down anyway?
            12. Production Expectation Error Handling: built-in every subsystem.
                1. Types
                    1. Pre-catching Errors. Every subsystem has can have 2 types of routines to try to catch problems before they actually produce errors.
                        1. Data checks: checking if input data satifies some constraints.
                        2. Prerequisite checks: checking if external services are in a desired state for the execution of a specific workload (e.g., there is a table with certian schema in the dat wharhouse or a specific service is up).
                    2. Handling Errors (the disavantage of handling wrt to pre-catching is that you are not fully aware of what caused the error)
                2. Steps after error
                    1. State: save state of processing when error occurs
                        1. Save internally
                        2. Use a task queue
                    2. Policy: the workload adopts a policy to how it should continue processsing after some error was encountered. Some types of policies can be:
                        1. Continue processing assuming erroneous input is isolated and will not appear or affect the majority of other inputs, and appends error code together with its output to the next component/system.
                        2. Use a fallback
                        3. Halt processing until error is fixed (assumes error is structural and will appear or afect next inputs), and outputs error code/some default value (e.g., None) to the next step
                    3. Report: When workload encounters an error, it generally sends the bad data error specification (should enable easy debugging) to the Monitoring System.
                    4. Correction (if workload was halted): receives a manual trigger from the Monitoring System saying it can go back to the moment error occured (retrieve error state) and do processing (retrieving again the input) because now input was corrected.
            13. Alerts/notifications (Tools: Apprise, MLNotify)(technically alerts/notifications are a part of Monitoring, but it is usefull to separate them from the Monitoring System, since it is separate from the Monitoring dashboard, and you dont want to overload your monitoring system with alerts/notifications. However, it does point you to places in the Monitoring dashboard you should look at.)
                1. Alert policy: condition for alert
                    1. Failed expectation
                    2. Problem not already covered by a more specific alert (e.g, "let's say we receive an alert that our overall user satisfaction ratings are reducing but we also receive another alert that our North American users also have low satisfaction ratings. Here's the system would automatically assess for drift in user satisfaction ratings across many different slices and aggregations to discover that only users in a specific area are experiencing the issue but because it's a popular user base, it ends up triggering all aggregate downstream alerts as well")
                2. Notification channels (from most urgent to least):
                    1. Phone call
                    2. Dedicated alert manager (Tools: keep)
                    3. Slack
                    4. Whatsapp
                    5. Email
                3. Ticket Content
                    1. Metadata
                        1. Which product
                        2. Time alert was generated
                        3. Source of the alert
                            1. Cluster
                            2. Machine
                            3. Service
                        4. Records
                            1. From the relevant window of time
                                1. Traces
                                2. Logs
                            2. Lineage Records of the failing component
                    2. Payload
                        1. Description (description of the alert)
                            1. Type of alert
                            2. Expectations failed
                            3. Tests that were conducted
                            4. Data used in tests
                        2. Call to action (what should the programmer do)
            14. User Product Feedback System (Goal is to receive feeback from users, so that the productcs can be improved continuously. _Note_ Feedback shouldnt be taken to be true, they should be verified before.)
                1. Components
                    1. Backend: exposes a feedback API, organized by product. UIs send feedback using these endpoints. Any other service can also get feedback data via this API. 
                    
                        (Tools: (1) Python: (1.1) HTTP API libraries (build them as a library for Dynamic HTTP Server Library to use): FastAPI; (1.1.1) REST (Old messy way very tied to DB principles): plain FastAPI; (1.1.2) RPC (New functional way that treats paths as functions, payloads as parameters, compresses data and fully embraces http 2.0)): gRPC + FastAPI; (1.1.3) GraphQL (New DB way that ignores vestige HTTP paths, treat payloads as lossened DB queries (not just reading, but also updating & adding data) payloads as parameters, compresses data and fully embraces http 2.0): add Ariadne + FastAPI; (1.2) Dynamic HTTP Servers (handles parallel socket connections, calling HTTP API libraries, error returns): Gunicorn (WSGI) with Uvicorn (ASGI) interface; (1.3) HTTP Reverse Proxies: NGINX; (2) Nodejs: (2.1) HTTP API libraries: Dont know any; (2.1.1) REST: plain FastAPI; (2.1.2) RPC: gRPC; (2.1.3) GraphQL: GraphQLjs; (2.2) HTTP Servers: HTTP Module; HTTP Module; (2.3) Single Dynamic HTTP Framework: Express, Nestjs; (1.3) HTTP Reverse Proxies: NGINX)

                    2. Feedback Processing: retrives feedback data from DB, does pricessing on it, and stores results again in DB (e.g., sentiment of feedbacks)
                    3. DB: stores the feedbacks with metadata such as person who sent, time it was sent, sentiment of feedback, which product, version of the product (repo commit ID)
                    4. FrontEnd/UI: lets prudct peoople see, filter, share & calculate anlytics on feedbacks (Tools:
                        (1) From scratch
                            (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                            (2) Automatic template: screenshot-to-code
                            (3) Implementation
                                (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                    (1) Code:
                                            (1) Template Libraries: Grapesjs
                                            (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                            (3) Supporting Libraries: 
                                                (1) State management: Redux; 
                                                (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                (3) Plots & Dashboards: deck.gl, d3, tremor 
                                            (4) Telemetry
                                                (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                (2) Session Replays: OpenReplay
                                            (5) UI Dev Environment: Storybook, One
                                    (2) Build: Webpack; 
                                    (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                    (4) Build & Web Serve: 
                                        (1) SSR: Nextjs, Astro, remix; 
                                    (5) Code & Build: 
                                        (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                        (2) Static Site Generators: Hugo, Gatsby, Quartz
                        (2) Using task-specific high level tools
                            (1) Dashboards: metabase, plotly, graphana)
            15. Event (and Message) distribution & queuing (Tools: RabbitMQ, NATS, ActiveMQ, ZeroMQ) (every time a write something like "triggered by", think of the pod requesting or subscribing to the message broker for that event. Why use message broker & not Worflow orchestrator for this? You could frame the service as a task & perform it only on some conditon with an workflow orchestrator, however, the workflow orchestrator would need to deploy the pod & undeploy the pod each time. For events that happen a lot & strict latency requirements, this becomes too slow. You would want to use the worflow orchestration approach for low frequency jobs with looser latency requirements. Plus, the rate at which events are produced is generally faster then the rate of consumption, so you need to queue these events, and this is not native to Workflow Orchestrators. _Note:_ when you are using a workflow orchestrators it is often usefull to attach it to a message broker, because it simplifies the process of listening to multi-source events & queuing them. _Note:_ message brokers/queues alo provide the req/reply mode where the producer, after sending its message, check some slot in the reply queu to see when the reply arrived. _Note:_ they also support authentication of producers and consumers.)
                1. Event producer Pods send events to the Message Broker
                2. Event Consumer Pods consume events either with re/res or streaming pattern
            16. Discovery System
                1. Services Discovery (e.g., LDAP protocol). Frontend + App Server that lets you discover and use internal services via a single interface.
                2. Data Discovery
                3. Internal Package Discovery
            17. Backup System (tool that lets you backup, delete and restore data from multiple different data sources and supports multiple data modalities)
                1. By source
                    1. Operating Systems/User programs (Tools: restic, duplicati, backuppc, kopia, rnsnapshot, )
                    2. Gmail (Tools: Gmvault)
                2. By modality
                    1. Image & Video (Tools: immich, ente)
            18. Employee Data Wharehouse (Stores data about all employees, each employee is referenced in the system via his ID. Its populated by the onboarding job.)

        2. ML-specific Subsystems
            1. Prediction Manager (Tools: seldon, ray serve, bento ml) (sits in front of master Inference Services).
                1. Summary (does these two options things in paralell)
                    1. Registers model consumers (which service is consuming which model) with the Model/Prompt Registry as soon as a service registers itself to consume a certian models
                    2. Receives request to do a task with provided input
                    3. Gives ID to the request (aka Correlation ID or TraceId)
                    4. Puts request on a request queue
                    5. Pop requests out of the queue
                        1. Batching: When queue is filled with a full batch (note: batch can be of size 1), get the request batch and proceeed next steps in batch mode
                        2. Scheduling: at a specific frequency to avoid overloading Inference Services
                        3. Service metrics aware: when service operational metrics (quereis from live performance informer) drop: CPU, memory, networking (e.g., below 100%)
                    6. Get inference piepline from piepline registry, according to the inference config
                    7. A simple inference pipeline:
                        1. Requests Feature Store for point-in-time feature row (remember that the request is timestamped) features of the specific model (that does the task the client wants)
                            1. If the features are ready (ready means that the last made datapoint in the feature store is within our time margin):
                                1. Not using request-time data: If all the features needed are the ones Feature Store gave: get the featured datapoint and continue normally
                                2. Using request-time data: If there are still some features that the we need to build, that are functions of data we just received on the request
                                    1. Stream raw data to inference data pipeline (Option 1: use streaming data processing tool (e.g., Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast) in which the pipeline of tasks is done internally in the tools' dsitributed environment (as defined by you in driver code) & streams back to prediction manager. The good thing is that these tools are very optimized but the bad things is that the tasks need to be launched; Option 2: sequential services connected to each other (pipeline) via streaming and that output features on feature store & streams back to prediction manager. _Note:_ first service will be listening to inference data pipeline trigger along with the request via streaming, and once listened, gets data needed from ML data wharehouse & sends to the next data pipeline service) to make featured data on the fly
                                    2. Listen to incoming stream coming from Infrence Data Pipeline containing the feature datapoint
                            2. If features are not ready:
                                1. Stream request to inference data pipeline (Option 1: use streaming data processing tool (e.g., Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming) in which the pipeline of tasks is done internally in the tools' dsitributed environment (as defined by you in driver code) & streams back to prediction manager. The good thing is that these tools are very optimized but the bad things is that the tasks need to be launched; Option 2: sequential services connected to each other (pipeline) via streaming and that output features on feature store & streams back to prediction manager. _Note:_ first service will be listening to inference data pipeline trigger along with the request via streaming, and once listened, gets data needed from ML data wharehouse & sends to the next data pipeline service) to make featured data on the fly
                                2. Listen to incoming stream coming from Infrence Data Pipeline containing the featured datapoint
                        2. If LLM service: map user input to a prompt via a standard prompt engineering proicedure
                        3. Send model input to an input filter
                        4. Get prediction: request master Inference Service & get the prediction with any additional metadata (e.g., explanation, or feature importance)
                        5. Send model output to an output filter
                        6. Deal with model's uncertainty on the prediction
                            1. If ''Confident=1'':
                                1. Send prediction to Prediction Filter
                                    1. If filter says ok: respond prediction to frontend normally
                                    2. If filter says not ok:
                                        1. Respond to frontend that prediction was caught in predition (output) filter
                                        2. Send POST request to Monitoring Service with: Model Name, Prediction, Client it was supposed to be sent to, Explanation of why it was filtered.
                                2. Return response to the client
                            2. Else:
                                1. If the decision can be delayed
                                    1. Can be delayed a lot (Offline Serving)
                                        1. Human Review: send featured datapoint to annotation (can be more than just labelling) system to be reviewed by humans
                                        2. Ask user for more informative data
                                    2. Can be deleyed only a little:
                                        1. If user can be asked for more information: Prediction Manager sends response to client with:
                                            >Need more information from user=1; I want to know x1,x2,x3 ...
                                        2. Else: 
                                            1. Heuristic Fallback: Prediction Manager sends response to client with his own default fallback plus a fallback ositive so that the client knows it is a fallback option and can decide to use its own fallback: >Use ML Fallback=1
                                            2. Intelligent Appeal: Prediction Manager asks for a model with higher predictive power (but slower and/or costlier)
                                2. If the decision cant be delayed: Prediction Manager sends response to client with his own default fallback plus a fallback ositive so that the client knows it is a fallback option and can decide to use its own fallback:
                                    >Use ML Fallback=1
                        7. Store datapoint and related information in Feature Store (technically this is not Feature Store stuff, but makes life so much easier for Monitoring, since Feature Store then stream all datapoint-related info):
                            1. <input/featured datapoint, output/estimate>
                            2. Time
                            3. Client JWT/Cookie (to get session), ID & location (Extracted from IP adress)
                2. Objectives
                    1. Decouple models from feature building
                    2. Decouple models from other models. If there is a need to compose multiple models, this will be done by the Prediction Manager
                    3. Cash Predictions
                    4. Decouple models from communication patterns (can switch from streaming to req/res)
                    5. Receive and store feedback on predictions
                3. Modes of operation
                    1. Offline (often called batch) vs Online Inference (giving prediction [before/after] some client needs it)
                        1. Offline: used for Processes with slow changing features. Compute prediction before the client needs it. Compute predictions and store them for later. E.g BI or Animal Classifier. _Note:_ often, its best not to make a Inference Service, its best to make a ml prediction dipatcher workload that gets deployed by your workflow orchestrator on new data trigger, the ml prediction dipatcher then deploys multiple parallel ml prediction workloads using a task queue (Tools: celery, rq, kueue) or parallel processing engine (Tools: Ray, Spark, Dask). The excpetion being when you need real-time prediction, then a dedicated service is better.
                            1. Periodically get a batch of datapoints without predictions
                            2. Compute predictions for them
                            3. Store inputs genrated and predictions for them in Feature Store
                            4. Send predictions:
                                1. Active: streaming. Get batch of predictions from Feature Store and stream stored predictions to client(s)
                                2. Passive: serving requests. Receive request from client, get prediction from Feature Store and respond client with this prediction or a prediction for a similar input
                        2. Online: used for Processes with fast changing features. Compute prediction when the client needs it. Compute predictions and immediatelly give them to client
                            1. Fast Serving: Sync. the majority of use cases, users cant wait more than seconds
                            2. Slow Serving: Sync or Async (client has to make quest later or subcribe to events). usually for key decisions that dont have strong latency requiremments (e.g., amount of credit to give to a people in a financial app)
                    2. Streaming vs Serving predictions
                        1. Streaming
                            1. If Process with slow changing features: input via batch, output via streaming
                                1. Periodically gets new offline Static Predictions from feature store
                                2. If serving batch size is available at feature store: get those from feature store and send to client
                                3. Else: peridically check feature store, and send predictions to client(s) when batch size is available
                            2. If Process with fast changing features: input via streaming, output via streaming. Listen to streaming data from ML Data whareshouse , on message arrival compute output and produce message to topic
                        2. Serving Requests (can be requested to give one or a few predictions)), upon request:
                            1. If Process with slow changing features: can precompute predictions for users logged in (Can even do this more intellignetly by using a model that predicts user's behaviour). Gets prediction, that was computed earlier with Offline Inference, from Feature Store, and serves it to client
                            2. If Process with fast changing features: compute prediction at request time and serve it to client
                    3. Unique Client vs Multiple Clients for a single prediction batch
                        1. When Processes with slow changing features: generally broadcasted to Multiple Clients. This is done via Event-Oriented (Streaming) where clients are consumers.
                        2. When Processe with fast changing features: generally sent to unique client and is usually done by seving requests
                    4. Single prediction vs prediction batch
                        1. Prediction batch is used in 3 cases: (1) offline inference and (2) batching requests via a request queue to maximize machine resource usage and (potentially) latency and (3) offering alternatives outputs (e.g., alternative transcriptions with lower joint probability, but that could actually be better than the main one);
                        2. Single prediction is used in online inferecen when request queueu batching doesnt make sense (e.g., request rate is not that high, would actually increase latency if had to wait for a batch of request to be filled)
                4. Prediction Filter (can use rules/scripts or ML)
                    1. Summary
                        1. Receive a fitering request on a prediction from a specific model
                        2. Loads filter from Model/Prompt Registry
                        3. Filter types:
                            1. Problematic behaviour
                                1. Inapropriate content (violates code of conduct) (Tools: Open AI evals, Bench, Pheonix, HELM, lm-evaluation-harness, truelens, checklist, guardrails, outlines, promptfoo, fiddler-auditor, trulens)
                                2. Absurd content (non-realsitic prediction, indicates bug in ML Model)
                                3. Socially Biased content
                            2. Normal behaviour
                                1. Dynamic Constraints (Constraints not present when the model was trained or not usually present. E.g., when you say to youtube that you dont want recommendations from a specific channel anymmore)
                        4. Apply filter and respond with result
                    2. Capabilities
                        1. Filter predictions of any model
                        2. Filter Decoupled from Prediction Filter logic, you can build a new filter and store in Model/Prompt Registry. This new filter filter will be loaded at runtime and used.
            2. Foundation Models System ("your own internal OpenAI") (think of it as sub ML system for general knowledge that will suport the making of more specialized models in the Main ML System): similar to the "Main ML System", but focused on Scale and instead of providing APIs for the general API gateway, it has its own Foundation Models API gateway (that will be consumed by the general ML System tasks and services) that makes use of all FMs. _Note:_ you will probably need multiple copies of the same LLM deployed together to ensure low-latency. _Note 2:_ IO Validation: always need to validate format of input & output of LLM. ("Our research revealed that deploying a fine-tuned DialoGPT-large model on AWS is 55% less expensive, and a staggering 89% cheaper when serverless, compared to OpenAI's GPT-3.5 Turbo"). _Some differences from the Main ML System:_
                1. FM/LLM Data Lake: analogous to the feature store of typical ML Systems. Since LLMs (or other types of FMs) are trained with unstructured data, there sould be a place to get the final training data used to train models, and test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) to evaluate them. _Note:_ stores both unwrangled (raw) data & wrangled (ready) data.
                    1. Organization
                        1. First, by FM model (should include metadata such as if the model is built from a open-sourc ehot-start or built from scracth)
                        2. Second, by:
                            1. Source of data
                            2. Quality of data
                            3. State/Version of data
                                1. Wrangled & Version
                                2. Not Wrangled & Version
                            4. Who stored the data/retrieved it
                                1. People
                                2. Models
                            5. Which Models are in production or even deployed using the data
                    2. Capabilities
                        1. For training data
                            1. Core: Store large corps of data (either receiving it directly or throigh some pointer) being v0 data or v>0 data & serve batch of datapoints (<input, output> samples).
                            2. Internal Processing
                                1. Build a lot of training examples (<input, output>) from an already wrangled (tools: argilla) large corps of data, according to some specification (e.g., size of input & output)
                                2. Identify potetially bad data with using a specific internal ML model to do this task.
                                3. Search
                                    1. Standard Regular Expression (Regex) Search
                                    2. Semantic Search (probably using embeddings & vector database under the hood)
                                4. Statistics on the data
                        2. For test data
                            1. Store and serve test datapoints along with evaluation setup metadata to guarantee reproducibility
                    3. Components
                        1. Backend that interacts with data lake and frontend
                        2. Frontend (Tools:
                            (1) From scratch
                                (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                (2) Automatic template: screenshot-to-code
                                (3) Implementation
                                    (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                        (1) Code:
                                            (1) Template Libraries: Grapesjs
                                            (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                            (3) Supporting Libraries: 
                                                (1) State management: Redux; 
                                                (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                (3) Plots & Dashboards: deck.gl, d3, tremor 
                                            (4) Telemetry
                                                (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                (2) Session Replays: OpenReplay
                                            (5) UI Dev Environment: Storybook, One
                                        (2) Build: Webpack; 
                                        (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                        (4) Build & Web Serve: 
                                            (1) SSR: Nextjs, Astro, remix; 
                                        (5) Code & Build: 
                                            (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                            (2) Static Site Generators: Hugo, Gatsby, Quartz
                            (2) Using task-specific high level tools
                                (1) Dashboards: metabase, plotly, graphana)
                            1. Lets you explore some sample parts of corps or datapoints
                            2. A place to write/get/render docs regarding issues with the current data & approaches for improvement
                        3. Security:
                            1. Communications: Public/Private key authentication together with user roles & SSL/TLS
                            2. Storage: at rest encryption
                2. Data Wrangling Task
                    1. Getting data: get data from various data sources such as: dbs, data wharehouses, object storage, etc (Tools: dlt)
                    2. Parsing Data: parse various data formats (e.g., PDFs, HTML pages, Markdown files, Powerpoint presentations, JPEG images, MP3 audio or MP4 video files) into optimized LLM data formats
                    3. Preprocessing Data: apply basic filtering and transformations
                    4. Storing Data: store in FM/LLM Data Lake
                3. LLM Feature Store: similar to normal feature store, but text-focused instead of tabular-data focused (stores input/output/ground thruth tuples + metadata, serves them to clients and streams them to monitoring system)
                4. FM Services (Robust APIs with FM models powering them)
                    1. Generative ML
                        1. Text2Text: LLMs (Tools: (1) Core: Colossal AI, DeepSpeed, HF Transformers; (2) Acceleration: MosaiCML Composer)
                            1. Support for:
                                1. Formated Inference: restrict output tokens according to a specified format by the user
                                2. Real-time Predictions: when instead of sending a sequence of input tokens to to a sequence of output tokens (e.g., translating a pience of text), you need to keep calling the API to generate each token (e.g., live translation of people speaking). The service needs to then remember the tokens it generated in earlier calls.
                            2. Domain-specific:
                                1. NL2SQL
                                2. NL2Latex
                        2. Text2Image (Tools: DALL-E, Midjourney)
                        4. Text&Image2Image (Image Editing)
                        5. Text2Video (Tools: Sora)
                        6. Text&Video2Video (Video Editing)
                        7. Audio2Text (Tools: Whisper, SpeechGPT)
                        8. Audio&Text2Audio (Tools: Audio Editing)
                        9. Text2Speech (Tools: TTS, SeamlessM4T, OpenAI Audio API, IMS-Toucan, Parler-TTS, GPT-SoVITS)
                        10. Speech2Speech (Tools: Moshi)
                    2. Embeddings (Encoders for generative embeddings from data (e.g., BERT, CodeBERT) & decoders for generating data from embeddings (e.g., some transfomer decoder variant or stable diffusion decoder)) (Tools: finetuner, LASER, fastText, sentence-transformers, clip-as-service, mmpretrain)
                        1. By task
                            1. Text2Embedding & reverse
                            2. Multimodal Embedding Models
                                1. Text&Image2Embedding reverse
                                2. Text&Video2Embedding reverse
                                3. Text&Image&Video2Embedding reverse
                        2. By scope
                            1. General Purpose: built using self-supervised learning; can generate (encode) embeddings from orignal data and can generate original data from embeddings (in a probabalistic sense)
                            2. Task-Specific (basically just a normal supervised model repurposed): built using supervised learning and maybe general purpose embedding models; can generate embeddings and use embeddings to perform a task.
                    3. Feature Extraction (Traditional ML)
                        1. Image2Text
                        2. Video2Text
                        3. Traditional ML Tasks (Regression, Classification, Segmentation, etc)
                5. Higher-level Services (act on on top of FM services)
                    1. LLM-specific wrappers
                        1. LLM Gateway (Tools: Kong AI gateway, gateway, LiteLLM, glide, BricksLLM, Paddler, llm-gateway, felafax-gateway): route to most adequate llm (e.g., simple queries go to small llm, as hard ones go to big one), guardrails, standardize llm calls, retries, fallback, telemetry, bypassing rate limits, caching (approximate/semantic caching - Tools: gptcache; exact caching), detect private data that shouldnt be sent to LLM (PII) (Tools: Octopii), prompt compression (Tools: LLMLingua), request batching, human transfer etc. (e.g., use a managed LLM API (e.g. OpenAI GPT) as a fallback in case our own FM is not shure (note: requires that we have a calibrated FM.)) across multiple vendors
                            1. LLM Routers
                                1. Actual Routers (Tools: (1) open source: glide, Unify, semantic-router; (2) proprietary: Martian, Not Diamond): instead of specifying which llm, let the router intelligently decide it for you
                                2. Router Frameworks (Deploy + Evaluate LLM Routers) (Tools: RouteLLM)
                            2. LLM Guardrails
                                1. Input Guardrails (make shure private data isnt leaked to external APIs and defend against adversarial attacks (jailbreaking))
                                2. Output Guardrails (Make shure outputs have certain format, dont leak private info & arent innapropriate)
                                    1. Structure Enforcement: Guidance, promptify, lmql
                                        1. Prompt Structure Enforcement:
                                        2. Output Structure Enforcement: guardrails, outlines, Faster-Outlines, instructor, lm-format-enforcer, Artificial Intelligence Controller Interface (AICI)
                                    2. Evaluation: Open AI evals, Bench, Pheonix, HELM, lm-evaluation-harness, truelens, checklist, guardrails, promptfoo, fiddler-auditor, trulens
                        2. LLM Orchestrators
                            1. Agent-less Orchestrators (Tools: DSPy, Guidance, mirascope): these orchestrators dont provide "sensors & actuators" for your LLM. However they are very usefull because they remove glue code to make multiple calls (with templated prompts & outputs).
                            2. Agent-full Orchestrators (Tools: AutoGen, Llama-index, zep, Langchain, haystack, Semantic Kernel, Dust, IX, MemGPT, BondAI, OpenAgents, SuperAGI, Agent Pilot, griptape, phidata, LLMCompiler, ragapp, CopilotKit, agentkit, gptscript, ten_framework, TaskGen): (the body of your LMM): from LMMs to LLM Agents: orchestrates LLM calls by putting the LLM in a agent framework (LLM as general information processing library that specifies observations it wants to get & actions it wants to take, where our wrapper code is the "sensors and actuators" it needs to be an agent). This involves pipelining LLM calls, feeding LLM-specified observations & performing LLM-specified actions. Actions are the uage of tools (e.g., using google api, using a DB, using a cli tool, etc). Think of te LLM a controller with virtual sensors & actuators (tools: internal actuators: E2B). _Note:_ LLM Service API calls interact only with the LLM Agent Orchestrator and the LLM service itself only serves the LLM Agent Orchestrator.
                                1. Multi-Agent Orchestrators (Tools: MetaGPT, ChatDev, Langroid, crewAI, langgraph, ControlFlow, agent-zero, AgentVerse). The goal of these is to produce semantically different specialist agents that work together to produce achieve some higher-level goal (e.g. produce an entire app).
                                2. No-code Agent-full Orchestrators: LLMStack, Magick, Rivet, Tribe AI
            3. ML-related Security
                1. ML-dependent actions
                    1. Doing actions
                        1. Open-ended ML Outputs (intractable to test entire output space). All systems/components that do actions that are dependent (directly or indirectly) on the output of an open-ended-output ML model (that wasnt reviewed by a human) must carry the information that it is taking an open-ended-ML-dependent action forward, along with its normal ouput, to the next system or component.
                        2. Closed-ended ML Outputs (tractable to test entire output space). All systems/components that do actions that are dependent (directly or indirectly) on the closed-ended-output ML model (that wasnt reviewed by a human) & the output option that was taken has harm potential, must carry the information that it is taking an closed-ended-potentially-harmfull-output action forward, along with its normal ouput, to the next system or component.
                        _Note:_ regression outputs are in practice close-ended because we can bin them.
                    2. Receiving actions: every system/component that can receive actions from an open-ended-output directly or indirectly), must verify if the action was open-ended-ML-dependent or closed-ended-potentially-harmfull-output. If it was one of them: it must restrict its action space according to its rules, for security purposes. If the action is beyond the restrcted action space, then an apprapriate error is sent in response.
    2. MLOps per se
        1. ML Infrastructure
            1. Development
                1. Generalized CI/CD / Workflow Orchestration / Main Pipeline (Tools: Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro; (2) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo/Hera), mleap, metaflow, flyte, sematic, covalent); (3) Generalized CI/CD Policy Enforcement: in-toto, Ortelius
                /
                _Note 1:_ generalized CI/CD is not a common terminology, but I like to think of workflow orchestrators as generalized CI/CD because CI/CD is very clear to understand, so I will be using CI/CD terminology, instead of Workflow Orchestration, throughout the document. 
                /
                _Note 2:_ Lower Levels are outter loops of higher levels. Things at the same level are implemented together. (Should have a GUI to facilitate understanding of whats happening). 
                /
                _Note 3:_ every task workload that is described in this sectio is implemented inside a pod, receive getting input from some storage location (e.g., s3, HDFS/Gluster, etc) and trowing output there also. A task can itself can deploy other pods in it's cluster or another cluster (using pure Kubernetes or a workflow orchestors like Argo/Hera).
                /
                _Note 4:_ this note is for when you are deploying services or tasks with kubernetes/kubeflow, you are working with a monorepo and you are doing integration testing. Only things that are coupled with changed code should be tested! You dont want to keep running tests for code you know wont be afffected. For this you should have a *coupling description/dataflows* of your system.
                /
                _Note 5_ the behaviour of generalized CI/CD will depend on a bunch of config files, these in theory can be stored anywhere, however it is good to store them in the same location, keep them organized (Tools: Apollo, NACOS, Salt, Configu, OmegaConf Hydra, DynaConf) & that they can be retrieved fast by CI/CD (e.g., directly in the repo or in S3).
                /
                _Note 6_ task are proactive & ephemeral, meaning they are started, they fetch data from a storage location and they dissapear when they finish running an particular task (e.g., model evaluation). However, some workloads (e.g., CI/CD)might be triggered a lot and you may want to avoid spawning the task all the time. So you switch from a task to a service pod. A service pod is reactive & always running in a loop serving incoming clients (the whole basis of microservices pattern that is the gold standard nowadays). But the service pod doesnt run here (by the workflow orchestrator in the ephemeral worloads cluster), it runs by kuberntes in the main services cluster.

                _Note 7_ after task I, which was triggered directly or indirectly by event A, is executed; its output status (running, expected output or error) must be reported back to the engineering team via a channel (e.g., email) and logged.

                _Note 8_ For CD: everything is deployed as containers (or lighweight VMs like Kata Containers for workloads with high security requirements), but rebuilding and redeploying containers/pods is a pain in the ass. Therefore, when possible hot reloading is done. Hot reloading is modifying container's files at runtime, which means that you can't modify container-level config (e.g., # of container copies, container resource allocation, etc) but it's way faster and should cover 90% of your needs when making changes.

                2. --
                    1. Repo Dependency scanning & automated PR's (Tools: github/gitlab/gitea (self-hosted) Dependabot)
                        1. Updating to newer versions available
                        2. Detecting Security vulnerabilities
                    2. Levels 
                        1. Level 1: Base ci/cd
                            1. Boilerplates CI/CD
                                1. Networking API CI/CD (Tools: gRPC, Finagle) (Only acts on feature branches) (Triggered by: API specification push; Produces: new commit with API (in all endpoints involved) boilerplate code for the components involved and with code placeholders for handling the api changes)
                                    1. CI: 
                                        1. Build boilerplate code according to the api specification in all endpoints (e.g., client and server)
                                        2. Integrate Boilerplate code into the endpoints
                                    2. CD: Make a push to the feature branch with the result of CI
                            2. Services CI/CD (Tools: Helm, Kustomize, timoni, JuJu, cdk8s, kpt). Runtime, Infrastructure (MLOps platform) & Generalized CI/CD code. (Workflow Orchestrator or Main Pipeline) code. IT Infrastructure & MLOps platform CI/CD (itegrates changes and deploys infrastructure as pods in a cloud cluster) (Tools: (1) DevOps CI/CD like Github Actions, Jenkins, CircleCI, Travis CI, Buddy, Tekton, Bamboo, gocd, woodpecker, Atlantis.
                                1. CI (Tools: Buildbot, pr-agent, Bitbucket Pipelines, AWS CodePipeline)
                                    1. Local Repo Stage:
                                        1. Pre-commit (tools: git pre-commmit hooks) (Note: you ight want to trigger pre-commit hook at every x commits instead of for every commit, to avoid slowing down your dev cycle too much)
                                            1. Package dependencies (see if packages you are using have imcompatible package dependencies)
                                            2. Conversions
                                                1. notebook --> script
                                            3. Static Analysis
                                                1. Linting
                                                2. Formatting
                                                3. Styling
                                            4. Dependecy Injection (making functions/classes more testable by putting their dependencies as arguments and modifying tests to adhere to this)
                                            5. Testing  (make sense to sync it with your main cloud-running CI unit and/or static tests)
                                                1. Local Build
                                                    1. Types of artifacts
                                                        1. Source code
                                                        2. Data
                                                            1. Args
                                                            2. Config
                                                        3. Virtual Environments
                                                    2. What to build
                                                        1. Executables
                                                        2. Libraries
                                                        3. Data
                                                        4. Container Images
                                                2. Actual Testing (Tools: pytest, moto)
                                                    1. Unit tests (Tools: unittest)
                                                    2. Integration tests
                                                        1. Funtion/Class-level integration tests (local)
                                                        1. Container-level integration tests (in a dev cluster).
                                                            1. Local virtual dev cluster (Tools: docker, testcontainers-python, dockertest, kubernetes in docker (kind), minikube, k3d)
                                                            2. Remote dev cluster
                                                                1. Real cluster 
                                                                    1. Go inside a dev environment pod (Tools: devspace, okteto, telepresence, gitpod, Tilt, coder, che, nocalhost, flox)
                                                                    2. Use a proxy service with external endpoint that gives access to the cluster services for any client oustide the cluster (Tools: mirrord)
                                                                2. Virtual cluster inside a namespace (Tools: vcluster)
                                            6. Artifact Substitition (data version control)
                                                1. Store large Artifacts in a remote storage
                                                2. Remove large artifacts from git tracking
                                                3. Put pointers to them in git tracking
                                    2. Remote Repo Stage
                                        1. If triggered by feature Branch Push: 
                                            1. Pull master (with conflict priority to the branch)
                                            2. Build code (Tools: pants)
                                                1. Types of artifacts
                                                    1. Source code
                                                    2. Data
                                                        1. Args
                                                        2. Config
                                                    4. Virtual Environments
                                                2. What to build
                                                   1. Executables
                                                   2. Libraries
                                                   3. Data
                                                   4. Container Images
                                            3. Setup/reconfig testing environment if necessary (Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                            4. Intra-container Tests (Note: 100% Testing coverage (% of code that is being tested) is very important! (Note: after exlcusions are registered))
                                                1. Static code analysis (anti-pattern detection)
                                                    1. Coding Standards (code quality)
                                                        1. Design Patterns (e.g., directory structure)
                                                        2. Linting
                                                    2. Cybersecurity
                                                        1. Software Composition Analysis (SCA)
                                                        2. Static Application Security Testing (SAST) (Tools: scorecard, criticality_score, semgrep, terrascan, nodejsscan, bearer)
                                                    3. Compliance (according to pre-defined design pattern standards)
                                                2. Unit Tests (Note: only run if testing file or dependencies were changed)
                                                    1. Functional Tests
                                                    2. Contract tests: function IO
                                                    3. Smoke tests
                                                    4. Regression tests
                                                3. Funtion/Class-level integration tests (container-level integration tests only later in integraion testing environment)
                                                4. Fine-Grained Performance Profiling
                                                    1. Latency
                                                    2. Throughput
                                                    3. Memory footprint
                                                    4. Networking Usage
                                            5. Setup/reconfig preview environment if necessary (Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                            6. Engineer interacts with his push in the preview environment
                                                1. If nothing wrong: pass
                                                2. Else: retract push with comments on what is wrong
                                        2. Elsif triggered by PR to master branch from a feature branch:
                                            1. Static & Unit Tests
                                                1. Pull master (with conflict priority to the branch)
                                                2. Build
                                                    1. Standalone executables
                                                    2. Container Images (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit))
                                                3. Setup/reconfig testing environment if necessary (Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                4. Tests (Note: 100% Testing coverage (% of code that is being tested) is very important (Note: after exlcusions are registered)!)
                                                    1. Intra-container:
                                                        1. Static code analysis (anti-pattern detection)
                                                            1. Coding Standards (code quality)
                                                                1. Design Patterns (e.g., directory structure)
                                                                2. Linting
                                                            2. Cybersecurity
                                                                1. Software Composition Analysis (SCA)
                                                                2. Static Application Security Testing (SAST)
                                                            3. Compliance (according to pre-defined design pattern standards)
                                                        2. Unit Tests & Function/Class-level Integration Tests (container-level integration tests only later in integraiton testing environment) (Tools: Behave, Lettuce, Robot, Pytest, moto, Unittest, Hypothesis, Coverage)
                                                            1. Standard software tests
                                                                1. Input/Output Data Types
                                                                2. Catch dependecies issues
                                                            2. ML-specific tests
                                                                1. Training Algo smoke tests
                                                                    1. Sanity checks/validations
                                                                        1. Check input and output shapes
                                                                        2. Overfit a single batch
                                                                        3. Guarantee loss function (on the training data) is decreasing in a few epochs
                                                                        4. Guarantee its numerically stable: set reasonable bounds on parameters and % of activation units of a layer being 0
                                                                        5. Run on smaller version of dataset
                                                                        6. Guarantee you know when out-of-memory error would start happening
                                                                    2. Reproducibility: guarantee that if the training algo "a" is given is given data "b" the resultant model is always "c"
                                                                    3. Reliability: crash test the training procedure. The model training should restore from where it left off
                                                        3. Fine-Grained Performance Profiling
                                                            1. Latency
                                                            2. Throughput
                                                            3. Memory footprint
                                                            4. Networking Usage
                                                    2. Container-level: networking API tests (contract tests)
                                                        1. Unit Tests (integration tests only later in CD staging) (Tools: Behave, Lettuce, Robot, Pytest, moto, Unittest, Hypothesis, Coverage)
                                                            1. Contract tests: API IO
                                                        2. Coarse-grained Performance Profiling
                                                            1. Latency
                                                            2. Throughput
                                                            3. Memory footprint
                                                            4. Networking Usage
                                                5. Intrumentation: add production of runtime checks and tests (or just the raw data, so that the ML Monitoring System does the actual verification), logs, metrics (or just the raw data, so that the ML Monitoring System does the actual computation) and traces based on a config file
                                                6. Results
                                                    1. Send Results back to github/gitlab/gitea (self-hosted)
                                                    2. Generate Report
                                                    3. Send Report to Master Admins
                                                    4. Push images to private image registry and packages to package registry
                                                    5. If tests fail: alert programmer who did PR & send the report to him
                                                    6. If tests pass: code review by Master admins
                                                        1. Tests relevance
                                                        2. If tests removed from master regression tests: see if a adequate replacement test was provided
                                                        3. Documentation specification quality
                                                        4. Code quality
                                                            1. Clarity
                                                            2. Simplicity
                                                    6. If code review passes: Update master branch: accept PR on master and generate a new PR on the integration testing branch
                                        3. Else: triggered by PR to release branch from hotfix branch (fixes that need to be done ASAP to releases)
                                            1. Unit Tests
                                                1. Function/Class tests
                                                2. Netoworking API tests
                                            2. Results
                                                1. Send Results back to github/gitlab/gitea (self-hosted)
                                                2. Generate Report
                                                3. Send Report to Master Admins
                                                4. Push images to private image registry and packages to package registry
                                        4. Delete built test artifacts
                                2. CD (triggered by: accepted PR/commit on master) (Tools: ArgoCD, Spinnaker)
                                    1. Documentation
                                        1. Build and/or Verifiy Documentation:
                                            1. Manual: 
                                                1. Some engineer need to check and give approval
                                            2. Automatic: documentation built from source
                                                1. Have documentation linked to code (when code changes, alert is sent to the pieces of documentation linked to the code; if the code doe not change, that piece of doc does not need to be updated) (Tools: OpenAPI-Specification, redoc, pdoc, devdocs, sphinx, Quarto)
                                                2. Use LLMs
                                        2. Deploy documentation (tools: github pages)
                                    2. Pre integration-testing tests
                                        1. Acceptance tests: test if the repo is in the expected format so that CD can be done
                                        2. Image tests
                                            1. Get images from image registry
                                            2. Verify if images and staging cluster are compatible (in terms of compute resources they will need)
                                            3. Inspect images for possible malicous code
                                    3. Deployment to integrated testing cluster
                                        1. Setup/reconfig integrated testing cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane) (should be like production, with the exception of some environment variables (and users) and for containers imitating embedded devices) based on IaC configuration files
                                        2. Build kubernetes manifests via container images, container environment variables and manifest templates
                                        3. Deploy
                                            1. Pods to integrated testing cluster via kubernetes client
                                            2. Config to the configuration manager (e.g., authorization config of a service, that is delivered to the authorization service via the cong manager)
                                    4. Integration Tests in integration testing cluster (Tools: docker, testcontainers-python, dockertest, kubernetes in docker (kind), minikube, k3d) (if tests are not crucial, they can run in background after deploys. Because if you put to many tests here, your dev cycle will get slow. Note: only things that are coupled with changed code should be tested (*coupling description/dataflows* of the ystem is used for this)! You dont want to keep running tests for code you know wont be afffected. Note: Does diff integration testing between specific services of new release (PR to release branch) and previous release (PR to release branch), using production data (Tools: GoReplay))
                                        1. Smoke tests
                                        2. Interface tests
                                            1. APIs and test latency/throuput for all Inference Services
                                            2. UI tests
                                                1. CLIs
                                                2. GUIs (Usually manual, but can be automated, for example, Selenium and cypress automate browser usage, therefore can do automated frontend tests)
                                        3. End-to-end tests
                                        4. Robustness tests
                                            1. Exploratory (trying to break system by testing unusual scenarios) tests.
                                            2. Cybersecurity tests (non-ML related, posible software exploits and attacks)
                                                1. Penetration Testing
                                                    1. Automated: using Dynamic Application Security Testing (DAST) or & Breach & Attack Simulation (BAS) tools
                                                    2. Manual: Manual Penetration Testing
                                                2. Compliance tests
                                                3. Minimum privieleges tests (things should have the leanest privileges possible)
                                            3. Traffic tests (generate a lot of synthetic traffic to see system bahaviour) (Tools: vegeta, locust, gatling, k6, jmeter)
                                                1. Load tests: see if it can handle a large amount of traffic
                                                2. Scalability tests: see if it can handle increasing amount of traffic
                                                3. Soak tests: see if it can handle çarge spikes of traffic
                                                4. Stress tests: see if it can degradate gracefully and recover from to much traffic
                                            4. Fault tolerance tests (make something (machine, node, pod, etc) go down on purpose to hot system handles it)
                                        5. Regression tests
                                        6. Backwards-compatibility tests
                                        7. Observability tests (e.g., "ensure a downstream system malfunction won't cause repetitive logs being stored")
                                    5. Results
                                        1. Send Results back to github/gitlab/gitea (self-hosted)
                                        2. Generate Report (with diffs included)
                                        3. Send Report to Master Admins
                                        4. If tests fail: alert master admins, seind report to them
                                            1. *Break*
                                        5. If tests pass: generate PR to staging branch
                                    6. Deploy to staging cluster (_Note:_ staging cluster should mimic production cluster)
                                        1. Types of testing
                                            1. Offcluster End-to-end testing of new release (PR to release branch)
                                            2. Offcluster Diff End-2-end testing between new release (PR to release branch) and previous release (PR to release branch) using production data (Tools: GoReplay)
                                        2. Steps
                                            1. Setup/reconfig staging cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane) (should be like production, with the exception of some environment variables (and users) and for containers imitating embedded devices) based on IaC configuration files
                                            2. Build kubernetes manifests via container images, container environment variables and manifest templates
                                            3. Deploy pods to staging cluster via kubernetes client
                                    7. Staging result: send report back
                                        1. Failed: dont accept PR and alert programmers
                                        2. Passed:
                                            1. Deploy documentation
                                            2. Accept PR and generate a new PR to release (PR to release branch) branch (Note: release (PR to release branch) needs to be cryptographically signed! so that user can verify that they are not being scammed and are installing the right software)
                                    8. Sanity check that staging and production clusters are the same.
                                    9. Deploy release to production: since stagning and production clusters ar part of a blue-green infrastrcture, deloy is simply pointing the "meta load-balancer" (which load balances to the load balancers of the cluster it is pointed to) to the staging cluster. This means: the staging custer becomes the production cluster, and now the production cluster will becomes the staging cluster, they keep swithicng roles after each production deployment.
                                    10. Send to Monitoring System the realease that is currently in production
                                    11. Trigger background non-crucial tests (will run in parallel to the production system in the background logical cluster (actual cluster or virtual (simulated) cluster))
                                        1. Types of tests
                                            1. Operational Performance (Efficency) tests/function
                                                1. Speed
                                                2. Memory usage
                                                3. Processor usage
                                                4. Disk usage
                                            2. Non-crucial unit & integration tests
                                            3. Long-running tests
                                                1. Large scale tests
                                                2. ML System benchmarking
                                                3. Formal Verification of crucial parts of the codebase
                                                4. Complex Dynamic Security Testing
                                                5. Complex Compliance Testing
                                        2. Steps: same as ''CD (triggered by: new image in private image registry)'' but instead of staging cluster & crucial tests, now you have the background cluster & non-crucial tests. Also, needs to dismantle environment when done doing tests.
                            2. Data Sources CI/CD (schema & data) (Tools: Bytebase, kubeblocks)
                                1. Change data schema (to backwards compatible one) (aka db migrations)
                                    1. CI
                                        1. Test user desired migration file
                                        2. Test backwards compatibility
                                        3. Build new schema from user desired migration file
                                    2. CD: 
                                        1. Final trigger to enable CD is only triggered if documentation (dataset card) is complete (Tools: OpenAPI-Specification, redoc, pdoc, devdocs, sphinx, mkdocs, read the docs, gitbook, Quarto)
                                            1. Manual way: some engineer checks it
                                            2. Automated way:
                                                1. Have documentation linked to data schema (when data schema changes, alert is sent to the pieces of documentation linked to the data schema; if the data schema doe not change, that piece of doc does not need to be updated)
                                                2. Use LLMs
                                        2. Update db with new schema, by requesting schema change to the db server
                                2. Change Data: use-case is usally either improving training data or test data (validates new data and schema changes, then deploys to/as data source (e.g., DB, s3 file, distributed file system, data wharehouse, data lake) (Tools: Pachyderm) (triggered by push to Manual Data Modification Repo (_Note:_ repo verioned with a data verisoning tool (e.g., dvc/fds, xvc, dud, Pachyderm)): featured data branch)
                                    1. Pattern 1: Data is built outside data source
                                        1. CI 
                                            1. Build raw data artifacts
                                                1. Data Validation (as defined by validation file in the respective datasets' directory) (Tools: JSON Schema, Great Expectations, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation)
                                                    1. Update when: validation file in the respective datasets' directory changes
                                                    2. Validations
                                                        1. On:
                                                            1. Data itself
                                                            2. Metadata
                                                        2. Of:
                                                            1. Schema
                                                            2. Descriptive statistics (should not be that diffferent between batches)
                                                            3. Ranges
                                                            4. Contraints
                                                                1. Uniqueness
                                                                2. Dependency between n values
                                                            5. Units
                                                            6. Common problems
                                                            7. Anomaly/outlier detection (e.g., isolation forest)

                                                2. Data processing: processing pipeline to generate raw data that is clean & formated raw data
                                            2. Wrap with data lineage metadata: the processing steps that took the previous raw dataset to the new raw dataset (Tools: OpenLineage)
                                            3. Final trigger to enable CD is only triggered if documentation (dataset card) is complete (Tools: OpenAPI-Specification, redoc, pdoc, devdocs, sphinx, mkdocs, read the docs, gitbook, Quarto)
                                                1. Manual way: some engineer checks it
                                                2. Automated way:
                                                    1. Have documentation linked to data schema (when data schema changes, alert is sent to the pieces of documentation linked to the data schema; if the data schema doe not change, that piece of doc does not need to be updated)
                                                    2. Use LLMs
                                        2. CD:
                                            1. Schema: store newly curated & validated schema in schema store (schema store will identify and store only the patch)
                                            1. Data: store newly curated & validated data in raw dataset store
                                                1. If small: data store will identify and store only the patch
                                                2. If big: datastore will store a new entire dataset
                                    2. Pattern 2: Data is built inside data source via SQL (ELT) (Tools: dbt + piperider)
                                        1. CI
                                            1. Tool-agnostic ELT specification file is validated
                                            2. Tool-agnostic ELT specificatio file is converted tool-dependent specification ELT file
                                        2. CD: ELT file is sent to the Data Source, which perform ELT and stores the bacwards computational patch (when the sequence of patches reaches a big number n, it stores a creates a backup instead)
                                3. Change data state of the whole ML System with another data state (e.g., when you want to replay the ML System , so you need to restore the data state of the ML System to some old data state. You get this old data state via backups + patches. Backups and patches are stored outside the DB itelf.)
                                    1. CI
                                        1. Test user desired data state file and the data sources associated with it
                                        2. Build Data State from user desired data state file
                                    2. CD: Update statefull services with new Data State
                            3. Config CI/CD (from new config to config in the system state store, where this new config update gets automatically validated, versioned and distributed to the subsystems/components) (Tools: IaC tools: Ansible, Terraform/Terragrunt/terramate, , Pulumi, Crossplane, Chef, Puppet; Configuration Management tools: Apollo, NACOS, Salt, Configu, OmegaConf, Hydra, DynaConf) 
                                1. CI
                                    1. Config Validation
                                        1. Format
                                        2. Ranges
                                    2. Final trigger to enable CD is only triggered if documentation (explanation of config) is complete (Tools: OpenAPI-Specification, redoc, pdoc, devdocs, sphinx, mkdocs, read the docs, gitbook, Quarto)
                                        1. Manual way: some engineer checks it
                                        2. Automated way:
                                            1. Have documentation linked to config schema (when config schema changes, alert is sent to the pieces of documentation linked to the config schema; if the config schema does not change, that piece of doc does not need to be updated)
                                            2. Use LLMs
                                2. CD: send new config to the system state store
                        2. Level 2:
                            1. Pipelines CI/CD
                                1. Pipeline Export CI/CD (converting from pipeline using tool A (e.g., scikit learn) to standard tool-agnostic format)
                                    1. CI
                                        1. Static Analysis
                                        2. Unit Tests
                                        3. Optimization
                                    2. CD: store tool-agnostic pipeline in data pipeline/graph registry
                                2. Pipeline Deploy CI/CD (from new data piepline in data pipeline/graph registry to deployed pipeline)
                                    1. CI (triggered by: new data piepline in data pipeline/graph registry)
                                        1. Get pipeline file from data pipeline/graph registry and depedendencies
                                        2. Test Data Pipeline according as defined by the testing routines inside pipelne file. If [passed/did not pass]: send to Pipeline/graph registry: "Pipeline <Pipeline ID, Push ID> [passed/did not pass] tests"
                                        3. Options of deploy
                                            1. 
                                                1. Update/Add entire production pipeline: ideal for heavy changes
                                                2. Do not alter/add production pipeline, add a computational patch directly in the target store (e.g. Feature Store): ideal for light changes
                                            2. 
                                                1. Backfill past datapoints
                                                2. Not backfill past datapoints (but in this case, the target store should provide a warning for clients)
                                        4. Options of deploy targets
                                            1. Directly to distributed processing tool (e.g., spark/koalas)
                                            2. To just Workflow Orchestrator (e.g., Pachyderm)
                                                1. Package as a set of container images (one for each task)
                                                2. Store images in private image registry linked together
                                            3. To Workflow Orchestrator making use of distributed processing tool inside tasks (e.g., sparl being used within kubeflow pipelines)
                                                1. Export pipeline to the internal pipeline format of the distributed processing tool
                                                2. Package as a set of container images (one for each task)
                                                3. Store images in private image registry linked together
                                            4. As a single service
                                                1. Package as a container image
                                                2. Store imag ein private image registry
                                        5. Export tool-agnostic pipeline to production tool format (e.g., to spark, polars or sql) (Tools: ibis)
                                        6. Final trigger to enable Production CD is only triggered if documentation (pipeline/DAG card) is complete (Tools: OpenAPI-Specification, redoc, pdoc, devdocs, sphinx, mkdocs, read the docs, gitbook, Quarto)
                                            1. Manual way: some engineer checks it
                                            2. Automated way:
                                                1. Have documentation linked to code (when code changes, alert is sent to the pieces of documentation linked to the code; if the code doe not change, that piece of doc does not need to be updated)
                                                2. Use LLMs
                                    2. CD
                                        1. Integration Testing CD
                                            1. Setup/reconfig integration testing cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                            2. Deploy to integration testing cluster
                                            3. End-to-end test it with production-like load & traffic
                                            4. Generate CI/CD Report (in easily parsable format (e.g., yaml, CUE) and rendering format (e.g., svg or pdf) and put in PR Comment
                                            5. Integration testing result
                                                1. If passed:
                                                    1. Send alert to human (via slack, email or dedicated alerts manager like: keep)/log with CI/CD report
                                                    2. Send image to private image registry saying ''status: ready for staging'' along with integration testing cluster configurations used
                                                    3. Triggers ''pipeline ready for staging'' to staging CD
                                                2. If failed: send alert to human (via slack, email)/log with CI/CD report
                                            6. Dismantle staging cluster if necessary (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                        2. Staging CD
                                            1. Setup/reconfig staging cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                            2. Deploy to staging cluster
                                            3. End-to-end test it with production-like load & traffic
                                            4. Generate CI/CD Report (in easily parsable format (e.g., yaml, CUE) and rendering format (e.g., svg or pdf) and put in PR Comment
                                            5. Staging result
                                                1. If passed:
                                                    1. Send alert to human (via slack, email)/log with CI/CD report
                                                    2. Send image to private image registry saying ''status: ready for production'' along with staging cluster configurations used
                                                    3. Triggers ''pipeline ready for production'' to production CD
                                                2. If failed: send alert to human (via slack, email or dedicated alerts manager like: keep)/log with CI/CD report
                                            6. Dismantle staging cluster if necessary (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                            7. Final trigger to enable Production CD is only triggered if documentation is complete (Tools: OpenAPI-Specification, redoc, pdoc, devdocs, sphinx, mkdocs, read the docs, gitbook, Quarto)
                                                1. Manual way: some engineer checks it
                                                2. Automated way:
                                                    1. Have documentation linked to code (when code changes, alert is sent to the pieces of documentation linked to the code; if the code doe not change, that piece of doc does not need to be updated)
                                                    2. Use LLMs
                                        3. Production CD
                                            1. Production CD listens to ''pipelines ready for production'' trigger  along with staging cluster configurations used
                                            2. Verify if production cluster is compatible with staging cluster (especially, if the complementary Inference Services are in place)
                                            3. Deploy pipelines to production cluster using workflow orchestrator/distributed processing tool client. _Note:_ need to tell the workflow orchestrator/distributed processing tool from where to start the new processing (if batch mode this is just changing the iteration number, but if it is streaming, you need to pass a timestamp so that it can fetch old messages from the message transport). 
                            2. Model CI/CD (Tools: ML Workflow orchestrators: )
                                1. ML Training CI/CD (From training procedure specification to Model/Prompt in Model/Prompt Registry) (Batch Offline Learning, either experimental or retraining)
                                    1. For prompts:
                                        1. If doing prompt learning: very similar to "For models" but instead of building a model, you will build a prompt
                                        2. Else: just do some basic processing to build the prompt & store it in the Model/Prompt Registry
                                    2. For models: (Tools: Colossal AI, DeepSpeed, HF Optimum, HF Accelerate, MosaiCML Composer, TF Distributed Training, Pytorch Distributeed Training, spark/koalas MLib, Ray Train, Dask Distributed Training, Kubeflow Training Operator)
                                        1. If building your own task-specific model
                                            1. CI (triggered by: new training procedure specification)
                                                1. Cluster setup: Setup/reconfig Manual Ephemeral workloads cluster if necessary (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                2. Build
                                                    1. Application
                                                        1. Build training code using information from training prcedure definition. (_Note:_ if the model is too big to fit into a single machine's memory: do distributed training.)
                                                    2. Container Wrapper
                                                        1. Single Node
                                                            1. Build training image: (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) image with training code and dependencies inside (e.g., dataset, config))
                                                            2. Send image to private training image registry with container runtime requirements as metadata (e.g., gpu, x amount of storage, y amount of memory)
                                                        2. Distributed
                                                            1. Build training images: (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) image with training code and dependencies inside (e.g., dataset, config))
                                                            2. Send images to private training image registry with container runtime requirements as metadata (e.g., gpu, x amount of storage, y amount of memory), as a single distributed application 
                                                4. Training: Deploy Distributed Training Manager (Note: single machine with multiple GPUs already is considered distributed training). Distributed Training manager is repsonsible for distributed training via deploying parallel training workloads (Tools: (1) Distributed Training tools: (1.1) Training: Horovod,  Distributed Data Parallel (DDP), Analytics Zoo, MosaiCML Composer, TF Distributed Training, Pytorch Distributeed Training, spark/koalas MLib, Dask Distributed Training, Petastorm, (1.2) Tuning: nni, hyperopt, optuna, raytune, Katib; (2) Task Queues: Celery, RQ, kueue). 
                                                    1. Types of distributed training
                                                        1. Core-training purposes
                                                            1. Data-parallel: each workload uses the same model but trains on different subsets of the dataset. The gradient are avaraged (at every n batches) and each workload uses the avaraged gradient.
                                                            2. Model-parallel: each workload receives part of the model, receives as input the output of the previous part, its output goes as input to the next part. The workload with the first and last parts are the ones that actually deal directly with the dataset.
                                                        2. Tuning purposes
                                                            1. Hyperparameter Tuning: each workload receives a space of hyperparameters they can operate on. (Note: computation can become quit heavy if using full dataset, therefore you can get representative subsets of it to do hyperparameter optimization)
                                                                1. Search for Optimal Hyperparameters
                                                                    1. Hard Hyperparameters: hyperparameters that define a model family (What type of architecture (what type of computations will we enable our learning algorithm to explore (architecture is the "hardware of our model"). Can depend on: ML task, size of dataset, efficency requirements and available hardware)) (These are typical treated as fixed in most applications, because people use model families that are mature and well known)
                                                                        1. Intuition & Heuristics
                                                                        2. Neural Architectural Search (NAS)
                                                                    2. Soft Hyperparameters: hyperparameters that define or bias towards a set of possible functions within a model family (e.g., for Feed Forward NNs: model hyperparameters (layer size, number of layers, activation function, etc), optimizer hyperparameters (e.g., optimizer method, parameter initilization, learning rate, decay, etc) regularization hyperparameters (e.g., regularization method, amount of regularization, etc))
                                                                        1. Heuristics & Brute-Force Search
                                                                        2. Bayesian Optimization
                                                                        3. Genetic algorithms
                                                                        4. Neural Architectural Search (NAS)
                                                                2. Sensitivity analysis (how robust our metrics are wrt changes in oir final (full-dataset) hyperparameters, aka the variance of our metric wrt hyperparameters. Ideally low variance)
                                                            2. Dataset Tuning: each workload make use of a diffrent dataset version.
                                                                1. Data-efficient fine-tuning: How to add relevant datapoints to the dataset and improve the model without: (1) having to do training over all the entire dataset again and (2) overfitting to the new datapoints?
                                                    2. Training Workload (Tools: Workflw Orchestrators such as Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro; (2) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo), mleap, metaflow, flyte, sematic;). _Note:_ But how to know if we can deploy another workload to the cluster? (i.e. if the clsuter's resources are already being fully consumed by the current workloads running on it). The workflow orchestrator (actually, any distributed processing tool, e.g., spark/koalas) should provide the current state of the cluster's resource conumption upon request (which they usually get from k8s or Istio/Kiali/linkerd2/Cilium/Consul API). A great workflow orchestrator tool can: (1) estimate the (avg or max) resource consuption increase if you where to deploy this workload. Normally, resoure consumption would have to be < 100% so that you can avoid speedowns due to hardware bottleneck; but if you are ok with speeddowns: then you need a time-based matric of multiple standard program running on the cluster + a max treshold time this program should be runnning to determine when you cannot deploy more workloads (because time of avg time of standard programs has reached your max treshold); (2) autoscaling: expand and retract your worker nodes when cluster resources are full (under the hood its probably using an IaC tool such as Terraform/Terragrunt/terramate, ANsible or Pulumi, Crossplane to do these cluster setups & then updating kubernetes to tell it about the new cluster setup).
                                                        1. Train (Managed Tools: Major Cloud Vendors, run:ai). deploy training job using kubeflow cli (that will make a request for the master node whcih triggers kubectl to pull images from pivate (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools:, NVIDIA Container Toolkit)) registry) that deploys pods on the cluster and deployment configuration files
                                                            1. If first run or manually requested: run only small subset of entire dataset (because there are high chances of bugs)
                                                            2. Else: run with entire dataset
                                                                1. Needs to keep sending operational (resource consumption) & training data (at each epoch) to Monitoring System, so that ML Engineers can monitor training
                                                                2. Needs to stop training if requested, and either send the model to Model/Prompt Registry or not
                                                        2. After training (naturally ended or forced stop): send run (from start of training to some stoppage) with [resultant model, Version ID, model type, loss curves, hyperparameters, metrics, type of stoppage, other metadata (e.g., person did the run)] to Experiment tracking system
                                                5. Debugging: change training procedure specification or ML training CI/CD code & Redeploy modified training workload until works as expected
                                                6. Dismantle enviroment if necessary (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                            2. CD (triggered by: training completed without bugs): send model with specific format (serialized like Pickle (not language agnostic), ONNX (most used), GGML) along with model-related data like: ID, model version, type of deploy, type of retraing, evaluation results, dependencies (models: pretrained third-party or internal, data: third-party or internal) & scaling laws; to the Model/Prompt Registry. Obs: the dependecies shoud be very specific (e.g., for the training data, it should say that it used the namspace "a" of the festure store at the version "b" of the feature store, and extracted datapoints [x-y])
                                        2. If Fine-Tuning LLM: (Tools: LLaMA-Factory, LMFlow, Simple LLM Finetuner, Axolotl, unsloth, langdrive, *xTuring*)
                                            1. Use LLM API (within Foundation Models API) to do fine-tuning with your dataset & store API endpoint of your fine-tuned model. _Note_: how to do traditioonal Supervised Learning (when y is not natively text) fine-tuning on LLMs?
                                                1. Methods
                                                    1. Black-box
                                                        1. Last Layers Fine-Tuning: \# of parameters constant. A nice method is "Formatted Training". (Formated Training: making LLMs substitute task-specific ML models. Restricts output to follow a certain format. In practice, what happens is that at each time step, when before any token could be the prediction, what happens is that only a few tokens can be predicted; the probabilities of the allowed tokens are normalized and then you sample just like before. This method makes it possible to train an LLM on any supervised task reliably. Note: the advantage of using an LLM is that you can insert prior knowledge of the task as text to help the model converge to the right place.)
                                                        2. Scaling Fine-Tuning: increasing \# of parameters. Freeze all parameters, then add units to each layer, and learn the new parameters.
                                                    2. White-box
                                                        1. Custom Fine-Tuning. White-box & increasing \# of parameters. Call prediction on the LLM API (within Foundation Models API) and instead of getting prediction results, get last latent layer and use it as input to your custom fine-tuning model.
                                                2. Steps
                                                    1. Cluster setup: Setup/reconfig integrated testing cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                    2. Build
                                                        1. Application
                                                            1. Build fine-tuning code
                                                        2. Container Wrapper
                                                            1. Build training image: (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) image with training code and dependencies inside (e.g., dataset, configs (might have a config that give info required to get batches of datapoints from the feature store)))
                                                            2. Send image to private training image registry
                                                    3. Tests (using LLM API (within Foundation Models API)) (if dont pass: send to (generally stream) Monitoring service and/or alert (via slack, email or or dedicated alerts manager like: keep) and/or log)
                                                        1. Standard software tests
                                                            1. Input/Output Data Types
                                                        2. Training Algo tests
                                                            1. Sanity checks/validations
                                                                1. Run on smaller version of dataset
                                                            2. Reliability: crash test the training procedure. The model training should restore from where it left off
                                                    4. Training: Deploy Training Workload (Tools: Workflw Orchestrators such as Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro; (2) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo), mleap, metaflow, flyte, sematic;)
                                                        1. Train (Tools: Major Cloud Vendors, run:ai, Runpod). deploy training job using kubectl (pulling images from pivate (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools:, NVIDIA Container Toolkit)) registry) that deploys pods on the cluster and deployment configuration files
                                                            1. If first run or manually requested: run only small subset of entire dataset (because there are high chances of bugs)
                                                            2. Else: run with entire dataset
                                                        2. After training (naturally ended or forced stop): send run (from start of training to some stoppage) with [resultant model, Version ID, model type, API endpoints, metrics, type of stoppage, other metadata (e.g., person did the run)] to Experiment tracking system
                                            2. If you only care about a specific task (or subsets of tasks) and/or want to lower latency: distill needed knowledge into a smaller model
                                                1. Build proxy dataset (a bunch (way more than original dataset) of (x, y_hat) tuples of the task(s) you are interested in, with y_hat being obtained from output of the fine-tuned LLM) by probing our fine-tuned LLM using the inference API endpoint we stored before.
                                                    1. Cluster setup: Setup/reconfig integrated testing cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                    2. Build
                                                        1. Application
                                                            1. Build proxy-dataset building code
                                                        2. Container Wrapper
                                                            1. Build training image: (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) image with training code and dependencies inside (e.g., dataset, config))
                                                            2. Send image to a private image registry & store path to it somewhere
                                                    3. Tests (using LLM API (within Foundation Models API)) (if dont pass: send to (generally stream) Monitoring service and/or alert (via slack, email or or dedicated alerts manager like: keep) and/or log)
                                                        1. Standard software tests
                                                            1. Input/Output Data Types
                                                    4. Proxy-dataset building: Deploy the workload (Tools: Workflw Orchestrators such as Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro; (2) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo), mleap, metaflow, flyte, sematic;) that will store the resultant proxy dataset in the Feature Store.
                                                2. Knowledge Distillation: distill knowledge from LLM to smaller model by training smaller model with proxy dataset. Same procedures as before when we were trianing our own task-specific model.
                                                    1. Cluster setup: Setup/reconfig integrated testing cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                    2. Build
                                                        1. Application
                                                            1. Build fine-tuning code
                                                        2. Container Wrapper
                                                            1. Build training image: (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) image with training code and dependencies inside (e.g., dataset, configs (might have a config that give info required to get batches of datapoints from the feature store)))
                                                            2. Send image to private training image registry
                                                    3. Tests (using LLM API (within Foundation Models API)) (if dont pass: send to (generally stream) Monitoring service and/or alert (via slack, email or dedicated alerts manager like: keep) and/or log)
                                                        1. Standard software tests
                                                            1. Input/Output Data Types
                                                        2. Training Algo tests
                                                            1. Sanity checks/validations
                                                                1. Run on smaller version of dataset
                                                            2. Reliability: crash test the training procedure. The model training should restore from where it left off
                                                    4. Training: Deploy Training Workload (Tools: Workflw Orchestrators such as Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro; (2) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo), mleap, metaflow, flyte, sematic;)
                                                        1. Train (Tools: Major Cloud Vendors, run:ai, Runpod). deploy training job using kubectl (pulling images from pivate (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) registry) that deploys pods on the cluster and deployment configuration files
                                                            1. If first run or manually requested: run only small subset of entire dataset (because there are high chances of bugs)
                                                            2. Else: run with entire dataset
                                                        2. After training (naturally ended or forced stop): send run (from start of training to some stoppage) with [resultant model, Version ID, model type, API endpoints, metrics, type of stoppage, other metadata (e.g., person did the run)] to Experiment tracking system
                                2. Direct Model Push CI/CD (Triggered by push of a model along with its metadata) (Only used if declarative ML training via config file is not sufficient to do some custom model building stuff)
                                    1. CI: 
                                        1. Ensure model + metadata are in appropriate format
                                        2. Ensure model runs
                                    2. CD: push model + metadata to Model/Prompt Registry
                                3. ML Inference Service CI/CD (from new model/prompt/filter/fallback + deployment config to evalauted & deployed Inference Service/deployed filter service/deployed fallback on all consumers, respectively) (can trigger level 1). _Note:_ Model/Prompt Registry + ML Inference Service CI/CD is called by some people a _Model System_ (but I dont like this way of thinking, prefer to put Model/Prompt Registry inside artifact store & ML prediction ci/cd inside workflow orchestration (generalized ci/cd)).
                                    1. Modes of operation
                                        1. Batch Offline Learning
                                            1. Experimentation
                                            2. Retraining
                                        2. Online Learning (Tools: Volwpal Wabbit, River, scikit-multiflow)
                                    2. CI (triggered by: new model/prompt/filter/fallback in the Model/Prompt Registry)
                                        1. If new prompt:
                                            1. First steps
                                                1. Cluster setup: Setup/reconfig production online ephemeral workloads cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                2. Get prompts & related data from Model/Prompt Registry
                                                    1. New prompt
                                                    2. Current production prompt
                                                3. Evaluation Methodology: evaluate new candidate prompt & current production prompt together to avoid regressions
                                            2. Prompt Compression
                                                1. Substitute words
                                                    1. For their abreviations
                                                    2. For shorter synonyms
                                                2. Throwing away random letters
                                                3. Summarization
                                            2. Deploy Automatic Prompt Evaluation Workloads (Using Workflow Orchestrator) (Note: shoudl support evaluating chains of prompt also)
                                                1. LLM-specific evaluations (Tools: OpenAI evals, HELM, lm-evaluation-harness, truelens, checklist, guardrails, promptfoo)
                                                    1. Validation against templates & regex
                                                    2. Predictive power: measuring distance of outputs wrt test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) targets
                                                        1. Via distance between embeddings
                                                        2. Via another LLM
                                                    2. Evaluation of prompt and output innapropriateness (e.g toxicity, trolling, private data being exposed, intellectual property being robbed)
                                                    3. Sensitivity Analysis (sensitivity of output quality & size/price)
                                                        1. Prompt changes (how output changes if prompt changes a little)
                                                        2. LLM changes (How output changes if LLM is changed or the same LLM but with different config)
                                                    4. Prompt injection (Tools: rebuff)
                                                        1. Integraty attacks
                                                        2. Privacy attacks
                                                        3. Availability attacks (for agents)
                                                    5. Output size/cost
                                                    6. RAG-specifc
                                                        1. Retrieval Power (how well does the retriver of a RAG retrive the adequate set of docs for user inputs)
                                                        2. Embedding Power (how well does the distance metric betwen embeddings reflect the actual semantic similarity between docs)
                                                    7. Steerability (evalauting prompt importance models, aka feature importance for LLMs, the goals is to estimate how the LLM can be steered, which type of prompt make it change the output in a certain way)
                                                    8. Stochasticity: make a bunch of llm calls with the same prompt, and see measure output variability
                                                2. Standard Model evaluations (explained in "Automatic Model Offline Evaluation")
                                        2. Elsif model:
                                            1. First steps
                                                1. Cluster setup: Setup/reconfig production online ephemeral workloads cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                2. Get models & related data from Model/Prompt Registry
                                                    1. New model
                                                    2. Current model
                                                3. Define origin of new model: experimentation or retraining (what was the process that generated it) (_Note:_ generally you will want your evaluation suite for model originated from retraining to be much leaner & faster than your evaluation suite for models originated from experimentation; because when retraining: you already did extensive evaluation before, you are not making major changes to the model & cannot afford to suffer too much from model staless due to Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s)
                                                4. Evaluation Methodology: evaluate new candidate model & current production model together to avoid regressions
                                                5. Checks (validations):
                                                    1. Model size
                                                    2. Model format
                                                    3. Model operations (need to be supported by runtime)
                                                    4. Model Computing Hardware Requirements for acceptable performance
                                            2. Deploy Automatic Model Offline Evaluation Workloads (Tools:HF Evaluate, TF Model Analysis, TF Responsible AI Toolkit, Microsoft Responsible AI Toolkit (RAI), Giskard, Learning Interpretability Tool (LIT), PiML, ZenoML, Evidently, AI Verify) (need to set treshold for these) (obs: current model mirrors evaluation procedures of new model) (Note: should support evaluation of chains of models also)
                                                1. Steps
                                                    1. Corrected predictive power (will get the featured dataset from Experiment Tracking (which will point to the datapoints on the feature store that compose training, cv and test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)))
                                                        1. Overall corrected predictive power
                                                            1. Label metrics
                                                                1. Empirircal (using test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)s. _Note:_ tests sets shouldnt be made by drawing random datapoints, they shloud be the last datapoints (indexed by time) available, because then you factor in distribution shifts)
                                                                    1. Types of metrics (computed on test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) that we retrieved from feature store)
                                                                        1. Acc
                                                                        2. Recall
                                                                        3. ROC Curve
                                                                        4. F1-Score
                                                                        5. MSE
                                                                        6. Mean average precision
                                                                        7. Distribution Error
                                                                        8. Cohen’s k
                                                                        9. ...
                                                                    2. If classification: verify corrected predictive power for each class
                                                                    3. Test set construction
                                                                        1. Typical case
                                                                            1. Statistically comparable sets (or Balanced split)
                                                                                1. If small dataset: typically people do random sampling but stratified sampling is better because you can ensure with more confidence statistically comparable sets. E.g., If classification: aim to have same label distribution across splits.
                                                                                2. If larger dataset: random split will very likely make a balanced split, no worries
                                                                            2. Sequential data: splits have to follow sequence (e.g., time series data)
                                                                        2. Worst-case Analysis
                                                                            1. Hard cases: manually curate the hardest test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) you can think of, only using the data you have (not artifically creating more data) (One approach is to get datapoints of minority groups, another is to get datapoints near decision boundaries)
                                                                            2. OOD Cases: curate an ou-of-distribution test set that will evaluate how much the model can extrapolate/how much it is overfitting the datset
                                                                        3. Balanced Analysis: split train/test with same percentage of hard examples (ones were model made mistake, preferebly large mistakes)
                                                                2. Theoretical (probabilistic, that uses performance on training set and model complexity)
                                                                    1. AIC (Akaike Information Criterion) from frequentist probability
                                                                    2. BIC (Bayesian Information Criterion) from bayesian probability
                                                            2. Generative metrics
                                                                1. Image metrics
                                                                    1. Groud-truth-independent (there isnt only one right output image)
                                                                        1. Inception Score (IS)
                                                                        2. Frenchet Inception Distance (FID)
                                                                    2. Groud-truth-dependent metrics (only one right output image)
                                                                        1. Compare representions
                                                                            1. Image embedding
                                                                            2. NN layer
                                                                                1. Learned Perceptual Image Patch Similarity (LPIPS)
                                                                        2. Structured Similarity Index Metric (SSIM, MSSIM)
                                                                        3. Peak Signal-to-Noise Ratio (PSNR)
                                                                2. Text metrics
                                                                    1. Groud-truth-independent (there isnt only one right output text)
                                                                        1. Diversity
                                                                            1. Self-BLEU
                                                                        2. Toxicity
                                                                        3. Hallucination (Tools: selfcheckgpt)
                                                                    2. Groud-truth-dependent (only one right text)
                                                                        1. Compare representions
                                                                            1. Embeddings
                                                                                1. Word Embeddings
                                                                                    1. Word Mover's Distance (WMD)
                                                                                2. Text Embeddings
                                                                            2. NN layer
                                                                                1. BERTScore
                                                                        2. ROUGE
                                                                        3. BLEU
                                                                        4. METEOR
                                                            3. Custom metrics
                                                                1. Resilience to Data Distribution shift (Note: sudden shifts are often indicative of data bugs) (Domain Adaptation): when production will have slighlty different distribution than training. Get a good test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) that resembles production cluster and make cv statiscally equivalent to test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant).
                                                                2. Custom metric depedent on (The loss function should have accounted for these priorities):
                                                                    1. Protected/High priority types of inputs
                                                                    2. Protected/High priority types of errors
                                                        2. On subsets/groups of feature space. Reasons:
                                                            1. Fairness: compare performance on different values of protected features  (Tools: Microsoft Responsable AI (RAI), Tensorflow Responsible AI Toolkit, LiFT, Tensorflow Model Analysis (TFMA) from TFX, fairlearn, AI Fairness 360 (AIF360))
                                                            2. Monitoring: Defining groups (the more homeogenous corrected predictive power the better because makes predictive power estimator have less variance in Monitoring), putting this info in group columns of datapoints in Feature Store. This info will be further important to the Monitoring System which can use these approaches:
                                                                1. Stratified sampling of featured datapaoints & delayed labels in Monitoring (taking advantage of groups with similar predictive power)
                                                                2. No-label corrected predictive power Monitoring (Inference datapoints are binned into one of the subgroups with known corrected predictive power, and then expected value of deploy predictive power can be calculated)
                                                            3. Error profiling: making error buckets, slices of the dataset the model has the largest errors (usually the slices where it has less data)
                                                    2. If origin==experimentation: evaluate resource consumption (because changing models parameters doesnt change resource consumption in general)
                                                        1. Speed
                                                            1. Training: total time
                                                            2. Inference: total time
                                                        2. Hardware usage/function
                                                            2. Memory
                                                            3. Networking
                                                    3. Bias
                                                        1. Through Causality: Evaluating causal effect of protected feature on target
                                                        2. Throgh Feature Importance: evaluating importance of protected features in predictions
                                                        3. Through Heuristics: running some invariance tests manually curated/templated
                                                    4. Innapropriate Outputs (Tools: (Tools: Open AI evals, Bench, Pheonix, HELM, lm-evaluation-harness, truelens, checklist, guardrails, promptfoo, fiddler-auditor, trulens))
                                                        1. Toxic Outputs
                                                        2. Outputs that expose private data
                                                        3. Outputs that violates IP/Licence Compliance
                                                    5. Data Distribution shift (Note: sudden shifts are often indicative of data bugs): shift of test-set wrt prodution data (you can easily detect the shift, but fixing it can be pretty hard, since, in theory you are already training with the most recent data. So a bad result here shows that you training job is too slow.)
                                                2. Results
                                                    1. If passed: Active Learning. Model requests some type of data he wants to be trained on (becuase currently he is not shure about this type of data)
                                                        1. Generate n datapoints the system would like labels to
                                                        2. Send these data collection requests to the Active Learning System
                                                        3. Send alert (via slack, email or dedicated alerts manager like: keep) to Human manually evaluate important things that cannot be yet automated
                                                    2. If failed:
                                                        1. Send alert (via slack, email or dedicated alerts manager like: keep) to Human & log it
                                                        2. Experimentation: Model debugging
                                            3. Dismantle enviroment if necessary (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                            4. Notify Engineer that automatic model offiline evaluation was done and the satus
                                                1. If passed: waits for model offline evaluation result provided by the engineer on the model evaluation environment
                                                2. If not passed: engineer debugs training code and does another push
                                    3. CD (triggered by: new evaluated model/prompt/filter/fallaback in Model/Prompt Registry) (Tools: 
                                        (1) ML Deploy tools like: Seldon Core, bentoML, Ray Serve, KServe, MLflow Models, Cortex, Truss, MLEM; 
                                        (2) LLM-specific deploy tools like: 
                                            (2.1) Cloud: 
                                                (2.1.1) Higher-level: Superagent, OpenLLM, Skypilot, substratus, lanarky, lamini, Xorbits Inference, ChatGPT, Reverse Proxy, dialog, aphrodite-engine, KubeAI, leptonai; 
                                                (2.1.2) Lower-level: MLC LLM, TensorRT-LLM, Intel Extension for Transformers, ipex-llm, ray-llm, Text Generation Inference (TGI), vLLM, lmdeploy, WebLLM, PowerInfer, NVIDIA Triton Inference Server with FastTransformer backend, FlexFlow, mistral.rs, lightllm, sglang, lorax; 
                                            (2.2) Edge: 
                                                (2.2.1) Just LLM: Open Interpreter, transformers.js, llama-cpp-python, GPT4All, LocalAI, Ollama, ExLlamaV2, Dalai, BigDL-LLM, ExLlamaV2, koboldcpp, Xorbits Inference, danielmiessler/fabric, torchchat, TinyLLM, unsloth, xTuring 
                                                (2.2.2) RAG/Agents: TinyAgent, Jan, khoj, UFO, llama-agent, JARVIS, gptme; 
                                                    (2.2.2.1) Can take screenrecord as input: screenpipe
                                        (3) Embedding model-specific: Text Embeddings Inference, infinity)
                                        1. Staging CD (triggered by: new best evaluated model in Model/Prompt Registry)
                                            1. Staging Cluster setup: Setup/reconfig staging cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                            2. If new model
                                                1. Model validations
                                                    1. Get Model
                                                    2. Validate metadata
                                                    3. Validate metrics
                                                2. Model Optimizations
                                                    1. Model Compression: High Level Optimizations (Tools: (1) General: Hugging Face Optimum, TF Model Optimization Toolkit, TF Lite, Intel Neural Compressor, Adlik, distiller, sparsify, sparseML, AIMET, TinyNeuralNetwork; (2) LLM-specific: Intel Extension for Transformers (_Note:_ included inside HF Optimum), llm-compressor, llm-awq, ). _Note:_ Hardware Dependent optimizations are done at the Compilation phase by ML Compilation tools (e.g., TVM, HF Optimum, XLA, voltaML (lib wrapper over multiple compilers/runtimes), Adlik, AITemplate, Hidet, iree).
                                                        1. Runtime/Hardware-unaware methods 
                                                            1. Sparsification (If NNs: Sparsification == Pruning) (Tools: sparsify, onnxSlim) + Compiler/Runtime that takes advatage of Sparsity (Tools: deepsparse) (setting to 0 some parameters, then model can leverage this sparsisity when doing computation) (''Most weights can be removed after training is finished (while only losing a few % in test-set accuracy!'')
                                                            2. Symbolic Search: search for symbolic functions that capture most of the model but are way smaller and more interpretable.
                                                        2. Runtime/Hardware-aware methods
                                                            1. Quantization (try to save memory as much as possible (e.g., float32 --> float16, this can go until binary ( but of course, acc will have severe damage with too much quantization)) (Tools: (1) General: HAWQ, quanto; (2) LLM-specific: AWQ, AutoGPTQ, SmoothQuant, VPTQ; (3) Hardware-specific: (1) Intel: bigdl-llm; (2) NVIDIA: )
                                                            2. Knowledge Distillation (Teacher Model (original model, lots of parameters) trains Student Model (less parameters)) (''E.g., DistillBERT, reduces size of BERT by 40%, and increases inference speed by 60%, while retaining 97% language understanding'')
                                                            3. Hardware-aware Neural Architecture Search (HANAS) (Tools: TVM does this under the hood) conditioned on an existing good (& too big) model. _Note:_ most of the time when NAS is talked about, is in the context of either (1) in the AutoML sense: automating the core modeling process, i.e., building models with high predictive power; or (2) in the SOTA sense: too find better architectures, enalbing gaining a bit of predictive power. However, here the use is to find a smaller architecture that has good predictive power aswell. But isnt this just pruning? Well, yes, pruning shpuld solve a lot of your problems, but there might be cases where you can find some sneaky operators that are just the inductive biase you need for the problem (e.g., attention in transformers)
                                                            4. Operation Fusion (some hardwares or runtimes have perform natively some higher level operations, these native implementations than can be leveraged)
                                                    2. Model Calibration (Making the models output probabilities actually reflect its uncertainty, then if the model is not shure we can make the decision of not using it)
                                                3. Actual Model Deploy
                                                    1. Onine Experimentation (Tools: (1) Open source: ML deploy tools: Seldon Core, BentoML, Ray Serve, KServe, MLflow Models, Cortex, Truss, MLEM; DevOps tools: Argo Rollouts, flagger, Posthog, Growthbook, Flipt, Flagr; (2) Managed: (2.1) Paid: Optimizely, AB Tasty, Apptimize; (2.2) Free: Google Optimize) (two Inference Services for the same task run in production, with traffic routing rules chosing which one. The traffic routing rules sit inside a proxy: either the API gateway or the prediction mnager. Note: the proxy should put info on the online experiementation in the response)
                                                        1. A/b tests or multi-armed bandits (Note: if a/b testing a model improvement: "that means for 90% of the traffic the model serves the same or a similar result. Isolate the changed part of the traffic in the treatment and control group and only compare those. Otherwise you have so much noise you often won't see a difference")
                                                        2. Canary deployment
                                                        3. Shadow deployment
                                                    2. Core Deployment (according to deployment config)
                                                        1. Option 1: each model gets its own Inference Service. Need to re-deploy Inference Service when model changes.
                                                            1. Inference Service (Tools: (1) For NVIDIA GPUSs: NVIDIA Triton Inference Server; (2) For Intel & ARM Processors: OpenVINO. _Note:_ ML Deploy tools use Triton & OpenVINO under the hood to build efficient Inference Services & then they also deplot it for you; (3) Vendor Agnostic: cog, chassis, chitra, crane)
                                                                1. On the Master Inference Service
                                                                    1. Which Inference Service is the Master Inference Service
                                                                        1. If there is only 1 Inference Service involved in serving the prediction of a particular model: then it will also be the Master Inference Service
                                                                        2. Else: then you will have multiple worker Inference Services doing part of the entire inference job & one master Inference Service that job will be to call these worker Inference Service in the right order, optionally wrap the prediction with some metadata  (e.g., explanation, or feature importance) & serve the final prediction to the Prediction Manager
                                                                    2. Working
                                                                        1. If a multi-prediction-service model: (_Note: probably you should be using an inference runtime (inside all Inference Services) that supports distributed inference_)
                                                                            1. Gets prediction pipeline (a pipeline is needed to support multi-step/multi-submodel predictions) from Data Pipelines Registry (which specifies the names of the pods running the models, IO of the models and data transformation steps (if receiving batch of inputs, can use a parallel processing framework to speed-ep (e.g., using threads: OpenMP; using multiple machines OpenMPI (low-level) or spark/koalas (high-level))) for the task
                                                                            2. Follows pipeline by getting predictions and doing necessary data transfomrations until final prediction is calculated
                                                                        2. Return prediction with any additional metadata (e.g., explanation, or feature importance) ot the Prediction Manager
                                                                        3. If some error occurs: go to a fallback
                                                                            1. Simple ML model
                                                                            2. Non-ML fallback
                                                                2. Building/Updating Inference Services. (assuming it is a single-prediction-service ml model, but it can be a chain of ml models mixed with transformation and heuristic steps) _Note_: If model is too big to do inference on a single machine, you will have to cut it into multiple sub-models and compose orchestrate them at inference time. Thus means doing this step for each of the parts of the model. _Note 2:_ Inference Service needs to have model ID/version in it! So that it can send this as metadata in its Monitoring requests/messages
                                                                    1. Inference Services to build (models will be in Model/Prompt Registry)
                                                                        1. Candidate wrapping Cadidate Model
                                                                        2. Current Best (being used in production) wrapping Current Best Model
                                                                    2. Steps
                                                                        1. Code (Tools: Ray Serve, Torch Serve). (_Note:_ ML-tailored tools also do the job of getting models, compising model pipelines, being the model runtime & sending metadata to Monitoring. General HTTP server libraries dont do this.)
                                                                            1. Model execution code:
                                                                                1. Option 1: Interpreted/JITd at runtime (_Note:_ Online learning needs to have Interpreted/JITd at runtime inference code). 
                                                                                    If single model: package model file & possibly a config file together in the container image, and listen to new best model to update the model when necessary. Also, load model in start-time.
                                                                                    
                                                                                    Elif multiple models: get the model from Model/Prompt Registry at inference time and cash most used models.

                                                                                    1. Interpreted at runtime (Tools: E.g., ONNX Runtime (_Note:_ the name can be a little misguiding since it does not necessarily have to interpret ONNX files, e.g.can transpile to TensorRT format and use TensorRT) voltaML (lib wrapper over multiple compilers/runtimes); LLM-specific (should support intra-model caching (prompt and KV caching)): Ollama, llama.cpp, TensorRT-LLM, GPTFast, FlexiGen). Use a runtime library to interpret these files, optimize and call .so kernels that implement model file operator (processig building blocks) functions to execute the computation. Will need to put standard libraries inside image.
                                                                                        1. Patterns 
                                                                                            1. 1 Model/Inference Service. Model baked in Inference Service: pulled at container build time. Note: need to rebuild and redeploy service in Inference Service ci/cd.
                                                                                            2. N Models/Inference Service. Model pulled from Model/Prompt Registry at container init when not in cache. Note: dont need to rebuild and redeploy service in Inference Service ci/cd, just need to trigger a container init (sidecar workload for updating some model that is in its cache), which is way faster. Or you can code the service as to keep checking for new available models.
                                                                                        2. Approaches
                                                                                            1. Out-of-the-box ONNX intepretation
                                                                                            2. ONNX Transpiled to some other model file format and then interpreted (Tools: TensorRT, TensorRT-LLM, ipex-llm, voltaML (lib wrapper over multiple compilers/runtimes)) (not compiled to native code, but to some compute-specification format with built-in operators (ONNX-like). At runtime this format is parsed and the kernels (implementation of the operators on the execution environment) are called) e.g., you can convert TF models to TensorRT (changing the computational graphs by making operators native to GPU execution) and then use a runtime that is packaged inside TF (operators present in the initial TF model that cannot be converted to operators supported bt tensorRT runtime are still put into the tensorRT model file.When the runtime encounters these operators it fallbacks to the tensorflow runtime) 
                                                                                    2. JIT at runtime (Tools: XLA does this for TensorFlow models). Specific functions are compiled to native code at runtime to provide speedups to the default interpretation. This enables model-specific optimization and removes the little interpreter overhead for that chunk of the model (runtime library code than has a lof of IFs (to check which operation is it), which are slow). These chunks of native code can be referenced within python (this will tell the python c interpreter to run them by giving its function pointer (address of the procedure)). Under-the hood this is basically invoking a compiler to make a .so, than dynamically load it and call it.
                                                                                2. Option 2: Compiled before. Just write the code that calls the model's library prediction function (a.k.a. some people call this a lightweight runtime) (Tools: (1) ML Compilers like: TVM, XLA, voltaML (lib wrapper over multiple compilers/runtimes), Adlik, AITemplate, Hidet, iree; LLM-specific Compilers like: MLC LLM, mnn-llm). _Note:_ just because we are dealing with compiled code doesnt mean we cant update the model in production, you can get the new .so library of the updated model & dynamically load it, then call it!
                                                                                3. Option 4: Calling External Services (e.g., OpenAI's API). Just write the client (usually HTTP) code. Most of the tools provide their client libraries.
                                                                            2. ML Wrappers
                                                                                1. Model management Wrapper (get models (can be ONNX or some other format, also can be .so library to be dynamically loaded) at either at inference time or scheduled times; if using federated learning: send model updates to parameter server)
                                                                                2. Uncertainty Wrapper (assuming model is calibrated) based on client-configurable confidence treshold.
                                                                                    1. If model is confident about the prediction, HTTP Server puts in the HTTP response:
                                                                                        >Confident=1 (together with the prediction, inside bundle)
                                                                                    2. If model is not confident about the prediction
                                                                                        >Confident=0 (together with the prediction, inside bundle)
                                                                                3. Explanability Wrapper (put explanation of the prediction together with the prediction)
                                                                                    1. Scenarios
                                                                                        1. Sends together with prediction (uses foil as a non-informative baseline)
                                                                                        2. Sends if client asks for a counterfactual explanation (why this output instead of this other one?) (uses foil as an informative baseline)
                                                                                    2. Methods
                                                                                        1. Feature importance/Attribitions (Good way to think about it: the level of importance of a feature X is measured by the entropy decrease between (P(Y=y) and P(Y|X=x). This is exactly how feature importance pops out of decision trees naturally, because they are naturally optimizing for this.))
                                                                                            1. SHAP (Improved version of Vanilla Shapley) values (there is a SHAP library for this)
                                                                                            2. Vanilla Shapley values (e.g., Integrated Gradients (Tensorflow Integrated Gradients is a great tool)) (_Note:_ computing Shapley values exactly in most cases is computationally intractable because of the number of feature, what is done is an approximation (the you can modify the Shapely accuray/computational cost tradeoff accordig to your needs))
                                                                                            3. Causal Inference (Tools: Microsoft Responsible AI (RAI), DeepCausality, Causal ML, dowhy, causalnex). Evaluating effect of intervention x on target y. Individual Treatment Effects (because the client wants to know what he has to change in order to improve their output)
                                                                                        2. Interpretable models (you get interpretability out-of-the-box)
                                                                                            1. Naturally interpretable Models (linear regression, decision trees, etc)
                                                                                            2. Doing knowledge distillation of Non-naturally interpretable model to naturally interpretable model
                                                                                        3. Decision Boundary methods (e.g., zooming in on decision boundaries close to the datapoint that we want expalanation for its output)
                                                                            3. Networking Wrapper: wrap model(s) inside HTTP (REST or RPC) servers to form Inference Service(s), setup other communications, and compile all the code (if using compiled language). _Note_: If model is too big to do inference on a single machine, you will have to cut it into multiple sub-models and compose orchestrate them at inference time. Each of these sub-models will now be layer servers (last layer == prediction) na you have a main prediction server that will just call these in order and add Wrappers. Layer Server are like prediction servers, but simpler, they generally wont contain Wrappers. (Tools: Fastapi, flask, etc)
                                                                                1. Overview
                                                                                    1. HTTP request endpoint(s) for clients to get preditions from their input data (RPC (preferred) or REST)
                                                                                    2. Role-based Access Control (Authentication & Authorization). Need to setup Streaming Ingestion endpoint to receive keys from key management service. Two cases:
                                                                                        1. When Inference Service is server (vast majority is only a server): needs public key of each of its clients key to authenticate them.
                                                                                        2. When Inference Service is a client of other service: needs its private key to generate cryptograpic signature
                                                                                    3. Streaming
                                                                                        1. Consume feedback on predictions messages
                                                                                        2. produce metadata messages to the monitoring system
                                                                                2. Operational modes
                                                                                    1. Single (Batch == 1) vs Batch (Batch > 1) Inference (computing prediction for [one input/multiple inputs]). _Note:_ in most cases, you should be giving the Inference Service the batch mode operation option, because if the client has many prediction requests he can leverage the batch mode & get all of them much faster (of course due to highly parallelized processing in the Inference Service either due to CPU vector processing, multi-threading/multi-core-CPUs or parallel processor acceleration (most common being GPUs))
                                                                                        1. Single Inference: usually when doing online inference, there is a cline that is requesting a prediction to a specific input
                                                                                        2. Batch Inference (the Inference Service gets data from data location):
                                                                                            1. Large Batch: usually when doing Offline Inference. Needs to be very optimized for parallel processing.
                                                                                            2. Small Batch: usually when doing Online inference. It is better if it is optimized for parallel processing.
                                                                                    2. Frozen parameters vs Online Learning:
                                                                                        1. Online Learning: needs Interpreter-based inference code, subscribes to <x,y> datapoints and does besides inference, learning aswell in microbatches to update the model. (Do both jobs asynchronously. Multi-thread of Multi-process). Periodically send model to model-registry. Can also use label Proxies to speed up retraining (because delayed labels can delay too much). 
                                                                                        /
                                                                                        _Note 1:_ The speed in which these delayed labels can come is crucial, so probably there needs to be a Stream Transport & Processing combo dedicated to get these datapoint directly from data collection to the Online Learning Inference Service (otherwise they would have to go to Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb) then ML Data whareshouse  then Feature Store before finally), and also if the labels are not natural, need a real-time annotation (can be more than just labelling) team (this is not common, generally Online Learning is used for tasks with natural labels like recommendation systems). 
                                                                                        /
                                                                                        _Note 2:_ everytime n times retraining occurs, the ew model must be sent to Model/Prompt Registry along with Metadata. Because Model/Prompt Registry is the place to know all models lifecycles.
                                                                                        /
                                                                                        _Note 3:_ Evaluating Online Learning Models before udating them can be hard, because you cant do test splits easily, but there are some options:
                                                                                            1. Waiting for reasonable test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)s: wait n number of new labelled datapoints arrive, treat this as your test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant), get a score; after good score you can learn on it
                                                                                            2. Multiple One-datapoint test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)s: Do Inference on every new labelled datapoint that arrives, do this n nummber of seuqntial times, then treat these as you test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)
                                                                                        2. Frozen parameters: non-online learning Inference Services keep their models with parameters frozen, retraining occurs externally and then a new Inference Service with an update frozen parameter model replaces the old one at the production cluster.
                                                                                    3. Offline vs Online Inference
                                                                                        1. Offline: precomputes predictions.
                                                                                            1. Getting input (featured datapoints)
                                                                                                1. Time-based: gets batch of datapoints from time to time
                                                                                                2. Datapoint-based: gets batch of datapoints when a new batch is ready
                                                                                            2. What to do with output (predictions)
                                                                                                1. Streams precomputed predictions to frontend so that the frontend doesnt need to call an api to get predictions when they are needed, becoming much faster.
                                                                                                2. Stores predictions on Feature Store so that at inference time the prediction manager just queries the feature store to get last prediction
                                                                                        2. Online: computes predictions as a result of request
                                                                                    4. Single-payload vs streaming payload
                                                                                        1. Single-payload: most models operate like this, the input x is sent in one single request, and output y is receives in a single return
                                                                                        2. Streaming payload: x is not fully known, because it is being generated in real-time by the data source, therefore, parts of x are sent to the Inference Service via streaming as they are generated. The Inference Service in this case needs to be adapted to this new paradigm to store sliding windows of data, hold state and send part of y as they are generated, also via streaming, to the consumers of the predictions.
                                                                                    5. Unique consumer vs Multiple consumesr p/prediction batch
                                                                                        1. Unique consumer: usually the case for online inference
                                                                                        2. Multiple consumers: usually the case for offline inference
                                                                                3. Prediction Serving Steps for a _single inference, online inference, serving requests, single-payload, unique consumer p/prediction batch, frozen parameters_ Inference Service (most cases)_ (''Serving Predictions'' and ''Receiving Feedback on Predictions'' both run in parallel)
                                                                                    1. Serving Predictions
                                                                                        1. Prediction Manager Receives request
                                                                                        2. Requests Feature Store for feature of the specific model that does the task the client wants
                                                                                        3. Calculate prediction
                                                                                            1. Gets prediction pipeline from Data Pipelines Registry (which specifies the names of the containers running the models, IO of the models and data transformation steps (if receiving batch of inputs, can use a parallel processing framework to speed-ep (e.g.,  using threads: OpenMP; using multiple machines OpenMPI (low-level) or spark/koalas (high-level)))) for the task
                                                                                            2. Follows pipeline by getting predictions and doing necessary data transfomrations until final prediction is calculated
                                                                                        4. Send prediction Prediction Filter
                                                                                            1. If filter says ok: respond prediction to frontend normally
                                                                                            2. If filter says not ok:
                                                                                                1. Respond to frontend that prediction was caught in predition (output) filter
                                                                                                2. Send POST request to Monitoring Service with: Model Name, Prediction, Client it was supposed to be sent to, Explanation of why it was filtered.
                                                                                        5. Return response to the client
                                                                                        6. Store datapoint and related information in Feature Store (technically this is not Feature Store stuff, but makes life so much easier for Monitoring, since Feature Store then stream all datapoint-related info):
                                                                                            1. <input/featured datapoint, output/estimate>
                                                                                            2. Time
                                                                                            3. Client JWT/Cookie (to get session), ID & location (Extracted from IP adress)
                                                                                    2. Receiving Feedback on Predictions (Weak or Strong Labels from UI) (_Note:_ these can also be published (like all the other data generated by UIs) to be stored in the Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb))
                                                                                        1. Prediction Manager Receives feedback containing feeback info: Type of Feedback, Model, datapoint ID, Feedback, Time, Client
                                                                                        2. send to (generally stream) Monitoring Service Feedback Message Containing feedback info
                                                                                        3. Store feedback on ML Data whareshouse  feedback info
                                                                        2. Validate IO schemas: make sure IO schemas of new model are compatible with production
                                                                        3. If Compiled before: build code (Tools: (1) ML Compilers like: TVM, XLA, voltaML (lib wrapper over multiple compilers/runtimes), Adlik, AITemplate, Hidet, iree; LLM-specific Compilers like: MLC LLM, mnn-llm)
                                                                            1. Type of Compile
                                                                                1. If Experimentation: Slow Compile. Compile model files as .so libraries (Some compilers like TVM and MLIR do the two step below, end-to-end) (Sometimes tools offer small runtimes (e.g., python library) that will use the .so library behind the scenes ) and can receive models parameters (Hybrid betwenn pure compilation and pure interpretation)). Will need to put resultant library <libModel.so, Model.h> files in container as dependencies if using small runtime or compile againt <other dependencies, libModel.so, Model.h, inference.c> into the standalone exetubale _inference_ (here you could also produce a dunamically linked executable to libModel.so, but a standalone executable is faster & you wont be updating the .so library inside the container, will be re-deploying a new pod). _Note:_ instead of building just the library and you write the prediction server code, some tools (e.g., NVIDIA Triton Inference Server) already do that job for you, making final standalone executable or container image that can just be deployed to serve predictions (according to supported serving protocols). 
                                                                                    1. Hardware Agnostic: Receive model file (computaional graph) and do high level IR optimizations (library-specific optimizations, computational graph optimizations (e.g., operator fusion (because sometimes there are powerfull instructions that we are not leveraging when we compute a lot of intermediate variables))) and lower code to lower level IR (compiler frontend)
                                                                                    2. Hardware-Aware:
                                                                                        1. High-Level IR Optimizations (e.g chosing path to execute computational graph) and lowering to lower level iR
                                                                                        2. Low-level IR optimizations and lowering to target hardware (compiler backend: LLVM usually)
                                                                                2. If Retraining: Super Fast Compile or no need to Compile (for Hybrid approaches, when there is a runtime that parses model/parameter file)
                                                                            2. Write (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) image
                                                                            3. Push it to private image registry
                                                        2. Option 2: model as data. Doesnt require you to re-deploy Inference Service
                                                            1. For compiled models & Inference Services that do get updated: put .so models on the Model/Prompt Registry & dynamically load the library: compile the model & CD .so model to Model/Prompt Registry, then runtime gets this new .so model upon event that says that new model is available.
                                                            2. For interpreted models (easier & faster to deploy): CD ONNX model to Model/Prompt Registry, then runtime gets the onnx model upon event that says that new model is available.
                                                4. Integrated Testing CD (triggered with: (1) if compiled model: push to private image registry; (2) elseif interpreted model: (2) push of evaluated model to Model/Prompt Registry)
                                                    1. Integrating Testing Cluster setup: Setup/reconfig cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                    2. Deploy candidate Inference Service/manager and current Inference Service/manager containers (get images from private registry) to integrated testing cluster (tools: Terraform/Terragrunt/terramate, Pulumi, Crossplane) using kubectl (pulling images from pivate (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) registry) that deploys pods on the cluster and deployment configuration files. _Note:_ if interpreted models: the Inference Service is not specific to one model usually, I tend to prefer calling them Prediction Managers that on a timely basis loads updated models from a storage location (e.g Model/Prompt Registry), and these can be different versions of the same models or different models that operate on different endpoints.
                                                    3. Evaluate candidate Inference Service/manager vs current Inference Service/manager in integrates testing cluster:
                                                        1. Automatic Evaluation
                                                            1. API testing
                                                            2. Operational metrics: 
                                                                1. latency & throughput
                                                                2. Resource usage/function
                                                                    1. Power
                                                                    2. Memory
                                                                    3. Networking
                                                        2. Human triggers ''Inference Service seems bettter than the old one, can send to image resgistry with production-ready status''
                                                    4. Push it to private image registry
                                                5. Staging CD (triggered with push to private image registry)
                                                    1. Staging Cluster setup: Setup/reconfig cluster if necessary depending on image hardware requirements (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                    2. Deploy candidate Inference Service/manager in the same way you do in production
                                                        1. IO Filters: many times, Inference Services have input and/or output services that act as filters. These need to be deployed with the model and can be retrieved for a specific model at the Model/Prompt Registry. The process of uilding it into a Inference Service from a filter file is very similar to the one explainined earlier of going from model to Inference Service.
                                                    3. End-to-end test it with simulated production workload & traffic. _Note:_ this can be too slow for you, so you might want to implement it in a background production online ephemeral workloads cluster
                                                    4. Generate CI/CD Report (in easily parsable format (e.g., yaml, CUE) and rendering format (e.g., svg or pdf) & put as PR comment
                                                    5. Staging result
                                                        1. If passed:
                                                            1. Send alert to human (via slack, email or dedicated alerts manager like: keep)/log with CI/CD report
                                                            2. CD artifact
                                                                1. If compiled model: Send image to private image registry saying ''mode ready for production'' along with staging cluster configurations used
                                                                2. If interpreted model: send model to Model/Prompt Registry saying ''mode ready for production'' along with staging cluster configurations used
                                                            3. If other models affected: Read Dependent model retraining policy
                                                                1. If it says to retrain immediately: retrian them
                                                                    1. Automatic ML Training Precedure Generation
                                                                2. Elsif it says that label shift has to reach certain treshold for retraining:
                                                                    1. Get test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)s of last model and updated model (can ask Experiment Tracking System that will give pointer to Place in Feature Store) and calculate label shift.
                                                                    2. If below treshold: dont do retraining
                                                                    3. Else: retrian them
                                                                        1. Automatic ML Training Procedure Generation
                                                            4. Trigger ''artifacts (services or models) (core model and dependent ones) ready for production'' to production CD
                                                        2. If failed: send alert to human (via slack, email or dedicated alerts manager like: keep)/log with CI/CD report
                                                    6. Dismantle cluster if necessary (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                                                    7. Final trigger to enable Production CD is only triggered if documentation is complete (Tools: OpenAPI-Specification, redoc, pdoc, devdocs, sphinx, mkdocs, read the docs, gitbook, Quarto)
                                                        1. Manual way: some engineer checks it
                                                        2. Automated way:
                                                            1. Have documentation linked to code (when code changes, alert is sent to the pieces of documentation linked to the code; if the code doe not change, that piece of doc does not need to be updated)
                                                            2. Use LLMs
                                                6. Production CD (triggered by: staged Inference Service image in private image registry)
                                                    1. Production CD listens to ''artifacts (services or models) (core model and dependent ones) ready for production'' trigger  along with staging cluster configurations used
                                                    2. Verify if production cluster is compatible with staging cluster
                                                    3. If model origin==experimentation: online experimentation configuration
                                                        1. Feature-related
                                                            1. A/B or Multi-armed bandit tests between different feature staleness margin (i.e. the parameter that defines the gap size between the features datapoint timestamp the prediction manager wants & the oldest featured datapoint it can still use with acceptable prediction degradation)
                                                        2. Model-related
                                                            1. Comparing Models
                                                                1. Traffic routing for A/B experiment (route packets from BFF 50% of time to one pred service (model A), and 50% to other (model B))
                                                                2. Traffic routing for multi-arm bandit experiment (''When you have multiple models to evaluate, each model can be considered a slot machine whose payout (e.g., prediction accuracy) you dont know. Bandits allow you to determine how to route traffic to each model for prediction to determine the best model while minimizing wrong predictions shown to your users'')
                                                            2. Evaluating Models
                                                                1. Traffic routing for Canary deploys
                                                                2. Shadow deploy
                                                        3. System-related
                                                            1. Latency Requirements (sometimes you user can wait longer than you expected, sometimes he cant wait the amount of time you hypothesized)
                                                    4. Deploy services to production cluster using kubernetes/kubeflow client (pulling images from pivate (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) registry) that deploys pods on the cluster and deployment configuration files
                                            3. If new IO filter
                                                1. Get filter
                                                2. Build filter service
                                                3. Deploy using kubernetes (using Shipwright, Skaffold/Flux/ketch/kubevela/kubeblocks, Lens/Kubeapps/porter-archive, Rancher, Istio/Kiali/linkerd2/Cilium/Consul, Helm/Kustomize/timoni/JuJu/cdk8s/kpt, yamllint/Carvel/kubesec/kubeaudit/kubeconform/kube-linter/polaris/conftest/Kubescape/Kyverno/OPA/Datree/Kubevious, SchemaHero/Datashim/Velero, Karpenter, Sealed Secrets/Kubernetes Secrets Store CSI Driver/External Secrets, Metacontroller)/OKD/kubeflow to the respective cluster
                                                    1. Integration Testing
                                                    2. Staging
                                                    3. Production
                                            4. If new fallback
                                                1. Get fallback & frontends linked with it
                                                2. Build consumer frontends
                                                3. Deploy using kubernetes (using Shipwright, Skaffold/Flux/ketch/kubevela/kubeblocks, Lens/Kubeapps/porter-archive, Rancher, Istio/Kiali/linkerd2/Cilium/Consul, Helm/Kustomize/timoni/JuJu/cdk8s/kpt, yamllint/Carvel/kubesec/kubeaudit/kubeconform/kube-linter/polaris/conftest/Kubescape/Kyverno/OPA/Datree/Kubevious, SchemaHero/Datashim/Velero, Karpenter, Sealed Secrets/Kubernetes Secrets Store CSI Driver/External Secrets, Metacontroller)/OKD/kubeflow client to the respective cluster
                                                    1. Integration Testing
                                                    2. Staging
                                                    3. Production
                            3. Frontend Component CI/CD (Tools: CMS tools: Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal; Ml-specific: agentlabs)
                                1. CI
                                    1. Validate Component Specification
                                        1. Core Valation (ha to obey specification rules)
                                        2. Contract tests: Validate aginst deployed API gateway
                                    2. Build Frontend Component in for your desired frontend tool
                                2. CD: deploy new html piece to webserver
                                    1. Option 1: webserver listens to html updates and rebuilds the html its serving online
                                    2. Option 2: redeploy webserver with new html
                3. Data Collection System
                    1. Data Collection Patterns
                        1. Passive: Streaming (Data Sources Publish data to stream transport cluster, and Business Backend consumes it and processes it with Stream Processing tool) (Tools: stream transport (e.g., Kafka, Mosquitto, RocketMQ, Pulsar, Kinesis, Pravega, Brooklin) + stream processing (e.g., Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast)
                        2. Active: Request/Response (Business Backend requests data from services)
                    2. Components
                        1. Business Raw Data Lake Ecosystem (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb)
                            1. Business Raw Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb) core: storage storage & query processing (where all raw data that might be usefull is stored & their contents can be queried, largest DB (usually distributed), usally cloud provided)
                                1. Organization
                                    1. Separate Historical, user generated (sent by frontends, or anlaytics system) and third party raw data
                                    2. Separate data by consumers, to ge the data they consume as close as possible to them
                                    3. Make shure that if a label arrives for a datapoint that arrives earlier, these get matched together
                                    4. Dataset Metadata: "dataset descriptions, annotations, preprocessing steps, and licensing details"
                                2. Processing
                                    1. Aswer queries
                                        1. Analytics
                                        2. Filtering
                                        3. Content Search
                                    2. Data Integraty checks: "should ensure data integrity by implementing checksums or hash functions to detect and prevent data corruption, maintaining the consistency and reliability of the datasets over time."
                                3. FrontEnd (Tools:
                                    (1) From scratch
                                        (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                        (2) Automatic template: screenshot-to-code
                                        (3) Implementation
                                            (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                                (1) Code:
                                                    (1) Template Libraries: Grapesjs
                                                    (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                    (3) Supporting Libraries: 
                                                        (1) State management: Redux; 
                                                        (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                        (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                    (4) Telemetry
                                                        (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                        (2) Session Replays: OpenReplay
                                                    (5) UI Dev Environment: Storybook, One
                                                (2) Build: Webpack; 
                                                (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                                (4) Build & Web Serve: 
                                                    (1) SSR: Nextjs, Astro, remix; 
                                                (5) Code & Build: 
                                                    (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                    (2) Static Site Generators: Hugo, Gatsby, Quartz
                                    (2) Using task-specific high level tools
                                        (1) Dashboards: metabase, plotly, graphana)
                                    1. Exploration
                                    2. Filtering
                            2. Business Backend listens to streaming messages/requests data from data sources, optionally applies some filtering by using some specialized service for this purpose (e.g., if receiving frames from a camera, you dont want to store all the data, because it is too much data and most of it has nothing special, so you want to possibly downsample and use some ML model on streaming data, to identify interesting slices) and stores in Business Data Lake (append-only)
                                1. Content of Messages
                                    1. Event Data itself (Appended with unique ID:= append(dataSourceID, uniqueIDGenerator)). _Note:_ when user stops using your product & start usign again are very important events!. This data will implicitly (can apply a function to get) contain:
                                        1. <Task, Features & Label ((X,Y))>
                                        2. <Task, Just features (X) or partial features (X')>
                                        3. <Task, Label (Y)>
                                    2. Metadata
                                        1. Feature Explanation
                                        2. Noise (if present) to estimate noise do experiments with data source where you gather the data in aprox the same experimental conditions as the real world data collection condition in one of two ways (which in the end are the same thing): randomized way or expected value way --> always changing some factor that wouldnt be taken int account into you real world data collection, and do it n times.
                                            1. Types of noise
                                                1. Feature Noise (e.g., noise of sensors)
                                                2. Label Noise
                                            2. Estimating noise
                                                1. When you can know the ground truth experimental value (e.g., sensors): do n data gathering trials and see the noise around it. If the noise is biased, send the calibration necessary to unbias it.
                                                2. When you can't know the ground truth experimental value (e.g., asking people their income): do n data gathering trials and see the noise around the expected value (e.g., income data --> you ask)
                                        3. Time data was gathered
                                        4. Which Device gathered the data
                                        5. Data Owners (if any)
                                        6. Processing Priority of the datapoint(s) (the more dynamic the features that are built using these raw datapoints, the more processing priority they get, until a treshold where it doesnt pay off trying to build super fast changing features, and this job is let to the inference data pipeline (Option 1: use streaming data processing tool (e.g., Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast) in which the pipeline of tasks is done internally in the tools' dsitributed environment (as defined by you in driver code) & streams back to prediction manager. The good thing is that these tools are very optimized but the bad things is that the tasks need to be launched; Option 2: sequential services connected to each other (pipeline) via streaming and that output features on feature store & streams back to prediction manager. _Note:_ first service will be listening to inference data pipeline trigger along with the request via streaming, and once listened, gets data needed from ML data wharehouse & sends to the next data pipeline service) to make featured data on the fly
                                2. Raw Data Sources
                                    1. Internal
                                        1. Active Sources
                                            1. BFF/API gateway or Frontends: Active Source
                                            2. Embedded devices: Active Source
                                        2. Passive Sources
                                            1. Department-specific Storage System (DB, Data Lake, Data Wharehouse, etc)
                                                1. Active Dataset (Send streams of messages to us))
                                                    1. Subscribe announcements lists (changes in the data that is being sent)
                                                    2. If changes occurred:
                                                        1. Send request to Monitoring System containing the changes
                                                        2. Stash data somewhere to be used later (when the pipelines are updated to deal with the new data)
                                        2. Passive Dataset (Need to request data to them): peridically get data or listen to new batch of data ready c
                                    2. External
                                        1. Active Sources (Send streams of messages to us) (e.g.,  Company we hired to get data for us)
                                            1. Subscribe announcements lists (changes in the data that is being sent)
                                            2. If changes occurred:
                                                1. Send request to Monitoring Service containing the changes
                                                2. Stash data somewhere to be used later (when the pipelines are updated to deal with the new data)
                                        2. Passive Sources (Need to request data to them)
                                            1. Structured Data: HTTP APIs (e.g., some .csv dataset from kaggle)
                                            2. Unstructured Data: web scrapping
                                                1. Using APIs to get data (e.g., using Twitter API)
                                                2. HTML Scrapping to get data inside HTML
                                3. Raw Data Transport 
                                    1. Stream Transport (e.g Kafka, Mosquitto, RocketMQ, Pulsar, Kinesis, Pravega): stores messages in cluster where consumer (Business Backend) can subcribe to topics (guarantees ordering of the messages for a particular consumer)
                                        1. Message production /topic by Data Source
                                        2. Message storage in cluster
                                        3. Message consumption /topic by Business Backend
                                    2. High-level declarative transport (Tools: rudder-server) 
                                4. Source2RawDataLake Data Pipeline (Tools: airbyte, skyplane, Fivetran, Stitch): Stream Processing (e.g., spark/koalas streaming, Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast): connects to Stream Transport using Stream Transport client or a specific integration
                                    1. Data injection (not technically a processing step, but inside the ETL idea)
                                    2. Data Validation (Tools: JSON Schema, Great Expectations, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation)
                                        1. Schemas
                                        2. Descriptive statistics (should not be that diffferent between batches)
                                        3. Ranges
                                        4. Anomaly/outlier detection (e.g., isolation forest)
                                        5. Metadata
                                        6. Heuristics
                                    3. Data storage: store in Business Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb).
                        2. Storage Ecosystems (Tools: DeltaLake, LakeSoul)
                            1. ML Data Wharehouse Ecosystem (4 tabular data) (Tools: managed: BigQuery, Snowflake, Redshift; open source: Doris, Hive or based on open source distributed SQL DB like YDB)
                                1. Raw2RawMLTable Data Pipeline: wrangles raw busineess data, makes pointer to raw ML data, structured it and stores as raw structured ML tables in the ML Data Wharehouse. ML Backend that injects data from Business Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb), processes it and stores ML-related data in an SQL database.
                                    1. Sources of Data
                                        1. Data Lake
                                        2. Static Data DB (that stores static data of users (e.g., date of birth, gender, etc), usually connected to the BFF & is a SQL DB)
                                    2. Modes of Operation
                                        1. Batch (ETL) (Tools: (1) Managed: Managed Pachyderm, Databricks Managed spark/koalas, Amazon EMR; (2) OSS: (2.1) Distributed processing engines: Daft, Dask, Ray, spark/koalas; (2.2) Workflow Orchestrators with custom code inside tasks: e.g., Argo, Prefect, Airflow, Dagster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau. (2.3) Some Distributed processing engine on top (as a task) of some workflow orchestratore e.g., Dask, spark/koalas))
                                            1. Batch injections (e.g.,  DB client or spark/koalas connector (DB client under the hood))
                                            2. Batch processing: filtering & joining
                                            3. Store batch in ML Data whareshouse
                                        2. Streaming
                                            1. Stream transport (e.g., Kafka, Mosquitto, RocketMQ, Pulsar, Kinesis, Pravega, Brooklin)
                                                1. Receiving messages from Business Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb)
                                                2. Sending messages to Raw2RawMLnon-tabular data Pipeline saying "I the middle of tabular data, I encountered some files that I need you to process for me, here are the details (as a string) on how to get the file and here is where i want you to put it after you finished processing (put details on how to get from your ML Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb) in My ML Data whareshouse column as a string)"
                                            2. Stream processing; filtering & joining (e.g., Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast)
                                            3. Store processes message as a row in ML Data whareshouse 
                                2. ML Data Whareshouse: storage & query processing (where all raw data that might be usefull for ML is stored, usally cloud provided)
                                    1. Data stored
                                        1. X_raw: raw features
                                            1. Meta-data
                                            2. Training Data itself
                                        2. Feedback Data. Below, from less informative to more informative feedback.
                                            1. Goodness of prediction (could be binary or have levels)
                                            2. Online Labels. Y_hat 4 X_raw (Labels on the models online performance, given indrectly or directly by the user)
                                                1. Proxy Labels (can be used as indicative of perfromance (and way of gathhering more data), or also as training data (traformed to strong labels afterwards or using a weakly labeled paradigm))
                                                2. Strong Labels (can be used direclty as training data)
                                            3. Online Labels + Explanation.
                                            4. Open-format Feedback.
                                        3. Data source (needs to specify the source) and time it was collected
                                            1. Online
                                                1. User
                                                2. Internal system
                                                3. Third-Party service
                                            2. Offline
                                                1. Curated Dataset
                                                2. Third-Party Dataset
                                                3. Internal Dataset
                                    2. Organization
                                        1. Separations
                                            1. Separate by Model
                                            2. Separate internal, user generated (sent by frontends/targets) and third party raw data
                                            3. Separate data by consumers, to ge the data they consume as close as possible to them
                                        2. All datapoints have an estra column called ''failed_validation'': it will be 0 if smoke tests did not identify anrmalities in it, else it will be 1
                                    3. Processing
                                        1. Aswer queries
                                            1. Retrieve/Delete/Update
                                            2. Analytics
                                            3. Filtering
                                            4. Content Search
                                            5. ELT (queries where data is retrieved and then stored again to update or create a new table), _Note:_ ELT can be modularized into ELT tasks and deployed under a workflow orchestrator!
                                        2. Validation Smoke tests:
                                            1. Policy
                                                1. When to run: at every insert
                                                2. What to do when anormality detected: log/send to (generally stream) Monitoring System and change column ''failed_validation=0'' to ''failed_validation=1''
                                            2. Testing Suite
                                                1. Raw schema validation
                                                    1. Update when: new production raw schema version
                                                    2. Methods
                                                        1. Columns
                                                            1. Existence
                                                            2. Order
                                                            3. Data type
                                                            4. Units
                                                        2. Rows: Number of rows
                                                2. Expectations
                                                    1. Update when: new production raw schema version
                                                    2. Methods
                                                        1. Descriptive statistics
                                                        2. Ranges
                                                        3. Relationships
                                                            1. Between dimensions (''e.g.,  DIM_1 > DIM_B'')
                                                            2. Between raw datapoints (e.g.,  raw datapoint needs to be unique, or a values in a dimension needs to be unique; or rawDatapoint_{t-1}[time] < rawDatapoint_{t}[time])
                                    4. FrontEnd (Tools:
                                        (1) From scratch
                                            (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                            (2) Automatic template: screenshot-to-code
                                            (3) Implementation
                                                (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                                    (1) Code:
                                                        (1) Template Libraries: Grapesjs
                                                        (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                        (3) Supporting Libraries: 
                                                            (1) State management: Redux; 
                                                            (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                            (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                        (4) Telemetry
                                                            (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                            (2) Session Replays: OpenReplay
                                                        (5) UI Dev Environment: Storybook, One
                                                    (2) Build: Webpack; 
                                                    (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                                    (4) Build & Web Serve: 
                                                        (1) SSR: Nextjs, Astro, remix; 
                                                    (5) Code & Build: 
                                                        (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                        (2) Static Site Generators: Hugo, Gatsby, Quartz
                                        (2) Using task-specific high level tools
                                            (1) Dashboards: metabase, plotly, graphana)
                                            1. Exploration
                                            2. Filtering
                                    5. Capabilities
                                        1. Validation and SQL-like storage or modification of ML raw data
                                        2. SQL-like retrieval of ML raw data
                                        3. Ability to track ML raw datapoint to raw datapoint
                                        4. Trigger ''new x datapoints available'' to Training Data Pipeline
                            2. ML Data Lake Ecosytem (4 non-tabular data) (processed files) (where all complex (non-tabular) ML raw data files that might be usefull are stored & their contents can be queried, distributed DB, usally cloud provided). _Note:_ can be the same Data Lake tool used for the Business Data Lake, just logically separate & with enough scaling to provide good performance for both uses cases.
                                1. Raw2RawMLnon-tabular Data Pipeline: basically filters & processes the raw business data files. ML Backend that injects data files from Business Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb), processes it and stores ML-related data files in an Unstructured Database (ML DataLake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb)).
                                    1. Batch (ETL) (Tools: (1) Managed: Pachyderm, Databricks Managed spark/koalas, Amazon EMR; (2) OSS: (2.1) Distributed processing engines: Daft, Dask, Ray, spark/koalas; (2.2) Workflow Orchestrators with custom code inside tasks: Argo, Airlfow, Prefect, Dahster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau, dagger; (2.3) Some Distributed processing engine on top (as a task) of some workflow orchestrator)
                                        1. Batch injections (e.g.,  DB client or connector (DB client under the hood) provided by the distributed processing tool) from Raw Business Data Lake
                                        2. Batch processing
                                        3. Store batch in ML DataLake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb)
                                    2. Streaming
                                        1. Stream transport (e.g., Kafka, Mosquitto, RocketMQ, Pulsar, Kinesis, Pravega, Brooklin) receiving messages from Raw2RawMLTable Data Pipeline saying "I the middle of tabular data, I encountered some files that I need you to process for me, here are the details (as a string) on how to get the file and here is where i want you to put it after you finished processing (put details on how to get from your ML Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb) in My ML Data whareshouse"
                                        2. Stream processing (e.g., Storm, Kafka Streams, Spark Structured Streaming, Numaflow, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast)
                                        3. Store processed file in ML DataLake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb)
                                        4. Embeddings (Tools: LASER)
                                            1. Generate Embeddings from your data using embeddding models (tools: vectorflow, superlinked, Text Embeddings Inference, infinity, pykeen)
                                            2. Store Embedding on Tensor DB
                                            3. Store Data (that was embedded) also on Tensor DB or just a pointer to it
                                            4. Store Embedding Pointer in in ML Data whareshouse
                                2. ML Raw Data Lake (tools: managed solution or based on open source distributed noSQL DB like (e.g., cassandra or scilladb)) core: storage & query processing of raw files that will be used by ML (e.g., audio, images, video)
                                    1. Organization
                                        1. Separate Historical, user generated (sent by frontends, or anlaytics system) and third party raw data
                                        2. Separate data by consumers, to ge the data they consume as close as possible to them
                                        3. Make shure that if a label arrives for a datapoint that arrives earlier, these get matched together
                                    2. Processing
                                        1. Aswer queries
                                            1. Analytics
                                            2. Filtering
                                            3. Content Search
                                    3. FrontEnd (Tools:
                                        (1) From scratch
                                            (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                            (2) Automatic template: screenshot-to-code
                                            (3) Implementation
                                                (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                                    (1) Code:
                                                        (1) Template Libraries: Grapesjs
                                                        (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                        (3) Supporting Libraries: 
                                                            (1) State management: Redux; 
                                                            (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                            (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                        (4) Telemetry
                                                            (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                            (2) Session Replays: OpenReplay
                                                        (5) UI Dev Environment: Storybook, One
                                                    (2) Build: Webpack; 
                                                    (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                                    (4) Build & Web Serve: 
                                                        (1) SSR: Nextjs, Astro, remix; 
                                                    (5) Code & Build: 
                                                        (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                        (2) Static Site Generators: Hugo, Gatsby, Quartz
                                        (2) Using task-specific high level tools
                                            (1) Dashboards: metabase, plotly, graphana)
                                        1. Exploration
                                        2. Filtering
                        3. Dataset Improvement System: its job is to improve data (Tools: cleanlab)
                            1. Raw Data Quality Enforcer (Tools: (1) Open Source: (1.1) General: JSON Schema, Great Expectations, Soda Core, Cleanlab, Evidently, Deepchecks, datachecks; (1.2) CV: cleanvision; (2) Proprietary: Monte Carlo, Qualdo, acceldata, DataBand, Metaplane, Talend Data Quality, Anomalo, BigEye, Ataccama)
                                1. Periodically, gets batch of data from ml data lake
                                2. Checks for (via rule, heuristic or ML model):
                                    1. Data Completeness
                                    2. Data Uniqueness
                                    3. Data Consistency
                                    4. Data inside Expectations
                                    5. Appropriate Data (non NSFW)
                                    6. Data Diversity
                                    7. Data Quality (e.g., use an LLM critic with good recall to filter data generate by an LLM synthetic data generator.)
                                3. Executes data correction policy
                                    1. Put problematic data in separate location
                                    2. Alert programmer via sending telemetry to the ML Monitoring System
                                4. Correct data problem 
                                    1. Manual (triggered by engineer)
                                        1. Put back corrected data
                                        2. Delete problematic data
                                    2. Automatic
                                        1. Deletion
                                        2. Value replacement
                                        3. Value Imputation
                                5. Give quality stamp do data (can't be ETL'd without quality stamp)
                            2. Adding more data: labelling and synthetic data (1) Open Source: Label Studio, Labelbox, xtreme1, docanno, cvat, diffgram, cleanlab, cleanvision, Universal Data Tool, refinery, Slicer, labelme, makesense.ai, aubio, Praat; (2) Proprietary: Dataloop, prodigy, FiftyOne, Superb AI, Scale AI, Human Signal, aquarium, v7labs, Appen, Lionbridge, Alegion, Kili, SuperAnnotate, Encord Annotate, labelme, praat, AWS Mechanical Turk)
                                1. Components
                                    1. Backend (Batch jobs)
                                        1. Perceives that there is only few datapoints left to label
                                        2. Stores featured datapoints with labels in Feature Store
                                        3. Extracts datapoints from Feature Store to get labelled. These datapoints need to be a representative subset of the most recent window.
                                    2. DB for storing labelled datapoints (within a certain window like a week)
                                    3. FrontEnd for labellers (Tools:
                                        (1) From scratch
                                            (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                            (2) Automatic template: screenshot-to-code
                                            (3) Implementation
                                                (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                                    (1) Code:
                                                        (1) Template Libraries: Grapesjs
                                                        (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                        (3) Supporting Libraries: 
                                                            (1) State management: Redux; 
                                                            (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                            (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                        (4) Telemetry
                                                            (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                            (2) Session Replays: OpenReplay
                                                        (5) UI Dev Environment: Storybook, One
                                                    (2) Build: Webpack; 
                                                    (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                                    (4) Build & Web Serve: 
                                                        (1) SSR: Nextjs, Astro, remix; 
                                                    (5) Code & Build: 
                                                        (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                        (2) Static Site Generators: Hugo, Gatsby, Quartz
                                        (2) Using task-specific high level tools
                                            (1) Dashboards: metabase, plotly, graphana): that Labellers interact with to label data 7 explains labels, talks with backend sending labelled data & explanations
                                    4. Automatic (ML-powered) labelling Service (Proxy Labels from combination of subject matter expert heuristics.
                                        1. Goals
                                            1. Automatically label data
                                            2. Make labelling easier: label data before labellers do, so that labellers can just aprove in cases it is already correct
                                        2. Methods
                                            1. Programmatic labelling via heuristic functions (Tools: Snorkel)
                                                1. Receives annotation (can be more than just labelling) requests from backend, labels according to heuristics and sends back multiple labels for a single datapoint
                                                2. Has its own Frontend for subject matter experts to make heuristic functions (this is a hard problem, heuristic function must orchestrate well together as to aproximate the real function we are trying to approximate)
                                                3. Has access to the Feature Store & uses knowledge of labelled datapoints to guide heuristic funtion building (can check if sung current heuristic functions, the avg datapoints will be close to the actual datapoints)
                                            2. Via ML models
                                                1. Structured data: traditional ML models (Tools: autoML libraries)
                                                2. Unstrctured data: LLMs (Tools: Autolabel, Adala)
                                2. Tasks
                                    1. Core Human Annotation (can be more than just labelling): consistent annotation (can be more than just labelling) (raising humman level performmance) (Especially important for low data regimes, because noise doesnt cancel itself, and algorithm confuses thinks noise is signal (overfits)). Annotation (can be more than just labelling) inconsistency can happen due to:
                                        1. Guidelines: not clear annotation (can be more than just labelling) guidelines. Solution: Multiple Labellers label same datapoint to check for inconsitencies/intrinsitc stochasticity/source of noise of the problem (Bayes error rate bound) (Typically happens in cases where the annotation (can be more than just labelling) can be interpreted in more than 1 way (inside the general (all reasonable) y's there are sub y's and your aim is to model one sub_y very well not have reasonable performance in all of them (what you would have by sampling from the general y distribution. Because P(general_y|x) has higher noise/entropy than P(sub_y|x)))).
                                        2. People: bad labellers. People just want to finish the job earlier and dont care if they are doing correclty. Solutions can be: review some samples of a labeller, and if they are bad, reject his entire annotation (can be more than just labelling) batch. Another one is to do _Jury Learning_ where you put the labeller iformation (age, sex, ... other features) as input to the model.
                                        3. Task: sometimes the task has difficult cases inhherintely, near decision tresholds, these cannot be solved by god guidelines neither good people. There are options to change th task to improve consistency such as merging or creating new classes (e.g., of creating new class: visual inspection of broken phones, you want to classify them as scrathed or not, but the middle cases are difficult and people label them differently, o you can change classes to {no scratch, maybe scratch and clear scratch}; or for audio transcription: when labellers cant tell what is being said, you can create a unintelligible class)
                                        4. Support for informed data labelling: should allow engineers to easily create labelling spaces (which define a the necessaity for a certain kind of data) according to Slice of input and Behaviorual tests. Also should support data generation for behavioral tests (data programming). E.g., invariance tests dont require labelled data.
                                    2. Beyond just labelling: explanation of labels, why the labeller labelled it that way in natural language. _Note:_ this is an advancced capabability because it requires a lot of labeller training to get consistent explanations.
                                    3. Annotation QA: annotations are reviewed. 
                                        1. Manual: This can be done by a manager or engineer, or one labeller can review another labeller's annotation.
                                        3. Automatic: a Foundation Model (FM) classsifies as right or wrong annotation.
                            3. Expert Review of cases ML is not shure or high stakes datapoints
                                1. Backend (Tools: (1) Python: (1.1) HTTP API libraries (build them as a library for Dynamic HTTP Server Library to use): FastAPI; (1.1.1) REST (Old messy way very tied to DB principles): plain FastAPI; (1.1.2) RPC (New functional way that treats paths as functions, payloads as parameters, compresses data and fully embraces http 2.0)): gRPC + FastAPI; (1.1.3) GraphQL (New DB way that ignores vestige HTTP paths, treat payloads as lossened DB queries (not just reading, but also updating & adding data) payloads as parameters, compresses data and fully embraces http 2.0): add Ariadne + FastAPI; (1.2) Dynamic HTTP Servers (handles parallel socket connections, calling HTTP API libraries, error returns): Gunicorn (WSGI) with Uvicorn (ASGI) interface; (1.3) HTTP Reverse Proxies: NGINX; (2) Nodejs: (2.1) HTTP API libraries: Dont know any; (2.1.1) REST: plain FastAPI; (2.1.2) RPC: gRPC; (2.1.3) GraphQL: GraphQLjs; (2.2) HTTP Servers: HTTP Module; HTTP Module; (2.3) Single Dynamic HTTP Framework: Express, Nestjs; (1.3) HTTP Reverse Proxies: NGINX)
                                    1. Receives datapoint from prediction manager and also prediction (to be double checked)
                                2. Frontend that labellers can just aprove or disapprove and label corerectly
                                3. send to (generally stream) Monitoring system the tuple (feature datapoint, prediction, human label, error (human label - prediction))
                                4. Send back label check result to prediction manager (can take a while)
                                5. Store label (given by human) in the respective datapoint row in the Feature Store (using unique ID of the datapoint (generated at data collection))
                            4. Active Learning System (Tools: Encord Active, modAL, libact, ALiPy, Baal, Aquarium): responsible for wise training data generation/selection. Obs: annotation (can be more than just labelling) is handled by the outer Annotation (can be more than just labelling) System) 
                                1. Capabilities
                                    1. Data Collection Guidance: decide which datapoint regions should be favoured
                                        1. For LLMs (text data): Identification of failure themes. Identify themes were LLM is not working well, so that we can add more data of that type to prompt/training/evaluation. Can use topic modelling for this.
                                    2. Data Labelling Priority: decide which datapoints should be labelled first
                                    3. Datapoint Sampling: fullfilling priority requests (How to sample existing data? Training Data Pipeline and Core Annotation will use will use this) (1) Training data pipeline wants to know which virtual datapoints to make first (when time is not a factor, becuase when time ia a factor, you need to process always the newest ones.), aka have processing priority or non-random sampling. This is better than just doing random sampling. (2) Core Annotation wants to know this, because we want to label a subset of recent data that is representative of recent data distribution, instead of just labelling the newest ones.
                                    4. Curriculum Learning: provides the training workload with the right order it should get the minibatches inside an epoch.
                                2. Components
                                    1. Backend (Tools: (1) Python: (1.1) HTTP API libraries (build them as a library for Dynamic HTTP Server Library to use): FastAPI; (1.1.1) REST (Old messy way very tied to DB principles): plain FastAPI; (1.1.2) RPC (New functional way that treats paths as functions, payloads as parameters, compresses data and fully embraces http 2.0)): gRPC + FastAPI; (1.1.3) GraphQL (New DB way that ignores vestige HTTP paths, treat payloads as lossened DB queries (not just reading, but also updating & adding data) payloads as parameters, compresses data and fully embraces http 2.0): add Ariadne + FastAPI; (1.2) Dynamic HTTP Servers (handles parallel socket connections, calling HTTP API libraries, error returns): Gunicorn (WSGI) with Uvicorn (ASGI) interface; (1.3) HTTP Reverse Proxies: NGINX; (2) Nodejs: (2.1) HTTP API libraries: Dont know any; (2.1.1) REST: plain FastAPI; (2.1.2) RPC: gRPC; (2.1.3) GraphQL: GraphQLjs; (2.2) HTTP Servers: HTTP Module; HTTP Module; (2.3) Single Dynamic HTTP Framework: Express, Nestjs; (1.3) HTTP Reverse Proxies: NGINX)
                                        1. For Data Collection
                                            1. Data Collection Demands
                                                1. Passive: Receiving annotation (can be more than just labelling) demands from training procedures with feature ranges, generate unlabelled datapoint (Human or Automated Data Generator) and store in Feature Store unlabelled datapoints
                                                2. Active: Receiving annotation (can be more than just labelling) demands from Monitoring failures modes of the system (high-level queues in text form) and sending request to the system responsible for data generation (Human or Automated Data Generator)
                                            2. Data Generator
                                                1. Human data (X) generator (generally the case)
                                                    1. Sends data desires to FrontEnd 2
                                                    2. Receives unlabelled datapoints from FrontEnd 2 and stores in Feature Store
                                                2. Automated data (X) generator (Generative Models that can be controller with parameters or latent variables)
                                                    1. Sends data desires to Data generator service
                                                    2. Receives unlabelled datapoints and stores in Feature Store
                                        2. For Priority Requests: receiving data processing priority requests from training data pipeline
                                            1. Model-based Active Learning and then matching (needs to get model file corresponding to the specific training data pipeline)
                                            2. Sampling Methods (e.g stratified sampling, wighted/temperature sampling, importance sampling)
                                    2. DB to store:
                                        1. Annotation (can be more than just labelling) demands
                                        2. Priority requests
                                        3. Data Generated
                                    3. FrontEnd (Tools:
                                        (1) From scratch
                                            (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                            (2) Automatic template: screenshot-to-code
                                            (3) Implementation
                                                (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                                    (1) Code:
                                                        (1) Template Libraries: Grapesjs
                                                        (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                        (3) Supporting Libraries: 
                                                            (1) State management: Redux; 
                                                            (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                            (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                        (4) Telemetry
                                                            (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                            (2) Session Replays: OpenReplay
                                                        (5) UI Dev Environment: Storybook, One
                                                    (2) Build: Webpack; 
                                                    (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                                    (4) Build & Web Serve: 
                                                        (1) SSR: Nextjs, Astro, remix; 
                                                    (5) Code & Build: 
                                                        (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                        (2) Static Site Generators: Hugo, Gatsby, Quartz
                                        (2) Using task-specific high level tools
                                            (1) Dashboards: metabase, plotly, graphana)
                                        1. Monitoring automated data generator: shows inputs/outputs of the automated data generator to human, to see if it working correctly
                                        2. Human data generator: shows data we want to gather do data collectors (feature ranges), so that he can chose a valid X
                                3. Internal Workings. Can decide regions to label data based on:
                                    1. Labbeling prioritization
                                        1. Slices
                                            1. Low # of datapoints on a slice
                                            2. Low Model metric on a slice
                                        2. Model Uncertainty: High model output uncertainty (considering a calibrated model)
                                        3. Proximity to decision boundaries: datapoints near decision boundaries are more important
                                        4. Manifold: identify a manifold (subspace) where we need more datapoints automatically via model errors
                                    2. Sampling priritization
                                        1. Errors: choose points with largest model errors
                                        2. "Big impact on training: choose points such that the expected gradient is large or points where the model changes its mind the most about its prediction during training"
                                    3. Curriculum Learning
                            5. Featured Dataset Improvement
                                1. Featured Dataset QA 
                                    1. Data validation (aka data testing) (Tools: (1) Open source: (1.1) General: Great Expectations, Soda Core, Cleanlab, Evidently, Deepchecks, datachecks, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation; (1.2)CV: cleanvision; (2) Proprietary: Monte Carlo, Qualdo, acceldata, DataBand, Metaplane, Talend Data Quality, Anomalo, BigEye, Ataccama): checking if feature dataset satifies some constraints.
                                    2. Identify labelling mistakes (Tools: cleanlab)
                                        1. Wrong labels
                                            1. Automatic: Anomaly detection
                                            2. Manual: Label reviews
                                        2. Bad labellers
                                            1. Automatic: Anomaly detection
                                            2. Manual: Label reviews
                                2. Synthetic Data (Tools: DistillKit)
                                    1. Methods
                                        1. Data Augmentation (Tools: AugLy, nlpaug, TextAttack, uda, audiomentations) (makes it easy for you to produce new X' data, where the model needs it, from existing X data (e.g., for CV: new views of a scene)). Note: we provide integrations with third-party data augmentation services.
                                            1. Automatic Labelling
                                                1. For tabular data
                                                    1. ML models
                                                    2. Heuristic Functions
                                                2. For non-tabular data (files) (often called semi-structured (e.g., pdf) & unstructured (e.g., media))
                                                    1. ML models
                                                        1. LLM-generated labels
                                            2. New X
                                                1. ML Models
                                                    1. Foundation Models
                                                        1. LLM generated text
                                                        2. Img2Img Diffusion-generated images
                                            3. New (X,Y) Datapoints from old ones
                                                1. For tabular data: learn P(X=x,=y) and sample from it (But if we could model P(X=x,Y=y) wouldnt we already have the final mode? Well, technically yes, but in paractice the P(X=x,=y) here is a bit low-quality, but is sufficient do generate a lot of data with low bias. Additionally, we can model P(X=x,Y=y) using different approaches as to average out the biases of each data model. Think of it like this: our data models can be wrong, but if their errors everage out, then the expectation becomes similar to the real model.)
                                                2. For non-tabular data  (pften called semi-structured (e.g., pdf) & unstructured (e.g., media))
                                                    1. Rule-based: output-invariant tranformations: given a spacific model, get datapoins from Feature Store, apply output-invariant transformations to generate new datapoints and store them back in the Feature Store
                                                    2. Model-based: use ML model (e.g. LLM) to augment datapoints for you.
                                                    3. Noise-based: add noise to X
                                        2. Pure Synthetic Data Generation: new (X,Y) datapoints out-of-the-blue (Tools: bonito, textbook_quality, NeMo-Curator, distilabel, lilac, ataDreamer, airoboros)
                                            1. Using Foundation Models
                                                1. CV
                                                    1. Text2Image (Image Generation)
                                                    2. Text&Image2Image (Image Editing)
                                                    3. Tex2Video (Video Generation)
                                                    4. TexVideo2Video (Video Editing)
                                                2. NLP
                                                    1. Text2Text (LLM Generated Data)
                                                3. Audio
                                                    1. Text2Audio (Audio Generation)
                                                    2. Text&Audio2Audio (Audio Editing)
                                                    3. Speech2Speech
                                            2. Physical Simulator Data
                                    2. Components
                                        1. Backend (Tools: (1) Python: (1.1) HTTP API libraries (build them as a library for Dynamic HTTP Server Library to use): FastAPI; (1.1.1) REST (Old messy way very tied to DB principles): plain FastAPI; (1.1.2) RPC (New functional way that treats paths as functions, payloads as parameters, compresses data and fully embraces http 2.0)): gRPC + FastAPI; (1.1.3) GraphQL (New DB way that ignores vestige HTTP paths, treat payloads as lossened DB queries (not just reading, but also updating & adding data) payloads as parameters, compresses data and fully embraces http 2.0): add Ariadne + FastAPI; (1.2) Dynamic HTTP Servers (handles parallel socket connections, calling HTTP API libraries, error returns): Gunicorn (WSGI) with Uvicorn (ASGI) interface; (1.3) HTTP Reverse Proxies: NGINX; (2) Nodejs: (2.1) HTTP API libraries: Dont know any; (2.1.1) REST: plain FastAPI; (2.1.2) RPC: gRPC; (2.1.3) GraphQL: GraphQLjs; (2.2) HTTP Servers: HTTP Module; HTTP Module; (2.3) Single Dynamic HTTP Framework: Express, Nestjs; (1.3) HTTP Reverse Proxies: NGINX)
                                            1. Do automatic data augmentation
                                                1. Get data from data source (Tools: dlt) (Feature Store or ML DataLake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb)) 
                                                2. Do data augmentation
                                                3. Store new datapoints in data source (Feature Store or ML DataLake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb))
                                            2. Manage Manual Data Augmentation
                                                1. Communication
                                                    1. Send Frontend datapoints
                                                    2. Receive new datapoints from Frontend
                                                2. Processing (Tools: Dask is a great from non-tabular data processing)
                                                    1. Do User-guided data augmentation
                                                    2. Do notebook processing (Pyhton Interactive Interpreter does this job)
                                                        1. Receive cells
                                                        2. Run code in cells
                                                        3. Return output for each cell
                                        2. FrontEnd (notebook): enables users to interact with a dataset (Tools:
                                            (1) From scratch
                                                (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                                (2) Automatic template: screenshot-to-code
                                                (3) Implementation
                                                    (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                                        (1) Code:
                                                            (1) Template Libraries: Grapesjs
                                                            (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                            (3) Supporting Libraries: 
                                                                (1) State management: Redux; 
                                                                (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                                (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                            (4) Telemetry
                                                                (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                                (2) Session Replays: OpenReplay
                                                            (5) UI Dev Environment: Storybook, One
                                                        (2) Build: Webpack; 
                                                        (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                                        (4) Build & Web Serve: 
                                                            (1) SSR: Nextjs, Astro, remix; 
                                                        (5) Code & Build: 
                                                            (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                            (2) Static Site Generators: Hugo, Gatsby, Quartz
                                            (2) Using task-specific high level tools
                                                (1) Dashboards: metabase, plotly, graphana)
                                            1. Shows datapoints
                                            2. Have common transformations implemented already
                                            3. Let users code transformations
                                            4. Let user send final transformations
                                3. Data Imbalance Correction
                4. Feature System (Feature generation + Feature Store) (Tools: (1) open source: Feathr, Hopsworks OSS, Chronon; proprietary, Michelangelo Palette, F3, Featureflow, Griffin, Fabricator, OpenMLDB). Should also have feature ownserhip capability:every feature should be owned the practitioner that made it, and this should be transparent.
                    1. Feature generation: Data Pipelines (Tools: e.g., Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro, hamilton, sqlflow; (2) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo), mleap, metaflow, flyte, sematic; are great tools for Workflow/Pipeline Orchestration (work on top of the other tools): capabale of organizing Data Sources (e.g., S3) and Processing engines (e.g.,  spark/koalas, Dask or Ray) jobs as a high level DAG of tasks)) (Important: needs to interact with the Feature Store namespace inthe same namespace of the production model it is producing features for)  (Implements the pipeline that was manually built during Experimentation, but now in a production environement, with high performance)
                        1. Summary: Receives a computational graph for the features it has to build for a specific model version. Stages (the data pipeline is defined by a computational graph were nodes are artifacts that are usually information or computation files. Information files are datasets or configuration and computation files are intermediate models or scripts). A stage_{t} can receive upstream information from staage_{t-1} (1) and pass downstream information to stage_{t+1}. E.g., a model downstream receives the upstream information that an email (datapoint) is a spam (done by an ML classifier or blacklist script upstream). _Note:_ should be able to: given a featured datapoint, trace the upstream data used to build it (by using the computational graph).
                        2. Types
                            1. Training Data Pipeline (Lamdda Architecture). _Note:_ features are computed based on sliding ML Data Wharehouse datapoint windows. Therefore a lot of computation can be shared among features datapoints. This is called statefull computation, a good distributed processing engine (be it stream processing or batch processing) should provide you this. \\ If batch: You compute the "first" featured datapoint & then just keep patching it by doing 2 things: adding the influence of new raw datapoints (that are now part of the window) & removing the influence old raw datapoints (that arent in the window anymore). \\ If streaming: you keep an internal state of the features you want to compute, and for each event (raw datapoint arrival) you update the state, and you need to keep track of the events (raw datapoints) that will not be usefull for the next featured datapoint computation, because when you finish the first window, you need to remove their influence and then start doing the same thing for the next window (but the next window, and all other after, will be ready much earlier than the first one, because they are reusinng computation form previous ones!); Also important to keep state checkpoints in storage, in case some crash happens. _Note:_ alwyas remember to put timestamps on featured datapoints also, the time stamp of a featured datapoint will be the the nwest timestamp among the raw datapoints that were used to make it. _Note 2:_ a featured datapoint is made out of the last n raw datpaoints where n is not ordered by message arrival, n is ordered by timestamp. Often, messages with older timestamps arrive after newr timestamps (when the gap between timestamps is small) due to networking or other delays. So the stream processing tool must wait some time t for the arrival of delayed messages, this is a parameter that can be tuned. "Therefore, the results of stream computation tend to be approximate."
                                1. Builds: static, slow changing & fast changing features
                                2. Processing mode: Streaming (for making real-time features) and Batch (for making the rest of datapoints)
                                    1. Batch: for making featured datasets to train models. Will "fill in all the datapoint holes left by Streaming". Gets data in batches from ML Data Wharehouse.
                                        1. Datapoints to avoid: outliers/anomalies
                                        2. Datapoint priority (i.e., from which virtual datapoint distribution to sample from & then build it)
                                            1. Same dataset:
                                                1. By Model Monitoring needs: representative subset of most recent window of virtual datapoints
                                                2. By Active learning: request/Subscribe to Active Learning System for which data it should give processing priority (priority requests) & then use an algorithm to go from virtual featured datapoinnt --> raw data to compute similar datapoint
                                                3. By Data Quality metric (the data source put this metric on it as metadata)
                                                4. By Business Goals: request/subscribe to Analytics Subsystem for which data it should give processing priority: i.e. give priority to which topics. (e.g., give priority to paid users, more active users, etc)
                                            2. Between datasets: by business goals. Request/Subscribe to Analytics Subsystem for which model it should give processing priority
                                    2. Streaming: for making real-time featured datapoins for doing live Inference. Always favours newest virtual datapoints. Consumes streaming messages directly from the data collection stream transport.
                                3. Objectives
                                    1. Training
                                        1. Quality training set
                                        2. Processing Efficiency
                                    2. Inference: usefull featured datapoints for doing live Inference.
                                4. When is used: always
                            2. Inference Data Pipeline: subscribe to streams coming from the Prediction Manager containg raw timestamped requests that need predictions & the features it needs to build, then combines sync real-time data (that comes with the prediction message) with async data (transforming raw input data from ML data wharehouse or getting directly from feature store) and stream back to prediction manager & store in Feature Store. (Example Implementation Options. Option 1: use streaming data processing tool (e.g., Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast) in which the pipeline of tasks is done internally in the tools' dsitributed environment (as defined by you in driver code) & streams back to prediction manager. The good thing is that these tools are very optimized but the bad things is that the tasks need to be launched; Option 2: sequential services connected to each other (pipeline) via streaming and that output features on feature store & streams back to prediction manager. _Note:_ first service will be listening to inference data pipeline trigger along with the request via streaming, and once listened, gets data needed (from the message itself (real-time data) & from ML data wharehouse) & sends to the next data pipeline service) to make featured data on the fly (gets less used when ration avg_time_to_make_a_featured_datapoint/avg_rate_of_change_of_fastest_changing_feature is lower, becaue then you can just start getting from feature store, since it wont be so outdated).
                                1. Builds:
                                    1. Fast changing features (need a treshold on feature avg rate of change to define dynamic, this treshold shouldnt be too low as to make you keep real-time computing features with their value changing so little (wrt the last feature store value), and not too high so as to use old values that are very different from actual value. And this depends on the speed of your training data pipeline and on how fast features change over time (which can me measured with avarage derivative over a period of time))
                                    2. Real-time features: features that need to be computed based on the data that arrives with the prediction request/message
                                2. Processing mode: Streaming
                                3. Objective: Processing Speed
                                4. Side-effect: helps build new training data
                                5. When is used: when up-to-date features are not ready in Feature Store, upon stremed messaged arrived from Prediction Manager
                        3. Intermediate Models (models used to build features for other models) Sync
                            1. Register intermediate models with Model/Prompt Registry: say to Model/Prompt Registry that it only wants to be warned of any updates on models it is using to do processing.
                            2. Listen to intermediate model updates from Model/Prompt Registry (note: Model/Prompt Registry wont send messages ): if a intermediate model it uses for processing gets updated, it will receive a message from the Model/Prompt Registry with the updated intermediate model. Will have to recompute, and modify in Feature Store, all features that were built and stored in Feature Store in this specific dataset/namespace in the past, that depended direclty or inderectly on the output of the intermediate model that was changed. If decides to recompute feature in Feature Store, alert Feature Store of this (with the details of dataset, namespace and specific features), so that the Feature Store can lock any kind of attempt to get this data, until we are fineshed recomputing the features.
                        4. Data Injection
                            1. If Training Data Pipeline:
                                1. Batch: Schedule raw data injection in batches from ML Data whareshouse
                                    1. Time-scheduled (if there is new x amounts of datapoints to process)
                                    2. On new x amounts of datapoints trigger (will be listeining for this event)
                                2. Streaming: receive, indefinetely, streaming data from coming from ML Data whareshouse
                            2. If Inference data pipeline: receive, indefinetely, streaming data coming from prediction manager & get additional data from
                            ML Data whareshouse
                        5. Processing
                            1. Types of datapoints
                                1. If Training Data Pipeline:
                                    1. Features & label (natural labels from the environment)
                                    2. Just features (need humans to label, or label from the environment didnt arrive yet)
                                    3. Just label (label that correponds to an erlier datapoint of type ''Just features'')
                                2. If Inference data pipeline : just features
                            2. Validations
                                1. Raw Data Validation (gets data from ML data whareshouse) (Send validation result to Monitoring System (could be failing due to change of schema of the source data or data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb))) (Tools: JSON Schema, Great Expectations, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation)
                                    1. Raw schema validation
                                        1. Update when: new production raw schema version
                                        2. Methods
                                            1. Columns
                                                1. Existence
                                                2. Order
                                                3. Data type
                                            2. Rows: Number of rows
                                    2. Expectations
                                        1. Update when: new production raw schema version
                                        2. Methods
                                            1. Descriptive statistics
                                            2. Ranges
                                            3. Relationships
                                                1. Between dimensions (''e.g.,  DIM_1 > DIM_B'')
                                                2. Between raw ML datapoints (e.g.,  raw ML datapoint needs to be unique, or a values in a dimension needs to be unique; or rawDatapoint_{t-1}[time] < rawDatapoint_{t}[time])
                                2. Feature Store Schema Validation (make shure that result of our pipeline matches the scema the feature store is expecting, if not, send alert to programmer and send to (generally stream) Monitoring System)
                            3. Raw Data Wrangling
                                1. Change data types
                                2. Filtering (can use heuristcs (e.g keywords) or an ML Model 4 this)
                                    1. Core Filtering
                                        1. By Inapropriate content (discard single datapoint)
                                        2. By outliers/anomalies (discard single datapoint)
                                        3. By Troll user in some time window (discard all datapoints with User A in the specific time window the datapoint B from user A was found on)
                                    2. Deletion discarded datapoints from ML Data whareshouse
                                    3. Send POST request to Monitoring Service with: Model ID/version, X input, Explanation of why it was filtered (type of filtering)
                                3. Joining tables, Groupbys
                                4. Cleaning
                                    1. Missing values
                                        1. Types (there is a big spectrum between Dependent on features and Completely Random diictated by the correlation)
                                            1. Dependent on features --> can be modelled
                                            2. Missing because of some hidden feature of the datapoint (non realistic, out of range, etc) --> we need to use previous value in some way
                                            3. Completely Random --> cant be modelled
                                        2. Approaches
                                            1. Drop datapoints with too much missing values
                                                1. Advantages: Simplifies computation, avoids making non-realistic datapoints
                                                2. Disavantges
                                                    1. Dependent on features: biasing data
                                                    2. Completely Random and/or Missing because of some hidden feature of the datapoint: Overfitting or dont capture complex patterns
                                            2. Drop features with too much missing values
                                                1. Advantages
                                                    1. Dependent on features: removes confounding
                                                    2. Completely Random and/or Missing because of some hidden feature of the datapoint: simplifies computation, improves on curse of dimensionality
                                                2. Disavantages
                                                    1. Completely Random and/or Missing because of some hidden feature of the datapoint: Lose important predictive information
                                            3. Do feature inputation for missing values
                                                1. Modelling missing values
                                                    1. Advantages: make very realistic datapoints, leveraging relationships between features
                                                    2. Disavantages: computationally intensive
                                                2. Sampling from marginal distribution of the feature
                                                    1. Advantages: reasonably realistic datapoints
                                                    2. Disavantages: less acurate than modelling missing values and more computationally intesive than using espected value
                                                3. Using expected value of the marginal distribution of the feature
                                                    1. Advantages: reasonably realistic datapoints, very computationaly friendly
                                                    2. Disavantages: injects strong bias, because you are always putting same value
                                            4. Treat missing value as data (e.g., adding a category if is categorical feature or using a specific fixed value if it a numerical feature)
                                                1. Advantages:
                                                    1. Missing because of some hidden feature of the datapoint: can extract usefull information from them
                                                2. Disavantages
                                                    1. Dependent on features: Injecting confounding
                                                    2. Completely Random: Injecting noise
                                                    3. Missing because of some hidden feature of the datapoint: needs carefull design and strong domain knowledge
                                    2. Leakages: check for existing leakages and, if present, remove the corresponding features  (e.g x-ray images with radiolgist pen marks exaclty where the problem is, or ID of the datapoint giving away the target because of the non-random way the dataset was generated)
                                        1. Twoway road to target is forbidden: you feature can lead to causal arrows only in one direction to target. aka your target cannot be cause (directly or indirectly) by some of your features and cause other part of your features, because if this happens it means that you have leakge happening
                                5. Categorical encoding
                                    1. One-hot encoding
                                    2. Hashing
                                    3. Learned Embedding
                                    4. Label encoding (uses the label encoding function deduced from training data)
                                6. Transforming (e.g log scaling for exponential data)
                                7. Normalizing
                                    1. Objectives
                                        1. Normalizing features
                                            1. Put features on same standard
                                            2. Model based on relative values not absolute values
                                            3. Occupy as much of the feature space as possible
                                            4. Make loss lanscape more circular than eliptic which improves a lot gradient descent
                                        2. Normalizing target: avoid exploding gradients & numerical problems
                                    2. Approaches
                                        1. Without Assumptions on feature distribution
                                            1. MinMax
                                            2. Standard Scaler
                                        2. With Assumptions on feature distribution
                                            1. Z-score normalization (normal data)
                                8. Changing format (e.g for images, change PNG (4 color channels) to JPEG (3 color channels))
                            4. Wrangled Data validation (Tools: Great Expectations, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation)
                                1. Schemas
                                2. Descriptive statistics (should not be that diffferent between batches)
                                3. Ranges
                            5. Feature Engineering (Tools: Anovos)
                                1. Scripted (_Note:_ Automated after offline manual work) (_Note:_ send samples to Monitoring System after this step (before generation of hardy-interpretable features))
                                    1. Drop
                                        1. Features
                                            1. Non-informative features (are defined by low information gain == E_x[Entropy(P^(Y=y|X=x))]/Entropy(P^(Y=y)) where P^(Y=y|X=x) and P^(Y=y) are estimates of P(Y=y|X=x) and P(Y=y) respectively) (Tools: ppscore)
                                            2. Feature very dependent on other features
                                        2. For training data pipeline: Datapoints
                                            1. Removing outliers/anomalies
                                            2. Class with much more examples than another class
                                    2. Modify existing features
                                        1. Discretizing: binning features (generally numerical) into categories
                                    3. Create new features as a function of existing features (will have dependent features problem if you dont drop the features you used to build the new one)
                                2. ML methods (hardy-interpretable features)
                                    1. Automatic Feature Generation
                                    2. Dimensionalty Reduction 
                                        1. PCA
                                        2. SVD
                                        2. Autoencoders
                                    3. ML Models as feature builders (upstream models)
                            6. Featured Data validation (Tools: Great Expectations, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation)
                                1. Schemas
                                2. Descrptive statistics (should not be that diffferent between batches)
                                3. Ranges
                            7. Update Feature Store
                                1. Staless features: Store features in Feature Store
                                2. Statefull features: Modify features in Feature Store
                            8. If inference data pipeline (Processing a request coming from the Prediction Manager)
                                1. Stream featured datapoint back to Prediction Manager
                                2. Store featured datapoint in the Feature Store (_Note:_ will store in the real-time DB of the Feature Store, & then this datapoint gets synced to the offline DB (which store the majority of the dataset) via Batch jobs)
                    2. Feature Store (Tools: (1) feature stores: (1.1) Just interface that uses your existing storage backends: Featureform; (1.2) Has storage itself: Feast, Tecton, chronon; (2) tools to help you build feature stores: Wicker) (a dual DB, both storage & in-memory parts): will hold the final datasets: featured datapoints for multiple clients to query/modify/store.

                    _Note:_ must store datapoints that have higher chance of being queried (e.g., newest datapoint for each dataset) in a real-time (in-memory) DB (e.g., if you want key-value: Redis; or if you want SQL then RediSQL) to be faster; while the rest is on offline DB using storage; and the datapoints in the in-memory DB constantly get updated (synced from the offline DB). Generally it is based on a key-value pattern (because youn dont need the flexibility overhead of SQL, because you are going to store & get rows entire rows). 
                    
                    _Note 2:_ when inference data pipeline stores a datapoint, it uses the real-time (in-memory) DB to do this, because it is faster; thus these datapoints store dby the inference data pipeline in the real-time DB needs to be sync (Tools: bitsail) periodically to the offline DB through batch (ETL) jobs.
                        1. Organization
                            1. Separations
                                1. Separation of featured datapoints for different ML tasks/models
                                2. Separation of featured datapoints with natural and artificial (needs human) labels
                                3. Separation of labelled (prediction context (e.g., docs retrieved by RAG retriever), Ys, proxy Ys & explanation ground truths (_Note:_ explanation is text data and can get big, so what is actually stored is a pointer to get the file in another location ((1) Data Lake: open source: (1.1) Kylo, Delta Lake; (1.2) managed: Azure Data Lake Storage, AWS Lake Formation + S3; (2) object storage: (2.1) open source: minio, ceph/rook, storj, swiftstack, longhorn, DAOS, openebs; (2.2) managed: s3; (3) distributed file system: HDFS, Gluster, moosefs), also, Y does not need to be a scalar)) and unlabelled featured datapoints
                                4. Separation of featured datapoints per ML task and per mode of operation (online or offline)
                                5. Separation of datapoints using old vs new schema in a dataset (if new schema has > n datapoints. Give them to clients, else continue giving old schema)
                                6. Separation of featured datapoints w/o prediction (<y_hat, y_hat_prob> (y_hat_prob is usefull to know if we need to retrain using this datapoint as part of training data, if y_hat_prob to very hugh, we dont need to)), featured datapoints with prediction (<y_hat, y_hat_prob>) (along with timestamp of  prediction request)
                                7. Separe location for needy features: features that arent in any dataset
                                8. Separation of traditional vs generative datapoints
                                    1. Traditional datapoints: 
                                        1. Have a label column which stores a variable or small data structure
                                        2. All datapoints follow the same structure
                                    2. Generative datapoints:
                                        1. Training datapoints have a label column which stores a pointer to a large data strcuture or file
                                        2. Inferece dataponts have a label column, a human judgement column and properties columns. The human judgement column stores a variable representing how good the generative's model output was by some human evaluator (datapoints with very positi judgment can be used for training later, and the other datapoints will need to be labelled). Property columns are for automatic assigment of metrics that identify some desired property the output (e.g., text lenght, perplexity, factual correctness, forbidden words), and the goal is that multiple of them can proxy a good output evalautor 
                            2. Extra columns (NaN if not present yet)
                                1. Collection info. Collection time and label collection time: collection time (when featured datapoint X was collected) and label collection time (time when label Y was gathered). Also, label source: which annotator, or if auto-labelling, which autolabelling system.
                                2. Group info: group IDs. These are columns that indicate that the datapoint belong to a certain group. They might take two columns each: one for value and another one for which group (e.g., groups with similar predictive acc)
                                3. Inference info: target estimate, explanation estimate, inference time, Client JWT/Cookie (to get session), ID & location (Extracted from IP adress)
                                4. Monitoring info: outlier?
                                5. output (y_hat and y) denromlization factors: how to denormalize the normalized model output.
                            3. Pointers: files (e.g., text, image, video, 3d model) dont fit into tabular DB, so a pinter to how to get them is stored. These can be part of X or Y.
                                1. Pointer to file itself: They are stored in external storage, more specifically ML Data Lake. The storage location is defined in a config file & used by the Raw2RawML data pipeline to store these ready files & used by the feature store to get them
                                2. Pointer to embedding in the Tensor DB.
                        2. Metadata
                            1. Data types of each feature
                            2. If known: noise of each feature
                            3. Dataset Explanation (Docs)
                            4. Preprocessing steps used for the dataset assembly
                            5. Configuration versioning (what config was modified and at which time) (Tools: git/dvc/fds, xvc, dud)
                            6. Track which models are using which <dataset A, version B, patch C> of model namespace C using training data pipline A.B
                            7. Training Data Pipeline configuration (that is generating the features of some <dataset A, version B>) & link to repo of the training data pipeline
                            8. Datasets statistics per feature (e.g., mean, variance and # of NaNs)
                            9. Per datapoint: processing priority of the datapoint (Data Sources attach processing priority to their datapoints when they publish messages, that will endup in the Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb))
                            10. Dataset contains or not synthetic data and how much if so
                            11. Real world labels or knowledge distallation originated labels
                        3. Validation Smoke Tests
                            1. Policy
                                1. When to run: run on every insert
                                2. What to do if identified anormalities:  log/send to (generally stream) Monitoring System
                            2. Testing suite
                                1. Schema validation
                                    1. Update when: new production schema version
                                    2. Methods
                                        1. Columns
                                            1. Existence
                                            2. Order
                                            3. Data type
                                        2. Rows: Number of rows
                                2. Expectations (needs to be updated when there is a new schema version)
                                    1. Update when
                                        1. New production schema version
                                        2. Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s (but generally, people analyze basic expectations that are very resilient, so this shouldnt be the major issue)
                                    2. Methods
                                        1. Range of values (''min, max, or median values of a feature are in [a, b]'' or ''Categorical data belongs to a predefined set'' or ''All values of a feature satisfy a regex'')
                                        2. Statistics (e.g '''the' is most common word in English'')
                                        3. Relationship between features (''e.g FEATURE_1 > FEATURE_B'')
                                        4. Relationship between datapoints (e.e.g entire datapoint needs to be unique, or a column values needs to be unique; or datapoint_{t-1}[time] < datapoint_{t}[time])
                        4. Capabilities
                            1. SQL Interaction for data in storage
                                1. Validation and SQL-like storage or modification of featured data
                                2. SQL-like retrieval of featured data
                            2. Stream new featured datapoints (generally for the offline offline  (batch) Inference Service  (precomputed predictions) to consume). *Note:* if the feature store you are using doesnt natively support streaming, you can use a change-data-capture tool (Tools: Debezium) to connect to it and produce these streaming messages.
                            3. Trigger ''new x featured labelled datapoints are available'' to Batch Offline Learning retraining jobs
                            4. Upon inference info & delayed label arrival: stream datapoints to Monitoring System
                            5. Periodically delete non-totally-static (that become useless to our models after some time) features after a specific time window
                                1. Discover the time window: make request to ML Metadata Management System to get feature specifications of each model
                                2. Validate with Humans: send alert message to ML Monitoring System with the features it wants to delete
                                3. If ok with humans: send to Trash Storage System
                            6. Versioning/Namespaces (Tools: git/dvc/fds, xvc, dud): some models or data scientists might want to get the dataset in slighlty different ways, but haveing a different data pipeline for each of these becomes costly and store multple very a-like datasets in the feature store. Therefore, the feature store can use namespaces. It stores the core dataset and patches of it, each patch is from a different namespace, avoiding duplication of features. 
                            
                            The patches are not data patches (storing literally the differences in data); these are computational patches -- store how to change (_Note:_ can be models themselves. Common is settings where you compose multiple models as a pipeline. But the model is not stored itself, rather a pointer to it on the Model/Prompt Registry where you can get always the most up-to-date one).

                            Computational patches need to stored in a append-only database, because that is the only thing we need to do.
                            
                            Models and data scientists sit within a namespace when they interact with the feature store. Namespaces can be hierarchical, a typical use would be the owner of a namespace be a model/version or a data scientist ID. Important: probably needs an internal distribted processing engine to apply the patches that are basically stored transform functions wrt the default dataset. 
                            
                            _Note:_ After data is stored, it gets transported to each namespace after applying transform_namespace_i(data) to it, where transform_namespace_i is a function that tranforms default data from the feature store to the format the namespace is using. E.g., when Data Scientist changes training pipeline, then the all the datapoints in the feature store get updated (in reverse chronological order), but then models trained the older version of the feature store wont work weell or work at all in production, plus you cant compare these models with new ones. Here computational patches come in very handy, the older verion of the feature store can be recreated by using the computational patches store in their namespaces.

                            _Note 2:_ if the changes are too heavy then patches are not recommended. A new training data pipeline should be made generated. creating a fresh featured dataset. However, feature system should check feature generation pipelines for duplication of features, and substitute processing by pointers to existing features.

                            7. Receive change in configuration by programmer or another system with credentials: mainly dataset schemas
                            8. On schema/logic change of the features or target of a dataset consumed by certain model namespace: send to (generally stream) Monitoring System
                            9. Ability to lock access to specific <features, namespaces, datasets> combinations. E.g.,  this is very usefull if Training Data Pipeline has to recompute some features, and in the mean time the retraining job tries to retrain a model using this data. If the retraining job got the chance of retraining, it would produce a model that will suffer from Input Shift in the inference data pipeline (because un upstream model that produces features for it, got changed).
                            10. Data privacy.
                                1. ""Assign roles to users and configure which data should be accessible by which roles.""
                                2. ""Automate masking of sensitive, PII data.""
                                3. ""Propagate data policy – if a feature is derived from a sensitive column, it should also be marked as sensitive.""
                            11. Keep track of (like any other DB):
                                1. Writes to the feature store since last backup (log it to Logging System)
                                2. Backups (store in Data Lake)
                                3. Migrations: timestamped changes in feature logic or schema for a specific dataset (Store internally). _Note:_ this is very important for guaranteeing that there isnt training-serving skew (i.e. ensuring model trained on the same schema/logic that it is doing inference on) & for comparing models (make a model that was trained on old logic/schema comparable to a model that was trained on new logic/schema).
                            12. UI
                                1. Discover: let Data Scientists navigate model naspaces, the features they are using, what these features mean, the data sources used to build its dataset, the history of changes to the featured dataset, models that are borrowing features from it (have pointers to it), features that are borrowed from other featured datasets/models, view computational patches in a nice way.
                                2. Analyze: plot, compute analytics and/or statistical tests of features (e.g., avg, ranges, dsitribution, missing values, unique values, etc) and/or beween features (e.g., feature dependency with each other and/or with label, rank features by importance). Could even suggest new features based on new label (defined as features from another model's featured dataset or processing of raw ML datapoints) you choose.
                                3. Act: let Data Scientists borrow features from other featured/datasets/models, make small processing adjustments to the featured datapoints, send manually a computational graph to update some trainning data pipeline that is making some specific featured dataset for some specific model, modify computational patches.
                                4. Retrieve: when making a new mode, often you want to get some features from some exsiting model's dataset, then some tother features from another model's dataset & son on to assembly your working dataset. Doing this manually is cumbersome. The feature store should let you easily (either throgh API or GUI) combine assemble custom datasets, using features from a lot of different daatasets & load them as a datastructure (e.g., pandas df, polars df, or spark/koalas RDD) to start working.
                            13. Authentication/User roles: specific capabilities can only be done by specific user roles. User roles are authenticated via public/private key distribution.
                            14. Internal Optimizations
                                1. Feature reuse: identify features that are the same, stop doing same processing twice and just point all other features to one feature. SOmetimes, the features are not exatcly the same, but are very similar. In this case, it should store it in an optimization possibilites storage that can be access with a frontend or API/backend combo, where users deny, allow or make updates the allow these optimizations to take place internally.
                                2. Computational Graph (the thing that makes data pipelines) optimization. The goal is to make a computational graph the does the eaxact same time, but with less computation (remove redundant computation).
                            15. Online Testing:
                                1. Makes it easy for you do define A/B feature tests, where you can set trafic % for each one. You can test between entire data piplines or just feature patches.
                                2. Allows real-time change of traffic %.
                            16. Backups & Logs: just like a simple DB it should do periodic backups to another location, along with storing writes & migrations (schema changes) since last backup.
                            17. Feature Imputation: the featured datapoints that will be available to models cannot have missing data, therefore an imputation protocol must be defined & implemented for every feature (e.g., you might want to use the mean (over last n datapoints) value if the feature is not crucial, and you might feed an error message explaining the lack of the feature to the prediction manager if the feature is crucial)
                            18. Privacy lock for specific feature columns
                            19. Normalizing features (async or sync)
                                1. Objectives
                                    1. Put features and target on same scale
                                    2. Model based on relative values not absolute values
                                    3. Occupy as much of the featureXtarget space as possible
                                    4. Make the loss function be more simetric which facilitates convergence
                                2. Approaches
                                    1. Without Assumptions on feature distribution
                                        1. MinMax
                                        2. Standardizing
                                    2. With Assumptions on feature distribution
                                        1. Z-score normalization (normal data)
                        5. Components
                            1. Storage:
                                1. Main DB: SQL DB (paths to non-tabular data are string columns)
                                2. Auxiliary DB: No-SQL DB for sotring non-tabular data
                            2. Compute:
                                1. Workloads: Backend that handles: DB interation, Validation Smoke Tests and Capabilities
                                    1. Streaming: some capabilities
                                        1. Subscribe & produce streamed messages> datapoints, triggers
                                        2. Do stream processing
                                    2. Batch: DB interaction, Validation Smoke Tests & some capabilities
                                        1. HTTP server
                                            1. Feature Store clients that want to interact eith Feature Store: receives POST requests to insert data and GET request to get data (support directl SQL queries)
                                            2. Feature Store managers that want to configure Feature Store: receive POST requests with desired configuration changes
                                        2. HTTP client:
                                            1. Alert requests
                                            2. Triggers
                                            3. Send data to Monitoring System
                                        3. Internal Routines
                                            1. Feature Validation
                                            2. Data deletion
                                            3. DB organization
                                2. UI: FrontEnd (Feature Catalog) (Tools:
                                    (1) From scratch
                                        (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                        (2) Automatic template: screenshot-to-code
                                        (3) Implementation
                                            (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                                (1) Code:
                                                    (1) Template Libraries: Grapesjs
                                                    (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                    (3) Supporting Libraries: 
                                                        (1) State management: Redux; 
                                                        (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                        (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                    (4) Telemetry
                                                        (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                        (2) Session Replays: OpenReplay
                                                    (5) UI Dev Environment: Storybook, One
                                                (2) Build: Webpack; 
                                                (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                                (4) Build & Web Serve: 
                                                    (1) SSR: Nextjs, Astro, remix; 
                                                (5) Code & Build: 
                                                    (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                    (2) Static Site Generators: Hugo, Gatsby, Quartz
                                    (2) Using task-specific high level tools
                                        (1) Dashboards: metabase, plotly, graphana)
                                    1. HTTP Server to serve Frontend. FrontEnd: displays all the features, datasets, namespaces and client models like shop, where you can rank or filter by any of these. Also lets you sample some feature datapoints and compute get statistics of it.
                                    2. Application Server that serves FrontEnd and other clients can consume. HTTP API: returns all the features, datasets, namespaces and client models like shop, where you can rank or filter by any of these. Also lets you sample some feature datapoints and compute get statistics of it; in the from of a json response.
                    3. Tensor DB (Tools: (1) Vector DBs: pgvector, Weaviate, Chroma, Milvus, Qdrant, ElasticSearch, HNSWLib, NMBSLib, Active Loop, lancedb, Marqo; (2) Similarity Search Libraries used to build your vector DB: Faiss, vearch, Vald, semantra, annoy): for dealing with embeddings
                        1. Capabilities
                            1. Should support tensor embeddings: not just vectors, but any tensor higher-order tensor (e.g., you might want to embed images as 3d tensors 
                            2. Should link embedding tensors to their data via a pointer to how to get the data (to be faster, should store the data in the DB aswell).
                            3. Should be able to perform basic embedding-based computation like:
                                1. Search for most similar embeddings relative to a client-provided embedding and apply filters: known as filterable aproximate KNN (and also should support ignoring syntax of text is this similary search (e.g., if client provided a question, the fact that a pience of text in the DB is not a question should not be a factor)). Idea: could also try to estimate the confidence in the knowledge expressed by the text (e.g., affirmation have highconfidence & questions low confidence); searching could then filter by confidence. 
                                2. Clustering of embeddings
                            4. Separations:
                                1. A separate tab for each Embedding Model used (e.g., if an CLIP model was used to embe images & text into the same representation, the embeddings that come from this model will all be stored and used to retrieve other embeddings in the same logical tab)
                        2. Usage
                            1. Raw2RawMLnon-tabular Data Pipeline gets embeddings, stores them here & stores pointer to embedding in the Feature Store as a feature
                            2. Feature Store queries VectorDB on a regular basis to make recent datapoints available in real-time in its in-memory DB
                            3. LLMs (Retrieval-augmented LLMs) can use it for:
                                1. Long-term memory: always divide chat history into chunks & embedded each of them, then at innference time search for chunks similar to input and feed thes most similar chunks, is some prompt format, to the LLM
                                2. Knowledge-base: LLM learns to write vector-db queries to get knowledge when it needs it
                                3. Cache: to store common <x:text,y:text> tuples (e.g., questions about historical or geographical facts), so that at inference time, if we get a very similar x, we can just fetch the y directly from the DB, without calling the LLM API.
                            (multiple filter maps) and videos as 4d tensors (multiple 3d tensor embeddings) to rpeserve spatial & temporal information)
                5. Experiment Tracking System (Tools: (1) open source: sacred, Tensorboard, Aim, MLflow tracking, dvc/fds + CML, Pachyderm, truelens, fasttrackml, MLTRAQ; paid: W&B, dvc + DagsHub)
                    1. Fundamentals:
                        1. Simple pattern: API-based Experiment Tracking
                            1. Backend that talks to clients via HTTP API (Tools: (1) Python: (1.1) HTTP API libraries (build them as a library for Dynamic HTTP Server Library to use): FastAPI; (1.1.1) REST (Old messy way very tied to DB principles): plain FastAPI; (1.1.2) RPC (New functional way that treats paths as functions, payloads as parameters, compresses data and fully embraces http 2.0)): gRPC + FastAPI; (1.1.3) GraphQL (New DB way that ignores vestige HTTP paths, treat payloads as lossened DB queries (not just reading, but also updating & adding data) payloads as parameters, compresses data and fully embraces http 2.0): add Ariadne + FastAPI; (1.2) Dynamic HTTP Servers (handles parallel socket connections, calling HTTP API libraries, error returns): Gunicorn (WSGI) with Uvicorn (ASGI) interface; (1.3) HTTP Reverse Proxies: NGINX; (2) Nodejs: (2.1) HTTP API libraries: Dont know any; (2.1.1) REST: plain FastAPI; (2.1.2) RPC: gRPC; (2.1.3) GraphQL: GraphQLjs; (2.2) HTTP Servers: HTTP Module; HTTP Module; (2.3) Single Dynamic HTTP Framework: Express, Nestjs; (1.3) HTTP Reverse Proxies: NGINX)
                                1. Clients send experiments
                                2. Clients retrieve specific experiment setup
                            2. Databases
                                1. Small Database (Tools: Postgres): to store experiments. Can Store: <commit ID of the experiment, experiment commit, configs used, artifacts used, training history (loss states, optimizer states and model states (aka checkpoints)) artifacts generated, resulting metrics>. To make browsing experiments easier, can store direcly experimentation metadata, metrics, hyperparameters used, configurations (e.g., random seeds for any random operator: for making training batches, weight initialization, dropout, etc) instead of having to go trough the repo & artifacts to get these.
                                2. Artifact Stores (Tools: Minio) (if ypu dont have artifact stores you can simply use object storage like S3). Stores artifacts themselves. These are accessed via version paths. When you
                            3. FrontEnd (Tools:
                                (1) From scratch
                                    (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                    (2) Automatic template: screenshot-to-code
                                    (3) Implementation
                                        (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                            (1) Code:
                                                (1) Template Libraries: Grapesjs
                                                (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                (3) Supporting Libraries: 
                                                    (1) State management: Redux; 
                                                    (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                    (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                (4) Telemetry
                                                    (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                    (2) Session Replays: OpenReplay
                                                (5) UI Dev Environment: Storybook, One
                                            (2) Build: Webpack; 
                                            (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                            (4) Build & Web Serve: 
                                                (1) SSR: Nextjs, Astro, remix; 
                                            (5) Code & Build: 
                                                (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                (2) Static Site Generators: Hugo, Gatsby, Quartz
                                (2) Using task-specific high level tools
                                    (1) Dashboards: metabase, plotly, graphana)
                                1. Show experiments that were done (each one with all related information), can visualize, compare, rank and filter them (e.g., by metrics & time). Allows user to define a custom evaluation function on top of experiment metrics, so that they can have a single final metric to pick the best experiment.
                                2. Allows users to delete experiments from the database
                                3. Allows users to create note son experiments
                                4. Export
                                    1. Experiment specification (e.g., requirements.txt file, virtual environment config file or container image)
                                    2. Reports in svg/pdf format
                    2. Advanced
                        1. + Container-based Experiment Tracking (Tools: KitOps): store an image that reproduces the experiment, along with metadata of the config state of all other systems/components that were dependencies of that experiment (e.g., config state of the feature system). With this metadata stored along side the image itself, we can not only reproduce the dev environment, but also the system's point-in-time correct state (of course, given config state of systems we then need to do backfilling in order to get the point-in-time correct data, following sequentially the flow of data)
                        2. + Repo-based Experiment Tracking (Tools: dvc/fds): integrate with dvc/fds and track experiments as code commits: you will have the branch visualization where each branch is a different experiment idea, each commit is an experiment version of that idea. Each commit has these things that makes it actually an experiment: <experiment type, config files defining the experiment, experiment metadata, documentation explaining this experiment hypothesis, command to run to generate artifacts, command to run to evaluate artifacts & generate metrics, location of artifacts, labels of artifacts, location of metrics, labels of metrics, experiment trace> + dev image pointer that repdoduces the generated artifacts which give the same metrics + system config state metadata. 
                        
                        Notes: 
                            1. Data scientists should mark their commit with "to be tracked: {yes, no}" as an experiment, he should put yes after ding some local testa and evaluations and have a bit of confidence (e.g., that he didnt amek a data leakage error).
                            2. Experiments can be of 2 main types (defined by the engineer): "production-not-going" or "production-going". "Production-going experiment" are not meant to fuel prediction serivce ci/cd, while "production-going" are. "production-not-going" are used to arrive at decent production-going" experiments.
                        
                        Experiments can be changing: (1) Data: (1.1) Manually curated data, (1.2) Data Pipelines (e.g., feature generation pipelines); (2) Model (new model file); (3) Code: (3.1) Runtime code, (3.2) Infrastructure code, (3.3) Generalized CI/CD (Workflow Orchestration or Main Pipeline) code; (4) Config. 
                        /
                            A good pattern is:
                                1. Give each of these its sub main branch (with someone as admin of that branch accepting PRs)
                                2. Generate PR from these sub main branches to the actual main, pull from actual main to these sub mains, and experiments themselves are actually branches of one of these submains. 
                                3. Each sub main has its own mini CI/CD were the output of CD is generating a PR for the actual main branch.
                                4. The actual main branch doesnt have only 1 CI/CD (like it traditionally does), it has 1 CI and 5 separated lightweight CDs: raw data CD (output is new raw data in data sources), code CD (output is new pods in production clusters), config CD (output is new config in system state store), pipeline CD (output is new pipeline in pipeline/graph registry) and model CD (output is newrtifact History Store model in Model/Prompt Registry). The data CD is triggered when the PR comes from the data branch, the code CD is triggered when the PR comes from the code branch and so on... This process makes it possible to experiment separately on data, code, config & models; while ensuring each of these parts works well together with the others. 
                                5. You can go further down the git tree rabbit hole and make the structure more fine grained also (e.g., expandining infrastructure code into data collection system code, feature system code, experimentation code, and monitoring code)
                        3. Ability to transpile source code (in case multiple teams are collaborating and might not use the same language or framework)
                            1. From one language to another
                            2. From one framework to another
                6. Data Stores
                    1. Raw Dataset Store (Tools: Pachyderm, Delta Lake, LakeFS + Cloud Storages, Nessie, versioned DBs): serves, stores and versions raw datasets. Can also be more of an OLAP DB and provide a bunch of dataset processing operators for the user to use, instead of the user having to get the whole dataset do processing separetly and store the whole dataset again. OLAP reduces communication time and makes it easier for the user; however couples processing with storage making it: "lock" the DB for transactional queries that need to be super fast, not flexible for on-the-fly db structure changes (e.g., adding/removing rows or tables), worse for scaling, worse for different teams working in parallel.
                    2. Artifact registries (Tools: Minio) (Note: all of them need to produce events to the message broker or streaming transport when their artifacts are changed or new ones are added, so that the workflow orchestrator can trigger tasks to handle update the system according to these new changes) (Note 2: its a read-only DB)
                        1. Model/Prompt Registry (close relation with experiment tracking) (Tools: MLflow Model/Prompt Registry, modelstore, ModelDB, optscale, aiconfig, KitOps, valor)
                            1. Components
                                1. Backend that talks to clients ((implements HTTP for general things & FTP for large model files) (Tools: (1) Python: (1.1) HTTP API libraries (build them as a library for Dynamic HTTP Server Library to use): FastAPI; (1.1.1) REST (Old messy way very tied to DB principles): plain FastAPI; (1.1.2) RPC (New functional way that treats paths as functions, payloads as parameters, compresses data and fully embraces http 2.0)): gRPC + FastAPI; (1.1.3) GraphQL (New DB way that ignores vestige HTTP paths, treat payloads as lossened DB queries (not just reading, but also updating & adding data) payloads as parameters, compresses data and fully embraces http 2.0): add Ariadne + FastAPI; (1.2) Dynamic HTTP Servers (handles parallel socket connections, calling HTTP API libraries, error returns): Gunicorn (WSGI) with Uvicorn (ASGI) interface; (1.3) HTTP Reverse Proxies: NGINX; (2) Nodejs: (2.1) HTTP API libraries: Dont know any; (2.1.1) REST: plain FastAPI; (2.1.2) RPC: gRPC; (2.1.3) GraphQL: GraphQLjs; (2.2) HTTP Servers: HTTP Module; HTTP Module; (2.3) Single Dynamic HTTP Framework: Express, Nestjs; (1.3) HTTP Reverse Proxies: NGINX)
                                    1. Serving requested model files. Note: should also be able to break the model into multiple chunks, and serve ech chunk separetley. This is important for very bug models, where the runtime cant simply put the whole model into memory.
                                    2. Storing requested model files to be stored (can contain forced model state (e.g., when a sceham changed and then you need to upload a new model file, that oyu want that immediately be retrained))
                                    3. Rollback/promotion of models requests
                                    4. Deleting models according to requests
                                2. DB that stores all things model (doesnt have to be a database, to make it simpler you can start with putting all of this in a big file stored in S3)
                                    1. Model itself
                                        1. Formats
                                            1. Lang agnostic formats: ONNX, GGML
                                            2. Language dependent formats: pickle, plain python
                                        2. Code (Often both types are present, with .so being the core model execution kernels and interpreted language library as runtime or just an interface)
                                            1. Hardware independent (Interpreted) code: interpreted language library (e.g., python library)
                                            2. Hardware dependent (Compiled) code: .so library for some target hardware.
                                    2. Other model-related artifacts
                                        1. Prompt function/engine (if LLM-based): python function that receives as input the user query and outputs the final prompt and alternatives prompts (e.g., chanign order of lists, using synonyms, etc)
                                        2. Prediction filter: prediction filter file (defines computation on how to filter the prediction of a specific models)
                                    3. Lifecyle data:
                                        1. Only for prompts & filters: 
                                            1. Model version it is linked to
                                            2. Only for prompts:
                                                1. Model API config
                                            3. Prompt config
                                        2. Only for retrievers:
                                            1. Prompt version it is linked to
                                            2. Retiever-specific config (e.g. data source and chunk size)
                                        3. Only for ml models
                                            1. Model Training data
                                                1. Link to a specific experient in the Experiment Tracking System (which will contain link to training data, size of training data, etc)
                                                2. Training/Retraining
                                                    1. Method
                                                    2. Hyperparameters
                                                        1. Fixed
                                                        2. Scaling (Optimal hyperparameters and which parameters are frozen, as a function of dataset size )
                                                    3. Retraining Policy:
                                                        1. Dependency based: Retrain when Upstream model changed
                                                        2. Time-based: retrain in fixed time intervals
                                                        3. Data Distribution shift (Note: sudden shifts are often indicative of data bugs) based: how much covariate or label shift is necessary to require retraining
                                                    4. Last Training/Retraining
                                                        1. Time
                                                        2. Link to container image (in private training image registry)
                                                3. Origin:
                                                    1. Experimentation: link to Last model: model file it used as hot-start when it did retraining
                                                    2. Retraining (the process that generated this model)
                                                    3. Pretrained Model (with the location it was fetched from and the licence)
                                                4. Season of the model: some season or None (for processes without data distribution seasonalities)
                                                5. Training time under some training context specification (e.g., distributed training with 3 pods on a public cloud cluster with 3 EC2 instances, each with 16GB of memory and 2 NVIDIA Volta GPUs on them, using Pytorch distributed)
                                        4. Only for ensembles
                                            1. Pointers to models
                                            2. File describing how to compose the models together
                                            3. Training details: which models were trained together
                                            4. Dependency between, and true predictive power for, averaging models
                                        5. For all
                                            1. Evaluation
                                                1. Passed or failed Evaluation suite
                                                2. Evaluation details
                                                    1. Sanity hold out test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) (for validation in deploy)
                                                    2. Evaluation Scores: List of all the evaluations the model was subject to, with details on how to reproduce it along with scores
                                                        1. Corrected predictive power Metrics (e.g., Accuracy) for slices
                                                        2. Explanability metrics (e.g., global feature importance)
                                                        3. Inference time under some inference context specification (e.g., running locally on 4.2GHz 2 core Intel CPU with 8GB RAM running Ubuntu version 23.04 under 20% of processor/ 30% of memory/ and 10% of adapter usage; deployed Inference Service or model being parsed at runtime by prediction manager on 4.5GHz 2 core Intel CPU with 16GB CPU Memory + 2 1024 core NVIDIA GPU with 16GB GPU memory & 1k b/s network adapter with specific physical communication setup (cables & switch) running Ubuntu version 23.04 under 0% of processor/ 0% of memory/ and 0% of adapter usage. Ideally this would be the same context as it is deployed in production.)
                                                        4. Fairness
                                                        5. Security
                                                        6. Calibration
                                                        7. ...
                                                3. If current_model==yes: evaluation schedule (can be defined in terms of time or numer of retrains done in between)
                                                4. For prompts:
                                                    1. Validation against templates & regex
                                                    2. predictive power: measuring distance of outputs wrt test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) targets
                                                        1. Via distance between embeddings
                                                        2. Via another LLM
                                                    3. Evaluation of prompt and output innapropriateness (e.g toxicity, trolling, private data being exposed, intellectual property being robbed))
                                                    4. Sensitivity Analysis (sensitivity of output quality & size/price)
                                                        1. Prompt changes (how output changes if prompt changes a little)
                                                        2. LLM changes (How output changes if LLM is changed or the same LLM but with different config)
                                                    5. Prompt injection
                                                        1. Integraty attacks
                                                        2. Privacy attacks
                                                        3. Availability attacks (for agents)
                                                    6. Output size/cost
                                                    7. Steerability (evalauting prompt importance models, aka feature importance for LLMs, the goals is to estimate how the LLM can be steered, which type of prompt make it change the output in a certain way)
                                                    8. Stochasticity: make a bunch of llm calls with the same prompt, and see measure output variability
                                            2. Deploy
                                                1. Hyperparameters and/or Configuration
                                                2. IO Filter files
                                                3. Model size
                                                4. Input (features) & Output (targets) specification (formats/shemas)
                                                5. Type of deploy (e.g., compiled Inference Service, interpreted Inference Service, federated learning, etc)
                                                    1. Single-machine
                                                    2. Distributed
                                                        1. Distributed Inference (e.g., when model is too big and you have to break it up into submodel chunks) (Tools: Colossal AI, DeepSpeed)
                                                        2. Distributed Continual Learning (e.g., Federated Learning)
                                                6. If deployed: pointer to containers that make up the Inference Service pod
                                                7. API speification it will be deployed under
                                                8. If deployed under an inference data pipeline  (applies to multi-prediction-service models): which pipeline (ID of the pipeline) and the exact place it is in the pipeline
                                                9. Which services (frontends or backends) consume this model
                                                10. Non-ML fallback files (can be complex or very simple heuristic)
                                                11. Chain (pipeline of prompt and/or models) it belongs to
                                            3. Metadata
                                                1. Governance: who pushed the model, who participated in building the model
                                                2. Dependencies/Effects
                                                    1. Model dependencies (upstream) (depedends on which prior models specification (should be able to rebuild model from these the specification))
                                                        1. Ensembles of prior models: need to link to the exiting models & the ensembling method employed (typically mixture of experts of stacking)
                                                        2. Knowledge distillation of prior model: Dataset used to train the model was actually generated by a prior model
                                                    2. Model Effects (dowstream) (is a dependency for which models)
                                                3. Model Card (Tools: model-card-toolkit): mini documentation with the objective of commoditizing the model (specifies input (space of possible inputs and space of working inputs)/output, performance, problems the model has & other relevant info that help pople improve the model in a card-like nice UI)
                                                4. History/Versioning: history of model versions
                                                5. Reproducibility: pointer to experiment that produced the model
                                                6. Lineage: Entire timestamped activity that led to the generated artifact + version/config state of services that paricipated in building this model. (Tools: OpenLineage)
                                                7. Privacy:
                                                    1. Differential privacy details
                                                    2. Unlearning details
                                                8. Alternatives
                                                    1. Heuristic Fallback
                                                    2. Appeal Model and how to deploy the appeal model prediction workload to get a prediction from it (better model that does the same task, but slower and/or costlier)
                                                9. Purpose
                                                    1. Value model
                                                    2. Statellite models
                                                        1. Evaluation model (used to evaluate the value model, to avoid manual human evaluation)
                                                        2. Filter model (used to filter value model's inputs and/or outputs)
                                                        3. Alternative model
                                                            1. Heuristic Fallback
                                                            2. Appeal Model
                                                10. Aproximate Hardware Requirements for Acceptable Performance
                                                11. Mission Critical? (e.g., Medical or Autonomous Car is mission critical because if wrong can directly lead to deaths)
                                            4. Demo: one-click model demo (deploy a model demo in one click to get a felling of what the model does and its behaviour)
                                    4. Model Artifact's State (and (action, time action was made) that transitioned states) (can get these info from Logging System and/or Monitoring System and/or Metadata Store and/or Directly from ML service CI/CD)
                                        1. Pushed [ID of push]
                                        2. Waiting Evaluation [ID of the evaluation]
                                        3. Evaluated [ID of the evaluation]
                                            1. Passed Evaluation [ID of the evaluation]
                                            2. Did not pass Evaluation [ID of the evaluation] (and why not)
                                        4. Waiting deployment [ID of the deployment]
                                        5. Staged [ID of the deployment]
                                            1. Passed Staging [ID of the deployment]
                                            2. Did not pass Staging [ID of the deployment] (and why not)
                                        6. Deployed: (current_model=1) deployed to production cluster [ID of the deployment]
                                            1. Deployed in special terms
                                                1. Shadow Deployed [ID of the deployment]: with place where predictions are going specified (Monitoring or logging)
                                                2. Canary Deployed [ID of the deployment]: with type of population specified, including % it represents of total population
                                                3. A/B Deployed [ID of the deployment]: with % of traffic specified
                                            2. Fully deployed [ID of the deployment]: 100% traffic and 100% population
                                3. UI: Frontend (Tools:
                                    (1) From scratch
                                        (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                        (2) Automatic template: screenshot-to-code
                                        (3) Implementation
                                            (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                                (1) Code:
                                                    (1) Template Libraries: Grapesjs
                                                    (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                    (3) Supporting Libraries: 
                                                        (1) State management: Redux; 
                                                        (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                        (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                    (4) Telemetry
                                                        (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                        (2) Session Replays: OpenReplay
                                                    (5) UI Dev Environment: Storybook, OneOne
                                                (2) Build: Webpack; 
                                                (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                                (4) Build & Web Serve: 
                                                    (1) SSR: Nextjs, Astro, remix; 
                                                (5) Code & Build: 
                                                    (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                    (2) Static Site Generators: Hugo, Gatsby, Quartz
                                    (2) Using task-specific high level tools
                                        (1) Dashboards: metabase, plotly, graphana)
                                    1. HTTP server that serves Frontend. Frontend: displays all the information being store in the DB as a model shop, separeated by type of model, deployed/not deployed, organzed in version sequence and with all the deitails of each versioned model along with the API endpoint + Authentication Required to access the Inference Service of a particular model (aka Model Card).
                                    2. Application Server that frontend and other clients consume. HTTP API: returns all the information being store in the DB as a model shop, separeated by type of model, deployed/not deployed, organzed in version sequence and with all the deitails of each versioned model along with the API endpoint + Authentication Required to access the Inference Service of a particular model (aka Model Card); in the form of a json response.
                            2. Capabilities
                                1. New Best Model. Upon storing a new model
                                    1. Organize best models in order based on metrics & last retraining version
                                    2. Trigger New Best Model Available
                                2. Fetch Open Source Models from defined location. Engineer can put the model metadata manually afterwards or confgigure how themetadata should be filled based on what the model location provides as metadata.
                                3. Backups & Logs: just like a simple DB it should do periodic backups to another location (append-only location), along with storing logs since last backup.
                                4. Build lineage reports of models ( avery specific & fully descriptive story of how it was built & its evaluations)
                                5. Model demos (Hugging Face Hub-style) (Tools: Streamlit, Gradio, Shiny, Dash, Voila, PyWebIO, mesop, text-generation-webui, reflex)
                        2. Pipeline/graph registry
                            1. Types of Pipelines
                                1. By goal
                                    1. ML System Pipelines
                                        1. Feature Fabrication Pipelines (Makes features, or brings data one step closer to being features)
                                            1. Training Data Pipelines (raw training data in the ML Data whareshouse --> featured training data in the Feature Store):
                                                1. When it runs: can start based on state of ML Data whareshouse, time schedules or requests.
                                            2. Inference Data Pipelines (raw input data received --> featured data in the Feature Store & sent to prediction manager):
                                                1. When it runs: starts upon receiving a request
                                        2. Inference Prediction Pipelines: Chain of models/prompts (Specifies to the Prediction Manager and/or Master Inference Service exatcly how to queries Inference Services in order to get a prediction)
                                    2. Workflow Orchestrator pipelines
                                        1. Experimentation Pipelines (organizes experimentation code, makes it reproducible and portable and makes it easily deployable to a high performance cluster. Produces ML Artifacts like models, datasets, etc)
                                        2. Generalized CI/CD Pipelines (listens to events and deploys changes to the ML System accordingly)
                                2. By implementation
                                    1. Pipeline of tasks (aka what people generally think of pipelines)
                                    2. Pipeline of streaming services (e.g., for Inference Prediction Pipelines)
                            2. Components
                                1. Backend that talks to clients (Tools: (1) Python: (1.1) HTTP API libraries (build them as a library for Dynamic HTTP Server Library to use): FastAPI; (1.1.1) REST (Old messy way very tied to DB principles): plain FastAPI; (1.1.2) RPC (New functional way that treats paths as functions, payloads as parameters, compresses data and fully embraces http 2.0)): gRPC + FastAPI; (1.1.3) GraphQL (New DB way that ignores vestige HTTP paths, treat payloads as lossened DB queries (not just reading, but also updating & adding data) payloads as parameters, compresses data and fully embraces http 2.0): add Ariadne + FastAPI; (1.2) Dynamic HTTP Servers (handles parallel socket connections, calling HTTP API libraries, error returns): Gunicorn (WSGI) with Uvicorn (ASGI) interface; (1.3) HTTP Reverse Proxies: NGINX; (2) Nodejs: (2.1) HTTP API libraries: Dont know any; (2.1.1) REST: plain FastAPI; (2.1.2) RPC: gRPC; (2.1.3) GraphQL: GraphQLjs; (2.2) HTTP Servers: HTTP Module; HTTP Module; (2.3) Single Dynamic HTTP Framework: Express, Nestjs; (1.3) HTTP Reverse Proxies: NGINX)
                                    1. Serving requested pipeline files
                                    2. Storing requested pipeline files to be stored (can contain forced pipeline state (e.g., when a sceham changed and then you need to upload a new pipeline file, that oyu want that immediately be retrained))
                                    3. Rollback/promotion of pipelines requests
                                    4. Deleting pipelines according to requests
                                2. DB that stores all things pipeline
                                    1. For multiple pipelines
                                        1. Common components (pointer to containers in private container registry) & description of them
                                    2. For a specific pipeline
                                        1. Pipeline itself: 
                                            1. Formats
                                                1. Language-agnostic formats (e.g., WDL (Workflow Description Language), Kedro pipeline format, Hamilton Pipeline format or some other graph-based format (analogous to ONNX for models))
                                                2. Language-dependent formats (e.g., pickle)
                                            2. Code (Often both types are present, with .so being the core model execution kernels and interpreted language library as runtime or just an interface)
                                                1. Hardware independent (Interpreted) code: interpreted language library (e.g., python library)
                                                2. Hardware dependent (Compiled) code: .so library for some target hardware.
                                        2. Pipeline lifecyle data:
                                            1. Pipeline Development data
                                                1. Link to experiments in Experiment Tracking System
                                            2. Pipeline Deploy data
                                                1. Configuration
                                                2. Pipeline size
                                                3. Input (raw data) & Output (features) shemas
                                                4. Input, Intermidiate & Output Storage locations
                                                5. Type of deploy
                                                    1. Batch
                                                        1. Using workflow orchestrator (Tools: worflow orchestrators such as: General workflow Orchestrators: Argo, Prefect, Airflow, Dagster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau, dagger; (2) Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro, Couler, metaflow, sqlflow; (3) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo), mleap, flyte, sematic; (4) Data-driven Workflow Orchestrators: Pachyderm; (5) Notebook-based Wrkflow orchestrators: Ploomber)
                                                        2. Using distributed processing tool (e.g., spark/koalas)
                                                    2. Streaming: Using stream distributed processing tool (e.g., Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast)
                                                6. Deploy specification
                                                    1. Physical cluster hardware
                                                    2. Logical cluster resource allocation
                                                    3. Pod specifications
                                            3. Pipeline metadata
                                                1. Governance: who pushed the pipeline and when (timestamp)
                                                2. Pipeline (DAG) Card: mini documentation with the objective of commoditizing the pipeline (specifies input/output, performance, problems the pipeline has & other relevant info in a nice compact UI (e.g., html, svg, pdf))
                                                3. Versioning: history of pipeline updates
                                                4. Dependencies: pipeline dependencies (e.g., python files or container images)
                                                5. Effects: pipeline is dependency for which other things
                                        3. Pipeline state (and (action, time action was made) that transitioned states) (can get these info from Logging System and/or Monitoring System and/or Metadata Store and/or Directly from ML service CI/CD)
                                            1. Pushed [ID of push]
                                            2. Waiting Evaluation [ID of the evaluation]
                                            3. Evaluated [ID of the evaluation]
                                                1. Passed Evaluation [ID of the evaluation]
                                                2. Did not pass Evaluation [ID of the evaluation] (and why not)
                                            4. Waiting deployment [ID of the deployment]
                                            5. Staged [ID of the deployment]
                                                1. Passed Staging [ID of the deployment]
                                                2. Did not pass Staging [ID of the deployment] (and why not)
                                            6. Deployed: (current_pipeline=1) deployed to production cluster [ID of the deployment]
                                                1. Deployed in special terms
                                                    1. Shadow Deployed [ID of the deployment]: with place where predictions are going specified (Monitoring or logging)
                                                    2. Canary Deployed [ID of the deployment]: with type of population specified, including % it represents of total population
                                                    3. A/B Deployed [ID of the deployment]: with % % of traffic specified
                                                2. Fully deployed [ID of the deployment]: 100% traffic and 100% population
                                3. UI: Frontend (Tools:
                                    (1) From scratch
                                        (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                        (2) Automatic template: screenshot-to-code
                                        (3) Implementation
                                            (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                                (1) Code:
                                                    (1) Template Libraries: Grapesjs
                                                    (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                    (3) Supporting Libraries: 
                                                        (1) State management: Redux; 
                                                        (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Material UI, Carbon Design System, Baklava
                                                        (3) Plots & Dashboards: deck.gl, d3, tremor 
                                                    (4) Telemetry
                                                        (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                        (2) Session Replays: OpenReplay
                                                    (5) UI Dev Environment: Storybook, OneOne
                                                (2) Build: Webpack; 
                                                (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                                (4) Build & Web Serve: 
                                                    (1) SSR: Nextjs, Astro, remix; 
                                                (5) Code & Build: 
                                                    (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                    (2) Static Site Generators: Hugo, Gatsby, Quartz
                                    (2) Using task-specific high level tools
                                        (1) Dashboards: metabase, plotly, graphana) 
                                    (displays all the information being storeds in the DB as a pipeline shop, separeated by type of pipeline, deployed/not deployed, the model it is making feature for; organzed in version sequence and with all the deitails of each versioned pipeline along with the API endpoint + Authentication Required to access the Inference Service of a particular pipeline (aka pipeline Card)
                                    1. HTTP server that serves Frontend.
                                4. Application Server that frontend and other clients consume (typically json responses).
                7. Operations Center ()
                    1. ML Monitoring System (Tools: (1) ML Monitoring/Observability tools: Evidently, Deepchecks, Phoenix, nannyml, datachecks, AimOS, Radicalbit AI Monitoring; (2) LLM-specific Monitoring/Observability tools: OpenLLM, langkit, Openllmetry, Helicone, llm.report, Vibranium Dome, Doku, LangWatch, langtrace, openlit; (2.1) Agent-specific Monitoring/Observability tools: agentops) (''The CTO of a monitoring service company told me that in his estimate, 80% of the drifts captured by his service are caused by human errors'').
                        1. Components
                            1. Stream Transport: big data stream transport (e.g., Kafka, Mosquitto, RocketMQ, Pulsar, Kinesis, Pravega, Brooklin) or Message broker builder (e.g., RabbitMQ, NATS, ActiveMQ, ZeroMQ)
                                1. Sources of monitoring data (this communication can be simplified using a Message Transport (e.g., Kafka, Mosquitto, RocketMQ, Pulsar, Kinesis, Pravega, Brooklin) or Message Broker (e.g., RabbitMQ) between these sources and Monitoring System)
                                    1. Inference Services (e.g.,  operational metrics)
                                    2. BFF/API gateway (e.g., (x,y_hat) tuples, new accounts, total users, active users)
                                    3. FrontEnds (e.g., user/environemnt ML feedback (weak or strong labels) and operational metrics (e.g., latency))
                                    4. Logging System  (e.g.,  operational metrics, (x,y_hat) tuples)
                                    5. Feature store (e.g.,  delayed labels with datapoint ID reference, new data available trigger (for retraining triggers), training data (or a random subset of it) of a model, schema changes)
                                    6. Annotation (can be more than just labelling) system (e.g.,  expert reviews for Static Predictions)
                                    7. Data pipelines (e.g., Data validation results or simply samples of Data for the Monitoring System to do validation)
                                    8. Retrain jobs (e.g.,  ''Data outage''  trigger)
                                    9. Data storage: Business Backend (e.g.,  ux operational metrics, changes in data dependencies)
                                    10. Level 2 and 3 CD frameworks (ML System monitoring metrics)
                                    11. Experiment Tracking System (e.g.,  responds request on how to get trainign data of a model with pointer to Feature Store datapoints)
                                    12. ML Data whareshouse (raw structured data)
                                    13. Experimentation (EDA Results in a standardized report/dashboard/notebook)
                                2. Metadata of Monitoring requests/messages
                                    1. Timestamp
                                    2. Name & ID of the system emiting the data
                                    3. IDs of all artifact dependencies (e.g., model ID: task ID/version)
                            2. Streaming Dashboard (Tools: perspective) (visualize streaming data) 
                            3. Backend
                                1. Stream (Custom Streaming Protocol)
                                    1. Receive data
                                        1. Stream transport (subscribes to messages) (_Note:_ if you are going to compute statistics on it, dont do random sampling! Use stratified sampling. But stratified sampling requires that the source of logs first computes the groups (data profiling) and then append it to the data itself wen publishing it))
                                        2. Stream processing (processes data: descriptive statistics, groupbys, statistical tests etc) (Tools: Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, KSQL, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast)
                                        3. Store data in ML Monitoring NoSQL DB (maintains recent data using some window, unless told otherwise, then the selected historical data goes to an external storage like S3)
                                    2. Give data
                                        1. Stream transport
                                            1. Potentially, produce messages with recent Monitoring data to ML Monitoring Frontend (_Note:_ Frontend needs to know how to handle lsiteining to streaming data, so it doesnt overload mmeory and crashes. Frontend needs to delete old data or data the user is not accessing in a smart way)
                                            2. Produce messages with old Monitoring data to Business Backend (to be stored in Business raw Data Lake (tools for: managed solution or based on open source distributed noSQL DB like cassandra or scilladb))
                                2. Batch (Requests) (HTTP)
                                    1. Receive data
                                        1. Get data from POST Requests
                                        2. Store data in ML Monitoring NoSQL DB (maintains recent data using some window)
                                    2. Give data
                                        1. Serve requests for Monitoring data
                            4. DB to store ML Monitoring data (most recent data) (Tools: influxdb, TDengine, timescaledb). Sends old data to ML Data Wharehouse or ML Data Lake.
                            5. Frontend with Dashboard that requests ML Monitoring data from backend (Tools:
                                (1) From scratch
                                    (1) Design: figma, FigmaToCode, penpot, draw.io, diagrams
                                    (2) Automatic template: screenshot-to-code
                                    (3) Implementation
                                        (1) Content Management System (CMS, aka Frontend CI/CD): Wordpress, Strapi, Ghost, Decap, wagtail, Keystone, Payload, Publii, Drupal
                                            (1) Code:
                                                (1) Template Libraries: Grapesjs
                                                (2) Framework Libraries: React, Preact, Vue, Svelte; 
                                                (3) Supporting Libraries: 
                                                    (1) State management: Redux; 
                                                    (2) CSS Frameworks: Tailwind, Sass, Bulma, Bootstrap, Carbon Design System, Baklava
                                                    (3) Plots & Dashboards: deck.gl, d3, tremor
                                                    (4) Out-of-the-box Components: Shadcn/ui, Material UI
                                                (4) Telemetry
                                                    (1) Analytics: (1) Open Source: umami, plausible, Snowplow, DataHog; Managed: Google Analytics, Tableau;
                                                    (2) Session Replays: OpenReplay
                                                (5) UI Dev Environment: Storybook, One
                                            (2) Build: Webpack; 
                                            (3) Web Serve: Uvicorn/Gunicorn, caddy, NGINX; 
                                            (4) Build & Web Serve: 
                                                (1) SSR: Nextjs, Astro, remix; 
                                            (5) Code & Build: 
                                                (1) Web Frameworks: Django, Larelevel, SpringBoot, Sveltekit, redwood;
                                                (2) Static Site Generators: Hugo, Gatsby, Quartz
                                (2) Using task-specific high level tools
                                    (1) Dashboards: metabase, plotly, graphana)
                                1. Crucial & Fast-changing monitoring data: streamed to frontend (carefull not to overload memory usage on frontend!)
                                2. Other monitoring data: queried (req/res) by frontend
                        2. Operation: what to monitor. _Note: it is very important to show samples/examples (+ ability to tag and annotate them), not just metrics. Because there is a long-tail of silent bugs in production that are very hard to know before-hand. Giving ML Engineers smaples/examples can help with this. E.g., if for some reason in training input images where flipped and in infernce they are being received normal. This would destroy performance but would be hard to detect with a metric. However, if you who sample/example inputs to the model in training vs to the model in inference you will see the flip easily. Note 2: alerts should go off actually before the metric reaches the treshold, it should go off when IT monitoring forecasts that the treshold will be reached in the near future, so that engineers can take appropriate actions before the problem occurs._ ("When calculating metrics, you can choose between spot checks and exhaustive checks. Spot checks involve sampling a subset of data to quickly identify issues, while exhaustive checks evaluate every request for a comprehensive performance view") ("When computing metrics, ensure they can be broken down by relevant axes, such as users, releases, prompt/chain versions, prompt/chain types, and time. This granularity helps in understanding performance variations and identifying specific issues")
                            1. Data (Tools: (1) Open Source: (1.1) Frameworks (vendor-independent): Vector; (1.2) General: Great Expectations, Soda Core, Cleanlab, Evidently, Deepchecks, datachecks; (1.3) CV: cleanvision; JSON: JSON Schema; (2) Proprietary: Monte Carlo, Qualdo, acceldata, DataBand, Metaplane, Talend Data Quality, Anomalo, BigEye, Ataccama)
                                1. Storage (Tools: Bytebase)
                                    1. Active
                                        1. Query DBs (e.g., check for data poisoning in feature store)
                                    2. Passive
                                        1. Monitor dataset access patterns and inspect specific queries
                                        2. Monitor schema changes
                                        3. Monitor backup events
                                        4. Monitor anomalies
                                2. Pipelines
                                    1. Types of pipelines
                                        1. Data pipelines
                                        2. Inference Prediction Pipelines
                                    2. What to monitor (Training and inference data pipelines) (''Within ML-specific metrics, there are generally four things to monitor: models accuracy-related metrics, predictions, features, and raw inputs. These are Artifacts (main I/O files) generated at four different stages of an ML system pipeline. The deeper into the pipeline an artifact, the more transformations it has gone through, which makes a change in that artifact more likely to be caused by errors in one of those transformations. However, the more transformations an artifact has gone through, the more structured its become and the closer it is to the metrics you actually care about, which makes it easier to monitor'')
                                        1. Input data checks
                                            1. Pattern 1: done by ML System components (tests done by components of pipelines, they just send result to Monitoring System): all data checks that dont suceed in the system get streamed to the ML Monitoring System backend (_Note:_ When to do data checks internally or let to the MOnitoring System to do? _Thumbrule_: If its lighweight and/or critical: do internally and send result to Monitoring System; else: can send samples to Monitoring System)
                                                1. For critical data checks: Send to the next service in the pipeline special message "Critical validation failed". This message gets passed until it reaches the Inference Service, which in turn returns it to the client (that ends up using the fallback)
                                                2. For non-critical data checks: Continue with the normal flow of the system
                                            2. Pattern 2: raw data is sent by components and ML Monitoring System does it (These tests could be done internally instead. _Note:_ When to do data checks internally or let to the Monitoring System to do it? _Thumbrule_: If its lighweight and/or critical: do internally and send result to Monitoring System; else: can send samples to Monitoring System do it for you)
                                                1. Types of Data
                                                    1. ML Raw Data data checks (data source can stream directly to Monitoring service samples or Monitoring Service can get from ML Data whareshouse) (_Note:_ this type of Monitoring is rarely done because its way messier because of the amount and non-wrangled data; gives less guarantees: due to the fact that there are still a lot of processing steps that might throw in bugs along the way; and because Monitoring teams generally dont even have access to get them)
                                                    2. Feature data checks (tools: JSON Schema, Great Expectations, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation) (receive features before aplying dimensionality reduction methods)
                                                2. Methods
                                                    1. Schema validation
                                                        1. Update when: change in schema
                                                        2. Methods
                                                            1. Dimensions
                                                                1. Existence
                                                                2. Order
                                                                3. Data type
                                                            2. Rows: Number of rows
                                                    2. Expectations
                                                        1. Update when:
                                                            1. New production schema version
                                                            2. Data Distribution shift (but generally, people analyze basic expectations that are very resilient, so this shouldnt be the major issue)
                                                        2. Rules to apply
                                                            1. Data Ranges and presence/Absence of values
                                                            3. Smoke tests
                                                                1. Between dimensions (''e.g.,  DIM_1 > DIM_B'')
                                                                2. Between raw datapoints (e.g.,  raw datapoint needs to be unique, or a values in a dimension needs to be unique; or rawDatapoint_{t-1}[time] < rawDatapoint_{t}[time])
                                                                3. Common errors
                                                            4. Data Volume
                                                                1. FrontEnd Requests: Repeated requests (indicates the user was not satisfied with first response. Important: need to guarantee debouncing is being used in the frontend, because pople then to click a lot in a row)
                                                            5. Data Size
                                        2. Input Data Distribution shift (Tools: alibi-detect, evidently, Deepchecks, nannyml, AimOS, Radicalbit AI Monitoring) (Note: sudden shifts are often indicative of data bugs)s: Featured Input (covariate) shift (not trivial because of the high dimensionality of this data) (need to get training data of the model from Experiment Tracking System (will give pointers to training datapoints in Feature Store)). (_Note:_ we are not comparing the distribution of the ost recent sliding window to the training distribution, we are comparing with the most similar window with accetable corrected predictive power. Because in this way we avoidcatching non-dangerous shifts (there are quite a few), that is, when there is a feature input shift that reaches our treshold but it does not worsen significantly our model). _Note on window sizes:_ "We can also compare across various window sizes simultaneously to ensure smaller cases of drift aren't averaged out by large window sizes.". _Note on implicit features (metadata):_ "we can also monitor other implicit features such as % of unknown tokens in text (need to maintain a training vocabulary), etc. While they may not be used for our machine learning model, they can be great indicators for detecting drift."
                                            1. Types
                                                1. Location shift (e.g., different geographic locations exibit different resulting P(X,Y) distributions)
                                                2. Time shift. Beware of:
                                                    1. Window Parameters (offset, size)
                                                    2. Shifts as a function of time (Knowing this, makes it possible to leverage more data and anticipate future necessities)
                                                        1. Seasonalities (cyclic Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s)
                                                        2. Convergence (distributions of some subset of features tend to to a specific distribution over time)
                                            2. Analysis
                                                1. Pre-analysis
                                                    1. Manual
                                                        1. Draw Hypotheses
                                                            1. Draw Hypotheses on models that [might be experiencing/will experience] either location shifts or time shifts
                                                            2. Draw Hypotheses on possible seasonalities of models (identifying seasonalities is great, because then will get way more training data than just using last time window, and dont make window parameter mistakes)
                                                        2. Validate Hypotheses
                                                            1. Visualizations
                                                            2. Statistical Tests
                                                        3. Tune parameters (e.g., time window for time shifts)
                                                    2. Automatic
                                                        1. Dimensionality reduction if data is high dimensional. Because most probability distribution distance methods only work well for low-dimensional data
                                                2. Techniques (most methods only work well for low dimensionality. So, a  lot of engineers end up picking the most important features (via feature importance) or do dimensionality reduction via a projection (analytical, random or statistical) to measure drift)          
                                                    1. Use Statistics
                                                        1. Types
                                                            1. Using raw feature inputs
                                                            2. Using properties/proxies
                                                                1. Feature importance
                                                                2. Confidence scores
                                                                3. Number of outliers
                                                        2. Methods
                                                            1. Compare data statistics with heuristics (eg. mean, median, variance, quantiles, skewness, kurtosis, etc) (get point estimate of the statsitic only)
                                                            2. 2 sample (2 empirical distributions) tests for difference in distributions with some tuned treshold
                                                                1. Between joint over numeric features:
                                                                    1. 1-dimenional data: e.g., Kolmogorov Smirnov, KL Divergence, infinity norm, Earth-mover's distance
                                                                    2. Multi-dimensional data: e.g., Jensen-Shannon Divergence, KL Divergence, Maximum Mean Discrepancy (MMD), Earth-mover's distance
                                                                2. Between joint over ordinal features: 
                                                                    1. 1-dimenional data: e.g., Chi-square goodnesss of fit
                                                                    2. Multi-dimensional data:
                                                                3. Between joint over categorical features:
                                                                    1. 1-dimenional data: e.g., L-Infinity Norm or Chi-square goodnesss of fit
                                                                    2. Multi-dimensional data:
                                                                4. Between joint over ordinal and categorical features: <TODO>
                                                                    1. 1-dimenional data: 
                                                                    2. Multi-dimensional data:
                                                                5. Between joint over numeric and categorical features: <TODO>
                                                                    1. 1-dimenional data: 
                                                                    2. Multi-dimensional data:
                                                                6. Between joint over numeric and ordinal: <TODO>
                                                                    1. 1-dimenional data: 
                                                                    2. Multi-dimensional data:
                                                                7. Between joint over numeric, ordinal and categorical features: <TODO>
                                                                    1. 1-dimenional data: 
                                                                    2. Multi-dimensional data:
                                                    2. Use ML
                                                        1. ML classfifier (tries to predict if datapoint came from training or production distribution, it should not ave any corrected predictive power)
                                                        2. Fit generative models (that model the the joint distribution P(X)) to training & deploy data
                                                            1. If explicit model (Density Estimation): can compare functions
                                                            2. If implicit model (e.g., GANs): sample from it and get statistics
                                                        3. For NLP:
                                                            1. Topic models
                                                            2. clustering on Embeddings
                                                    3. Use Visualizations: project high dimensional data into low dimensional space (2 or 3 dimensions). E.g.,  tSNE. And then go by eye or do Statistical tests on this easier data
                                                3. If a shift is detected: investigate.
                                                    1. Analyze data distribution shifts on subsets of the feature space. 
                                                        1. Aggregate. The goal here is to maybe find that a shift is localized to some features only.
                                                        2. For each value/range of fixed features. The goal here understand how the real world process changed, wich will be usefull to improve modelling.
                                        3. Pipeline Profiling: time each component of each pipeline takes to run.
                                            1. Manually inspect
                                            2. Define ranges
                                            3. Impose constraints (e.g., sum of times must equal total time in each pipeline)
                                        4. Track Lineage of data with graphs (should be able to draw examples of raw datapoints and resulting artifact of each part of the pipeline, until final featured datapoints) (Tools: OpenLineage)
                                        5. Skew Identification
                                            1. Training-Serving Skew: comparision between Training & inference data pipeline s. For a given raw ML datapoint, featured datapoint shuld be exaclty equal
                                            2. Staging-production Skew: comparison between Stagin and production clusters. Should be aprox equal: operational metrics in staging and production
                                                1. Latency
                                                2. Throughput
                                3. Data Annotation (can be more than just labelling)
                                    1. Example <x,y> tuples
                                        1. By labeller
                                        2. By some labeller feature
                                        3. By time
                                        4. By Mistakes
                                    2. Operational Metrics
                                        1. Types
                                            1. Latency
                                            2. Throughput
                                        2. By
                                            1. Overall
                                            2. labeller
                                            3. Annotation (can be more than just labelling) group
                                            4. Type of data
                                            5. Mistakes
                                    3. Analysis of Active Learning strategies (by increase on model-related benchmarks such as corrected predictive power & fairness)
                                        1. No active learning vs some activa learning strategy
                                        2. Comparison betweeen 2 active learning strategies
                            2. Models 
                                1. Training (monitoring remote training jobs) (Tools: labml)
                                    1. Types
                                        1. Batch training
                                        2. Online training
                                    2. Things to monitor
                                        1. Operational Metrics (resource consumption)
                                            1. Memory usage
                                            2. Processor usage
                                        2. Sample batches/datapoints
                                        3. Paramater updates
                                            1. Parameter Diffs
                                            2. Low-dimensional Visualizations
                                        4. For traditional ML models
                                            1. Training metrics
                                                1. Train & CV loss plot
                                            2. Offline Evaluation metrics: test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) metrics by slices of feature space  
                                        5. For LLMs
                                            1. Training
                                                1. Training metrics
                                                    1. Train & CV loss plot
                                                    2. test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) metrics by slices of feature space
                                                2. Model Alignement methods
                                                    1. RLHF (Reinforcement Learning with Human Feedback) and RLAIH (Reinforcement Learning with AI Feedback)
                                                    2. DPO
                                                    3. TKO
                                            2. Offline Evaluation metrics: test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) metrics by slices of feature space 
                                                1. LLM-powered evaluation on test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)
                                                2. Embedding-based evaluation on test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)
                                                3. Manual evaluation on test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)
                                2. Evaluation
                                    1. Anomaly detection (Tools: PyOD, Darts, Anomalib, Anomstack)
                                        1. For any task/model & version combination: too low or too high metric
                                        2. Same task/model, different versions: detect drastic changes in metrics (e.g., global explanability)
                                3. Inference (Note: more important models (according to manual setting or automatic model-business kpi alignment) and models that have higher prediction frequency should get higher fixing priority)
                                    1. Primers
                                        1. On predictive power Monitoring
                                            1. On ground truth labels
                                                1. Models with natural ground truth from the environment/user
                                                    1. Types
                                                        1. Delayed Labels: models that predict what will happen in the future (indirect time-series)
                                                            1. Environment label feedback:
                                                                1. Predicting way ahead of time (can be simulated time also) (takes time to get ground truth labels) (e.g.,  predicting size of crops after 1 year; or predicting output of CAE program). These models tend to be static in nature.
                                                                2. Predicting right ahead of time. These models tend to be dynamic in nature. Models with tight feedback loops (e.g., predicting time of an uber travel)
                                                            2. User label feedback: Predicting right ahead of time. These models tend to be dynamic in nature. Models with tight feedback loops (e.g., of recommendation systems, if user clicks recommended content, can assume that got the prediction right or if user selects suggested next word).
                                                        2. Immediate Labels: labels after decision. Environments that give infromation to the model after it made a decision (e.g.,  like ''you gor it right'', or '' you got it wrong'', or ''you got it wrong and here is the right answer'')
                                                            1. Artificial environments (e.g.,  Techer-student environment, where model is the student trying to get the answers right or in simulations)
                                                            2. Natural environments (e.g.,  robot trying to pick which box to open that has a present (robot can see))
                                                    2. Speed vs Quality tradeoff
                                                        1. Engineering for Delayed Labels or Proxy Labels 
                                                            1. Delayed Labels: higher quality evaluation; but will be evaluating with delay
                                                            2. Proxy Labels: lower quality evaluation; but will be evaluating without delay
                                                        2. Label Definition parameter (e.g., for recommender systems: click x time is an exponential decay)
                                                            1. Low time range to occur label: lower quality prediction system (because you will miss a lot of true labels that occur after your small range); but evaluates with less delay
                                                            2. Big time range to occur label: higher quality prediction system (because you will miss few true labels that occur after your big range); but evaluates with more delay
                                                2. Models without natural ground truth from the environment
                                                    1. Delayed Labels: solving with fast Annotation (can be more than just labelling) System. If you annotation (can be more than just labelling) system is very fast, can use delayed labels (gets from feature store)
                                                    2. Proxy Labels 
                                                        1. Solving with User ML Feedback. Depending on UI/UX & Data collection, can get proxy labels  from user ML feedback (get in ML Data whareshouse) (Can only be used in cases where user doesnt have an incentive to not make an objective correct feedback, and can be tricky because some users may be trolls/hackers that give wrong feedback)
                                                            1. Level 1 - Binary feedback
                                                            2. Level 2 - Adding correct prediction feedback
                                                        2. Solving with Proxy Signals: in some tasks, data can be gathered in real-time that lets you infer with varying degree of acc the real label. You can even train a ML model to map from proxy label to real label. 
                                                            1. Proxy Signal for individual datapoint (e.g., tesla AI engine that detects if decision was right based on drivers behaviour after it)
                                                            2. Proxy Signal in aggregate (often because of privacy issues) (e.g., "with loans it might be the aggregate amount of loan defaults per month based on different demographics rather than the actual individual’s loan default statistics")
                                                    3. No labels (Tools: nannyML)
                                                        1. Quantifying volatile resgions of X (idea: try to quantify how much of your test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) and deploy set are in distribution-shift suceptible areas. Build matrices for both; each row of the matrice is one example and each column 1 if contains an specific aspect that you suspect can change a lot in deploy. If the matrices are the same)
                                                        2. Clustering X to known predictive power regions: putting inference X into a subgroup with known corrected predictive power (calculated in offline model evaluation). Do this for all incoming streamed deploy data, and then you can calculated expected value of corrected predictive power
                                                        3. Estimating causal effect of Data Distribution shift (Note: sudden shifts are often indicative of data bugs) on Predictive Power
                                                        4. Relying purely on Data Distribution shift (Note: sudden shifts are often indicative of data bugs) detection
                                                        5. Monitoring violating of invariants. invariants are an establishes relationship betwenn output A and output B for different inputs A and B, that are problem-specific and dont require knowing output A neihter output B (aka dont require labelled data)
                                            2. On receiving datapoints (Generally Streamed by Feature Store). 2 major issues.
                                                1. Monitoring System already gets it handed: 'Representation differences. Accuracy and class ratios across windows may not be the same (e.g.,  the fraction of positives in one window may be very different from the fraction of positives in another window).'' This causes the estimate of accuracy to have high variance. Solution: Stratified Sampling. Making groups of known predictive accuracy and class labels (carving up the p(x,y) landscape into subdistributions), and then sampling from them accroding to their size. The smaller the group the better because the subdistribution will have less variance, however too small has first the problem of sampling few datapoints from a group (e.g., 1 or 2) and thus nothaving a statistically significatnt empiral subdistribution (representative of the actual distribution) making the estimate have high variance because the "ditribution itself has high variance", and further, has the problem of not being able to sample anything from a group and actually make the empirical sudistribution, which will destroy the global estimate by making it biased. Note: Group info will be present in the generalized datapoint.
                                                2. Monitoring System needs to do hard work
                                                    1. Delayed labels with varying delay. Solution: Startified Sampling. Making groups of known classes and then sampling from them accroding to their size. The smaller the group the better because they will have less variance, however too small makes the acc of the group unrealiable.
                                                    2. ''Varying sample sizes. Bad because you get windows with very unrelaible acc estimations and other with unncessaririly high realiability. The number of data points in each window may vary (e.g.,  the number of requests received on a Sunday is less than the number of requests received on a Monday)''. Define window parameters (size, offset) as to make periods of time with peaks or lows be braken down into two windows, in this way every windows gets a peak and a down and overall sample size is aprox the same.
                                    2. Inference Monitoring
                                        1. Inference Service Operational metrics
                                            1. Latency (time between request and response for the client)
                                            2. Throughput (see if the system couldnt handle some requests and how many/time)
                                            3. cloud/premise cost/time or prediction
                                            4. Number of predictions p/day
                                        2. Chains (Inference pipelines of models, prompts, filter, retrievers)
                                            1. Type of chain
                                                1. Agent-less chains: each step of the some chain that can contain prompts, ml models, filter, retrievers
                                                2. Agent-full chains: each step of the some chain that can contain prompts, ml models, filter, retrievers + sensors/actuators (tools: internal actuators: E2B)
                                            2. What to monitor
                                                1. Trace of chain
                                                2. Overall chain operational metrics
                                                3. Overall chain ML metrics
                                        3. Prediction Request Metadata (Motivation: most of the time, a problem like data distribution shift is not due to change in the real world process, but due to data processing mistakes. Request metadata helps debug these mistakes. "E.g. each request needs to come with more metadata that is not needed to call the endpoint (e.g. timestamp, app version, use case id if the algorithm is used for multiple places, ab test ids, ect) to detect bugs on segments/clients")
                                        4. Model Online Evaluation (Done to clone model in the background cluster)
                                            1. Predictive power
                                                1. Approaches
                                                    1. For tabular data: Predictive Power on Slices/Subpopulations (Fairness) (Tools: Microsoft Responsable AI (RAI), Tensorflow Responsible AI Toolkit, LiFT, Tensorflow Model Analysis from TFX, fairlearn)
                                                        1. How to identify slices of data:
                                                            1. Automatic
                                                                1. Decision Tree (maimizes information gain/minimizes entropy on p(E=e|X=x) where e is the error (that is binary for classficaition, and 1 means that the model made the wrong prediction for that datapoint))
                                                                2. Search
                                                                3. Clustering
                                                                4. Bias-driven (seeing if the model presents some social dangerous bias)
                                                                    1. Analyzing predictions to special subsets of input data (e.g.,  recommending black people a lot of ...)
                                                                    2. Analying predictions in general (e.g.,  recommending a lot of male ...)
                                                                5. Confusion-matrix driven (when you are worried with recall/precision on some slice of the data)
                                                            2. Manual
                                                                1. Heuristics (requires domain knowledge)
                                                                2. Error analysis (patterns that misclassified examples follow)
                                                    2. Predictive Power on a test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) (Note: should be adapted through time)
                                                        1. Representative of production data (iid)
                                                        2. Hard: edge and unerrepresented cases (ood)
                                                    3. Heuristics (e.g., "Spotify has a personalized ranking of their home page. One heuristic they use is "where does the algorithm rank the user's most used element?" Intuitively, personalized algorithms should rank a user's most used element highly")
                                                2. Techniques
                                                    1. Automatic
                                                        1. Real-time
                                                            1. Weak label approach
                                                            2. No label approach (Tools: nannyML)
                                                            3. LLM-as-Judge approach (Tools: prometheus-eval, EvalLM, evalgen, cappy, arena-hard-auto, JudgeLM, PandaLM, Auto-J)
                                                        2. Delayed: delayed labels (Obs: even if some datapoints already have labels, you might not want to use them. You only use delayed labels on your specified delay window. In this way you remain time consistent. The datapoint you ignored, you will end up using when its in the delay window (e.g., 1 day)). The delay window is defined based on the speed of the distribution of feedback loop time for the feature (e.g., smallest window that gets > 70% of datapoints)
                                                    2. Manual
                                                        1. If static process: Expert Reviews (situations where the offline  (batch) Inference Service  (precomputed predictions) asked for Human to review its prediction)
                                                        2. Check <input, output> samaples.
                                                            1. LLM-specific
                                                                1. Embedding Model: low-dimensional plots and statistical tests to see if similar input actually have similar embeddings
                                            2. Output validation
                                                1. Content
                                                    1. Output Distribution shift (Note: sudden shifts are often indicative of data bugs) (Tools: alibi-detect, evidently, nannyml, Deepchecks, AimOS): output/label shift (''You can monitor predictions for distribution shifts. Because predictions are low dimensional, its easier to compute two-sample tests to detect whether the prediction distribution has shifted''). (_Note:_ we are not comparing the distribution of the recent sliding window to the training distribution, we are comparing with the last sliding window (in which corrected predictive power as good). Because in this way we avoid catching non-dangerous shifts (there are quite a few), that is, when there is a feature input shift that reaches our treshold but it does not worsen significantly our model). _Note for Generative Models:_ when delang with gen models, the output becomes high dimensional aswell, so caputiring shift in p(y) is much harder. A promising approach is to work with embeddings of these outputs and capture the shift in embedding space. Other more simple approaches are working with low dimensional text descritors (e.g., sentiment of the text). _Note on window sizes:_ "We can also compare across various window sizes simultaneously to ensure smaller cases of drift aren't averaged out by large window sizes."
                                                        1. Motivations
                                                            1. Absence of labels (''not available or too delayed to be useful'') to Monitor corrected predictive power directly
                                                            2. Proactively anticipating corrected predictive power drops (when modelling distribution shift)
                                                        2. Methods
                                                            1. Use Statistics
                                                                1. Look directly at distributions statistics (eg. mean, median, variance, quantiles, skewness, kurtosis, etc) (get point estimate of the statsitic only)
                                                                2. Bootstrapping (get a distribution for the statistic)
                                                                3. 2 sample tests for difference in distributions
                                                                    1. Between joint over numeric features:
                                                                        1. 1-dimenional data: e.g., Kolmogorov Smirnov
                                                                        2. Multi-dimensional data:  Jensen-Shannon Divergence
                                                                    2. Between joint over ordinal features: Chi-square goodnesss of fit
                                                                    3. Between joint over categorical features: e.g.,  L-Infinity Norm or Chi-square goodnesss of fit
                                                                    4. Between joint over ordinal and categorical features: <TODO>
                                                                    5. Between joint over numeric and categorical features: <TODO>
                                                                    6. Between joint over numeric and ordinal: <TODO>
                                                                    7. Between joint over numeric, ordinal and categorical features: <TODO>
                                                                4. Low-dimensional analysis: project high dimensional data into low dimensional space (2 or 3 dimensions). E.g.,  tSNE. And then go by eye, uses clustering methods or do Statistical tests on this easier data
                                                            2. Use ML
                                                                1. ML classfifier (tries to predict if datapoint came from training or production distribution, it should not ave any corrected predictive power)
                                                                2. Fit generative models (that model the the joint distribution P(X)) to training & deploy data
                                                                    1. If explicit model (Density Estimation): can compare functions
                                                                    2. If implicit model (e.g., GANs): sample from it and get statistics
                                                                3. Model distribution shift: simple model that maps: history of <shifts, predictive power metricc> --> next corrected predictive power metric (proactively anticipating corrected predictive power drops)
                                                        3. Important: analyze data distribution shift on different slices, the most common use for this is to guide your data collection (need to get more data on degrading slices).
                                                        4. Anomaly detection/Short-lasting shift (Tools: PyOD, Darts, Anomalib, Anomstack). But how to know if it is a short-lasting shift (anomaly) or long-lasting shift before we get future data? Well, we cant, but we can wait for a little bit of future data, to then try various window sizes, and if a specific window size gives great anomly compatibility, it just might be one.
                                                    2. Innapropriate content (manually see inputs and outputs of the model)  
                                                    3. Absurd behaviour (''E.g., if predictions are all False in the last 10 mins'') (In teory this is checked by Data Distribution shift (Note: sudden shifts are often indicative of data bugs), but would require way more computation, so it is usefull to define some absurd behaviours to monitor with relative easy)
                                                2. Format
                                                    1. Common errors
                                                    2. Schema validation
                                            3. Uncertainty/Calibration (check if model's errors are consistent with the uncertainty in its predictions)
                                            4. Explainability: 
                                                1. If model outputs explanation together with prediction: see if explanations provided by the model to it's output make sense
                                            5. Degenerate Feedback Loops: see degree of homegeneity of predictions in time (e.g., for recsys: plot output recommendations and popularity in time (shouldnt be exploiting just popularity of items to recommend them, i.e. homegeneity increasing together with popularity))
                                                1. Environment-driven (e.g., recsys that user umber of downloads as a feature)
                                                2. User-driven (e.g., users that change their features in order to change their output (they do interventions))
                                        6. ML System Online Evaluation
                                            1. Posterior effects with a/b flags in them (''For example, when youre building a system to recommend to users what videos to watch next on YouTube, you want to track not only whether the users click on a recommended video (click-through rate), but also the duration of time users spend on that video and whether they complete watching it (completion rate). If over time, the click through rate remains the same but the completion rate drops, it might mean that your recommendation system is getting worse'')
                                                1. Types 
                                                    1. Single or Mass Effects
                                                        1. Isolated Effects (e.g., time user spends in the platform after prediction)
                                                        2. Mass effects (e.g.,  Vehicle route suggestion, suggests same route to all the cars, leading to traffic there) (hard to measure)
                                                    2. Short or Long term Effects
                                                        1. Short term effects (e.g., time user spends in the platform after prediction or if user quits on using ML feature)
                                                        2. Long term effects (e.g.,  Facebook recommernder system making people have extreme views, people adict etc) (hard to measure)
                                                2. Data
                                                    1. Aggregate metrics
                                                    2. Whole user journey per user
                                            2. Online Experimentation (Tools: (1) Open source: ML deploy tools: Seldon Core, BentoML, Ray Serve, KServe, MLflow Models, Cortex, Truss, MLEM; DevOps tools: Argo Rollouts, flagger, Posthog, Growthbook, Flipt, Flagr; (2) Managed: (2.1) Paid: Optimizely, AB Tasty, Apptimize; (2.2) Free: Google Optimize)
                                                1. Types of Online experimentation
                                                    1. Feature-related
                                                        1. A/B or Multi-armed bandit tests between different feature staleness margin (i.e. the parameter that defines the gap size between the features datapoint timestamp the prediction manager wants & the oldest featured datapoint it can still use with acceptable prediction degradation)
                                                    2. Model-related
                                                        1. Model Comparison
                                                            1. A/B Testing (when you know the new model works, but you want to know which one of them is more effective wrt business metrics for the general public. Traffic percentage can be either distant (e.g., 90-10) or close (e.g 50-50). Can have more than 2 models being tested at the same time). Includes Orchestration of models (Interleaved experiments): combining models in different ways. (e.g  1. ''Take outputs from both model A & B''; 2. ''Mix them together and show them to users''; 3. ''See which recommendations are clicked on'')
                                                            2. Multi-Armed Bandit Evaluation (In improvement to A/B Testing. A/B testing uses Randomized controlled tests, this is great for  drawing causal conclusions of whihc model is better (akak exploration), however, can direct a lot of traffic to a crappy model. Multi-Armed Bandit tries to balance exploration with exploitation, by giving good performing models to users. Requires that you can online access to user/proxy business metrics (will be the reward))
                                                        2. Model Evaluation
                                                            1. Canary Deployment (when you dont know if the new model works, then you deploy it just to the less dangerous community to avoid disasters (maybe even within company workers), before full deployment. Traffic percentage is always distant (e.g., 90-10). We are testing only 2 models at the same time.)
                                                                1. Minority Deployment (when the community is composed of minority groups. This is used to identify fairness problems before reaching general public)
                                                            2. Shadow Deployment (Its kinda of a Fake deploy, ore ML-specific. The outputs of the model are not consumed by users, you just want to see if you can deploy it normally and if it would behave ok. Traffic is mirrored to the shadow Inference Service.)
                                                        3. Orchestration of models (Interleaved experiments): combining models in different ways. (e.g  1. ''Take outputs from both model A & B''; 2. ''Mix them together and show them to users''; 3. ''See which recommendations are clicked on'')
                                                    3. System-related
                                                        1. Latency Requirements (sometimes you user can wait longer than you expected, sometimes he cant wait the amount of time you hypothesized)
                                                        2. Retraining schedule (pick a retraining schedule that is the best wrt mainataining predictive power of the model and avoiding unnecessary retrianing costs)
                                                2. Performance gain check: Comparison of new model A with current active deployed model A
                                                3. Non-regressive Performance check: Comparision of current active deployed model A with former model
                                            3. Malicous (attackers or trolls) user identification
                                                1. Too many requests
                                                2. Poisoned training data (e.g., fake reviews)
                                                3. Adversarial inference input data (e.g., image modified to cause ml model mistake) (Tools: alibi-detect)
                                        7. ML CI/CD Monitoring (can be done together with  ''General Monitoring - Level 1 CI/CD'')
                                            1. Metrics relative to: Generalized CI/CD Levels [2-5] (Level 1 is traditional CI/CD, which is bigger than ML, thus is located at General Monitoring)
                                            2. Metrics:
                                                1. Deployment Frequency. Requirement baseline for this should offset by model decay metric an new data availability
                                                2. Commit2Deployed time. Requirement baseline for this should offset by the time working with the ML system and how often users expect the system to get better.
                                                3. Mean time to restore (when something crashes, how long to fix it (can include manual debugging, deployment and possibly retraining)).
                                                    1. Fixing Service bugs. Requirement baseline for this should offset by how critical a wrong prediction is to the client apps (how long can a bug last in prod and not causa a massive impact. And this is already optimistic because you would have to include the time to find the bug also)
                                                    2. Fixing Non-availability of the Service. Requirement baseline for this should offset by how critical unavailability of the service is to the client apps
                                                4. % of changes leading to a worse system
                                                    1. Level 2 - MLOps platforms: worse system is measured by the above metrics, operational metrics, model decay relative relative to no retraining, etc
                                                    2. Level 3 - Inference Services (Models): is measured by corrected predictive power metric os models in production data (often proxied with by soft labels) and subsequent user actions/time spent in the app
                                                5. % of evals that passed. (Should be around 60-70%)
                                4. Monitoring actions. 
                                    1. Treshold tuning (Optima trseholds that avoid mssing important stuff & also avoid alert fatigue)
                                        1. Manually-tuned thresholds: make plots of ML KPIs vs proxy metrics (e.g., can plot acc vs treshold alerts/notifications and see which trsheold is the best to capture only real problems)
                                        2. Auto-tuned tresholds: estimate the causal effect of thresholds on ML KPIs find optimal tresholds based on user config.
                                    2. Actions
                                        1. Typically automatable actions
                                            1. Trigger ML Training CI/CD using retraining
                                                1. Automatic retrain triggers (retraining service will be listening to them)
                                                    1. Reactive:
                                                        1. Predictive acc dropped below treshold (usually measuring with proxy labels)
                                                        2. Data Schema changes (e.g. new feature or deleted feature)
                                                    2. Proactive:
                                                        1. Predictive acc is forecasted to drop below treshold (usually measuring with proxy labels)
                                                        2. Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s pass treshold (need to specify if it is input/covariate shift or process/concept shift because changes retraining approaches) (threshold modulated by feature importance)
                                                            1. Input/covariate shift
                                                            2. Process/concept shift
                                                2. Manual retrain trigger: programmer decides to retrain <model A, version W> (e.g., input schema changed, so automatic retraining encountered some problems, then programmer received alert or saw in Monitoring System, fixed training algo (changed the model file to have the right I/O hyperparameters))
                                            2. Model emergency cutoff (Re-deploy BFF/API gateway to repond ''no predictions, model cutoff'' to requests to that a specific ML Inference Service)
                                                1. Manual
                                                2. Automatic: if the pipeline is in  '' data outage '' and Predictive acc dropped below treshold
                                            3. Automatic alerts/notifications if some metrics reach a treshold (thorugh e.g.,  whatsapp, email, slack or dedicated alerts manager like: keep)
                                            4. ML system compliance reports generation (if there is like some AI Compliance agency): these are sent in a request to the ML Metadata System to store it
                                        2. Typically Manual actions (for the time being) (_Note:_ essential to prioritize problems to fix here)
                                            1. Send request to Active Learning System demaning more data on failure cases the system is presenting (data flywheel).
                                                1. For hard-to-generate data (e.g., images): these are data desired and are generraly high-level information in human-readable format (data desires).
                                                2. For easy-to-generate data (e.g., bag of features of some known domain): can send already the ranges of features, potentially in serialized form.
                                            2. Model rollback/promotion (can be just decreasing/increasing traffic) requests to Model/Prompt Registry (very manual)
                                            3. Send request to User Data Deletion/Retrieval service, to delete data gathered from malicous actor
                                            4. Upon concept drift: Iterate on model/(training/re-training) algorithm
                                            5. Upon ''data outage trigger'': manually fix raw data storage and/or Data Sources
                                            6. Do Experimentation to improve ML Monitoring metrics (and Offline Evaluation metrics)
                                            7. Fix Data pipelines
                                                1. Directly identified bugs in pipeline monitoring
                                                2. Sudden Data Distribution shift (Note: sudden shifts are often indicative of data bugs)
                                                3. Raw data changed
                                                4. Input and/or Output validation problems
                                            8. Pipeline bottlenecks: Upon seeing large amount of time spent in some stage of the pipeline, examine it offline and maybe fix it if something is wrong or can be improved
                                            9. Get more data for some specific slice
                                                1. Get fresh data
                                                    1. Manually curating data
                                                        1. Manually storing features
                                                            1. Manual curate new features datapoints & put them into a file
                                                            2. Send a store request to the Feature Store with this file
                                                        2. Atomatically storing features
                                                            1. Manual curate new raw datapoints & put them into some storage (e.g., S3, DB)
                                                            2. Update Data Collection
                                                                1. Update raw data pipeline to handle this new data
                                                                2. Hook up this storage as a new data source to the system
                                                    2. Changing the config of the Data Collection System
                                                2. Use existing data to generate more data
                                                    1. Upsample
                                                    2. Interpolate
                                            10. Change Annotation (can be more than just labelling) System
                                                1. Change Annotation (can be more than just labelling) Guidelines
                                                2. Change Labellers
                                            11. Recover deleted artifacts from Trash Storage System and put them in production locations where they will be used
                                            12. ELT, move and/or sync data between locations (Tools: airbyte, skyplane, Fivetran, Stitch, ingestr, syncthing)
                                            13. Add/Modify tests
                                            14. Change test dataset: include hard examples encountered in production
                                            15. change monitoring config (e.g., add new slice of interest to monitor predictive power)
                    2. ML (Fine-grained) Observability System: (Tools: (1) Managed: Arize AI, Arthur AI, Fiddler)
                        1. Capabilities
                            1. High-level visualization of the system working (Tools: rerun, pheonix)
                                1. Support for different views of the system:
                                    ""

                                        * In a model view, consider direct quality expectations on the model, such as accuracy and explainability

                                        * In a data view, consider the availability, quantity and quality of data. Also the dataflow.

                                        * In a system view, consider goals and requirements for the entire system from the perspective of the provider and end users; specifically consider how the software interacts with the environment (see the world and machine chapter).
                                        
                                        * In an infrastructure view, consider the infrastructure needed to operate the system, including training cost, reproducibility needs, infrastructure for model serving, and monitoring needs.
                                        
                                        * In an environment/societal view, consider how the system interacts with users and the society at large, including considering possible safety and fairness concerns.

                                    ""
                            2. Debugging: root cause finding via traces and visualizations
                                1. Tracing
                                    1. Reactive and System-wide: Main artifacts chains: IO samples or "runs" of your system (where a run is all IO that happens between components, based on a triggering event, and where the last output is the one that doesnt trigger anything to run or based on a custom condition (e.g., frm user request to delivered prediction, from new raw data to stored featured datapoints in feature store, from configuration change in State System to new config log & new IO handling of some component/subsytem)). Note: distributed tracing requires attaching IDs to applicaiton protocols and propagating the same ID in downstream app protocols caused by the original one. This generally requires support from the networking app layer API tool (e.g. FastAPI) which needs to see which requests/streaming messages trigger other requests/responses/streaming messages;
                                2. Active and component-wide: IO flows fo a specific component interacting with other compinents to analyze it. E.g., analyze behaviour of an LLM Agent.
                            3. Simulated deployment in background cluster to perform observability, end-to-end evaluations (tools: (1) General: UMLAUT (Universal Machine Learning Analysis UTility); LLM-specific: Vidur) & train engineers to operate the system
                                1. System replayability (needs prodution config and maybe data versioning of subsystems)
                                2. Profiling
                                    1. Performance
                                    2. Cost
                            5. Anomaly/outlier detection (e.g., via isolation forest)
                        2. Components
                            1. Metadata Core Combo (Tools: (1) Only reactive (recieve, store, visualize): aimstack, ml-metadata; (2) Reactive & Proactive (modify): OpenMetadata, diffgram)
                                1. Metadata Transport (Tools: (1) ML-specific: whylogs, neptune-client; (2) General: OpenTelemetry, collectd, Elastic Stack) (send (push metrics)/serve (pull metrics) metadata from services)  
                                2. Metadata (Tools: Marquez, Datahub, Egeria, Aligned)
                                    1. Logs (Tools: graylog2-server,  ELK stack, Loki)
                                    2. Metrics (Tools: OpenTelemetry, collectd)
                                    3. Traces (need to contain id's of input and output artifacts, aswell as computation done to arrive at output artifact). 
                                    Traces then can be used by the Observability system to geenrate lineage of artifacts and store them in a lineage store(Tools: Lineages: OpenLineage; Traces: Istio, Odigos, Jaeger, Zipkin)
                                        1. Reactive and System-wide: Main artifacts chains: IO samples or "runs" of your system (where a run is all IO that happens between components, based on a triggering event, and where the last output is the one that doesnt trigger anything to run or based on a custom condition (e.g., frm user request to delivered prediction, from new raw data to stored featured datapoints in feature store, from configuration change in State System to new config log & new IO handling of some component/subsytem)). Note: distributed tracing requires attaching IDs to applicaiton protocols and propagating the same ID in downstream app protocols caused by the original one. This generally requires support from the networking app layer API tool (e.g. FastAPI) which needs to see which requests/streaming messages trigger other requests/responses/streaming messages;
                                        2. Active and component-wide: IO flows fo a specific component interacting with other compinents to analyze it. E.g., analyze behaviour of an LLM Agent.
                                    4. Events (Event Streaming + Event Sourcing)
                                    5. Errors
                            2.  ML System State Store (receives and versions Data and Config states of the ML System so that when you want to replay the system, to can do it from the last checkpoint, effectively saving a bunch of computation. you would have to do otherwise if you were to replay the ML System from its birth.)
                                1. Config Store (Tools: Apollo, NACOS, Salt, Configu, OmegaConf Hydra, DynaConf)
                                2. Schema Store (Tools: Liquibase)
                                3. Data Store (Tools: (1) Event sourcing/storing: EventStore, realtime; (2) Backup versioning: LakeFS + object storage or DB)
                                4. Deployment Store (e.g., k8s deploy state)
                                5. API Store (stores history of APIs for each service)
                            3. Replay Job (enables the engineer to replay the entire system from any point in time, by using the codebase version control & the production state, and get point-in-time correct artifacts)
                                1. Config replay: dispatches timely triggers to the state store to update system configs
                                2. Data replay: 
                                    1. Data Backup Shortcut. Uses available backup data + logs since that backup to reconstruct data state of services without having to go through the whole lineage cain that start with streaming data in data collection system
                                    2. Event Simulation. Simulates data sources by using historical event data (stored in Data Lake), being timely "produced".
                                        1. Simulated data collection stream producers and frontends
                                        2. Simulated storage changes (e.g., repo pushes, S3 updates or DB dumps)
                            4. Artifact Lineage Store: insert/read-only (Tools: Pachyderm, OpenLineage) (its job is to provide the lineage of all artifacts generated within the ML System. It usefull to aswer questions about how a specific artifact came to be, which is usefull for governance, debugging, finding impovements, etc) When ML System components generate some artifact and send it somewhere, it sends info on the processing it made and where it put the output (along with metadata) to the the Observability System. Observability System listens to this and builds up lineages. When Artifact Lineage Store receives some lineage ending in location C and other beggining in location C it automatically merges into a bigger lineage, this goes on, until the whole ML system is just a few big lineages working in parallel
                                1. Components
                                    1. Backend:
                                        1. Serves Frontend with a given artifact's lineage metadata. communicates with frontend & DB. Receives requests from Frontend, gets data from DB and returns to Frontend
                                        2. Registers a given data artifact location: receives artifact location; generates artifact ID, gives back artifact ID to the client subsystem/component and stores <ID, Location>
                                    2. DB: stores <data aritfact ID, data artifact versions, data artifact lineage for each version>. Organized by data source.
                                    3. Frontend
                                2. Capabilities
                                    1. For a given ID-full artifact: return its lineage
                                    2. Advaced Queries:
                                        1. Artifact with lineage containing a specific step (specified by its name) in some time window
                                    3. Diff of Lineages
                                    4. Analytics of latency of components & susbsytems
                                    5. Defining lineage (pipelines that were run using some data d as input) expectations (how diffs of specific lineage subsets should look like (e.g., for training-serving skew), avg latency of specific subsets of lineages) and triggering alerts/notifications (along with details of the problem) to send to Monitoring System when these expectations dont hold
                                    6. Deletion of all traces of a specific user's data
                                        1. Identify raw data location & artifacts geenerated from it
                                        2. If data artifacts: request the data source controller deletes the user's data
                                        3. If model artifacts:
                                            1. If differntally private models: no worries
                                            2. Else:
                                                1. Force retrain the model (model unlearning is also a new method for this)
                                                2. Disqualify previous models trained on that user's data in the Model/Prompt Registry (this will automatically make the new model the current best model in the Model/Prompt Registry which will trigger model CI/CD)
                            5. Post mortem management: this is where engineers create, save, read and edit operation problem cards. Operation problem cards contain:
                                1. Time of problem
                                2. Type of problem
                                3. Location of problem (which service)
                                4. Affected depedent services + level of how they were affected: not working correctly, not working correctly for some cases, working but with bad performance
                                5. Magnitude of the problem
                                6. If the problem was solved (resolved) or not (pending)
                                7. Explanation of problem
                                8. Who was alerted
                                9. Who was resposinble to deal with it or who dealt with it (if resolved)
                                10. Actions took to resolve it (if resolved)
                                11. Lessons learned to avoid similar problems in the future
                    3. ML Project Management (Governances) (Tools: (1) Open source: Kubeflow, MLFlow; (2) Managed: W&B, Comet ML, Arize AI, DasgHub, all MLOps platforms)
                        1. Responsabilities: who is responsible for e;ach subsytem or subtasks in the MLOPs project
                        2. Actions: who did what
                        3. Contacts: enable any person to reach an expert on a topic
                        4. Documentation: design docs (not official docs for outsiders, this one is more informal and used as a project management tool)
                        5. Shedules: meetings and deadlines
                        6. Access Control: each engineer has centralized place to manage all his keys. Managers can modify user roles, access policies & give/take away access of specific engineers to specific resources.
                    4. Compliance (ML System has to comply to regulations)
                        1. External Audit results
                        2. Certificates Management
                        3. Periodical Internal Auditing & Report generation (e.g., GDPR-like compliace)
                            1. Results
                                1. Automatically generated
                                2. Huuman aid: Manual or Hybrid
                            2. Code
                9. Procedure Specifications (e.g config file, API, GUI)
                    1. ML Training (Tools: Ludwig, NNI, Autogluon)
                        1. For Bulidng your own task-specific model
                            1. Training Mode (only Batch Offline Learning)
                                1. Training Mode==Experimentation
                                    1. Data
                                        1. Source
                                            1. ML Data whareshouse (always scheduled)
                                            2. A file somewhere (need API endpoint & file format) (e.g., S3)
                                            3. A Database (need db credentials & table name)
                                        2. Check schema: check if data schema remains the same (e.g of playstore recommonder system: features might be apps on the playstore, if your model was trained with data that included app x which is no longer present, you need to change the training algo and retrain it). If modified schema: alert programmer (whats, email, slack, or dedicated alerts manager like: keep) and send to (generally stream) Monitoring System; else continue
                                        3. Injection
                                            1. If experimenting on small scale before large scale deployment: % of dataset to use for small scale training
                                            2. If biased data (basically detecting that we will have Data Distribution shift (Note: sudden shifts are often indicative of data bugs) before it actually happens): Importance Sampling (upsample groups where ratio probDeploy/probTraining is high andsubsample when it is low)
                                    2. Pre-processing plug-in (apply some function to data as preprocessing)
                                        1. Manual preprocessing (data wrnagling + feature engineering)
                                        2. If training an Ensemble: Apply Other Models
                                    3. Output Model: store resultant models in Model/Prompt Registry
                                2. Training Mode==Retraining (only batch, does not apply to online learning. For online learning, retraining is done by the Inference Service itself internally)
                                    1. Data
                                        1. Source: Feature Store (Important: needs to get features from the Feature Store, using same namespace of the model version that is being run in prod and we want to retrain)
                                        2. Checks 
                                            1. Schema: check if data schema remains the same (e.g of playstore recommonder system: features might be apps on the playstore, if your model was trained with data that included app x which is no longer present, you need to change the training algo and retrain it). If modified schema: alert programmer (whats, email, slack, or dedicated alerts manager like: keep) and send to (generally stream) Monitoring System; else continue
                                            2. If retraining on recent windows: compare the aggregate statistics between the old and new windows to ensure there aren’t any bugs
                                        3. Injection
                                            1. If Data Distribution shift (Note: sudden shifts are often indicative of data bugs) (Obs: real world can actually be a mixture of both types below, seasonality can be a bit present and but if you get data from a long time ago it will not be very faithfull)
                                                1. If process/concept shift: can't use data before shift for training
                                                    1. If not leveraging seasonality
                                                        1. How to define new training data:
                                                            1. By time: get recent time window of datapoint
                                                            2. By shift: Get datapoints since shift started
                                                            3. By new data: Use datapoint gathered since last retraining
                                                        2. Old data vs new data protocol
                                                            1. Just train with new data
                                                            2. Just add new data to old data
                                                            3. Exponential Weighting (can be umplemente either through upsampling or actual weighting in the loss function)
                                                    2. If leveraging seasonality (when process shift is cyclic)
                                                        1. Find season its in now
                                                        2. Get old model from this season
                                                        3. Get data on this season since old model was trained or with a cap on time (Tools: dlt)
                                                2. If input/covariate shift (after not seeing any issues in data pipeline): can use data before shift for training
                                                    1. Leveraging old data: Importance Sampling (upsample groups where ratio P_deploy(X=x)/P_old_training(X=x) is high and subsample when it is low); or
                                                    2. Leveraging new data
                                                        1. If not leveraging seasonality
                                                            1. How to define new training data:
                                                                1. By time: get recent time window of datapoint
                                                                2. By shift: Get datapoints since shift started
                                                                3. By new data: Use datapoint gathered since last retraining
                                                            2. Old data vs new data protocol
                                                                1. Just train with new data
                                                                2. Just add new data to old data
                                                                3. Exponential Weighting (can be umplemente either through upsampling or actual weighting in the loss function)
                                                        2. If leveraging seasonality (when process shift is cyclic)
                                                            1. Find season its in now
                                                            2. Get old model from this season as hot-start
                                                            3. Get data on this season since old model was trained or with a cap on time
                                            2. Else: use all data you have
                                    2. Pre-processing plug-in: If training an Ensemble, apply Other Models on X
                                    3. Output Model: store resultant models in Model/Prompt Registry
                                3. Training Mode==Tuning
                                    1. Data
                                        1. Source
                                            1. ML Data whareshouse (always scheduled)
                                            2. A file somewhere (need API endpoint & file format) (e.g., S3)
                                            3. A Database (need db credentials & table name)
                                        2. Check schema: check if data schema remains the same (e.g of playstore recommonder system: features might be apps on the playstore, if your model was trained with data that included app x which is no longer present, you need to change the training algo and retrain it). If modified schema: alert programmer (whats, email, slack, or dedicated alerts manager like: keep) and send to (generally stream) Monitoring System; else continue
                                        3. Injection
                                            1. If biased data (basically detecting that we will have Data Distribution shift (Note: sudden shifts are often indicative of data bugs) before it actually happens): Importance Sampling (upsample groups where ratio probDeploy/probTraining is high andsubsample when it is low)
                                    2. Pre-processing plug-in (apply some function to data as preprocessing)
                                        1. Manual preprocessing (data wrnagling + feature engineering)
                                    3. Hot-start model
                                    4. Types of tuning
                                        1. Hyperparameter Tuning
                                            1. Hyparparameter Tuning Algorithm
                                            2. Space of hyperparameter search
                                        2. Dataset Tuning: 
                                            1. Pre-buil datasets: Pointers to dataset versions
                                            2. Datasets built on-demand: ELT-based dataset versions
                                    3. Output Model: store resultant models in Model/Prompt Registry
                            2. Wrappers
                                1. Hot-start Wrapper
                                    1. From where it left off: store periodically in persistent storage (e.g., S3, Hive, HFS/Gluster, etc) the state of the training procedure (current parameters)
                                        1. To be crash-resilient
                                        2. To enable stopping the training procedure without losses
                                    2. Loading external parameters
                                        1. From other training jobs
                                        2. For transfer learning
                                            1. As Prior: not freezing any parameters
                                            2. Fine-tuning: freezing a portion of the parameters
                                            3. Scaling: Freezing all parameters and adding new parameters to learn to the model, this addition of parameters being defined by scaling hyperparameters.
                                2. Ablation Study Wrapper (should allow visualizations on how each ablated part is affecting our metrics) (objective: disprove parts of your hypothesis, maintain only important stuff, remove noise)
                                    1. Feature removal (runs several instances of the Training Algorithm with k features, each one of them removing a diferent feature k number of times, see there are r features that are not so relevant and remove them. Start the same process now with k-r features). If removing some feature degrade significantly model's performance, might suspect there is a leakage, so alert programmer and send to (generally stream) Monitoring System
                                    2. Model removal (removig parts of the model, lowering computation. (e.g., removing layers of a depp learning model or removing models in an ensemble))
                                3. Training Profiling Wrapper (should allow visualizations) (Fine grained measure of some resource through operators of the model). _Note:_ results for measuring this resources will vary do to uncertainty in hardware implementation (assuming same <machine, OS, processor used, cgroup>): branch prediction, threads, concurrent processes, process priority, etc. So these should be measured in a statistical sense: drawing a few samples then compute mean a std deviation.
                                    1. Time (time spent on each stage of model training epoch and generates report) (time per perator) (if one operator is a bottleneck you ca decide to modify it and train again)
                                    2. Energy (measure energy consumption of each operator involved in training and generates report) (energy per perator) (if one operator is a bottleneck you ca decide to modify it and train again) (Tools: codecarbon, scaphandre, mlco2)
                                4. Diagnosis Wrapper (Tools: WeightWatcher): tools that give you training-time interpretability into the behaviour of your model
                                5. If there can be data privacy attacks: Differential Privacy Wrapper (Tools: google/differential-privacy, TF Privacy). Adding Noise to learning process in the way that guarantee that the output of the model will be aprox the same with or without any single input, degrading as less as prossible corrected predictive power --> this implies that specific datapoints cannot be memorized by the model and consequently that attackers exploit this memorization.
                                6. If Featured Input must be encrypted: Homomorphic Encryption Wrapper (Tools: SEAL, HElib, tf-encrypted)
                            3. Training step (generally will runs several training jobs, then: (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) + k8s combo on Cloud PaaS or IaaS, with TF/Pytorch distributed training to do data parallism. Can use spark/koalas MLlib also)
                                1. Hardware Requirements
                                    1. Number of Nodes (Logical Machines)
                                        1. Single node (VM, container or physical machine)
                                        2. Distributed
                                            1. Virtually Distributed (can be multiple containers or VMs)
                                            2. Physically Distributed (multiple physical machines. Generally its virtually distributed aswell)
                                    2. Type of processor
                                        1. CPU
                                            1. Single thread (Most basic, any language/framework does it)
                                            2. Multi-thread
                                                1. Single process multi-thread (Tools: OpenMP)
                                                2. Multi-process that also be distributed (Tools: OpenMPI)
                                        2. GPU (Tools: CUDA, OpenACC, OpenMP, OpenCL, bitsandbytes, Bend, Halide, Chapel, ThunderKittens)
                                        3. TPU (Google Cloud/Colab)
                                        4. Multi-Processor (often called Heterogeneous Computing) (Tools: OpenCL)
                                2. Training Algorithm
                                    1. Model/Estimator Class/Family (Space of models where we will search for the best configuration, through optimization) (assumptions of the model (hard priors/assumptions, meaning there has 0 chance of reaching an other model class/family by walking in the space of parameters) have to match as much as possible assumptions on the process that geenrates the data of interest)
                                        1. By capacity
                                            1. Traditional ML Models (Less capacity or degrees of freedom)
                                            2. DL Models (More capacity or degrees of freedom)
                                        2. By probabilistic approach
                                            1. Frequestist (getting point-estimates on parameters just doing maximum-likelihhod)
                                            2. Bayesian (Getting posteriors on parameters)
                                        3. By inference speed
                                            1. Fast
                                            2. Slow
                                        4. By type of probabilistic modelling
                                            1. Generative (models P(Y == y, X == x | ;theta), for Bayesian Supervised Learning this is the evidence term;that can be used to also get P(Y == y | X == x))
                                            2. Discriminitive (models P(Y == y | X == x; theta), for Bayeaisan Supervised Learning this is the likelihood term)
                                        5. By Task
                                            1. Regression
                                            2. Classification
                                                1. One label
                                                2. Multi-label
                                        6. By type of problem (domain)
                                            1. Physical world (e.g., Images/Videos have certain architectures (CNNs) that take advantage of worlds inductive biases)
                                            2. Sequence data
                                                1. Output is sequence of input: Time-Series
                                                2. Output is not sequence of input: Seq2Seq
                                        7. When features are noisy: by feature noise propagation to target (you want models that amplify less the noise in features)
                                    2. Training Patterns (Optimization method or Learning)
                                        1. Closed-Form
                                            1. Frequentist/Deterministic (e.g.,  lin regression using MSE when there isnt much data)
                                            2. Bayesian/Probabilistic
                                                1. Exact Posterior Inference (e.g., using conjugate priors)
                                                2. Exact Maximum Likehood Estimation (MLE) (e.g.,  gaussian regression (fitting gaussian))
                                                3. Exact Maximum a Posteriori Estimation (MAP)
                                        2. Numerical Methods
                                            1. Approaches
                                                1. Frequentist/Deterministic
                                                    1. Empirical Risk Minimization (Traditional Supervised Learning with Loss Function)
                                                        1. Vanilla
                                                            1. Regression: MSE
                                                            2. Classification: Cross-Entropy Loss
                                                        2. Customized
                                                            1. Constrained Optimization (e.g., imposing robustness or fairness contraints (Tensorflow contrained optimization is a great tool))
                                                            2. Loss function dependent other things:
                                                                1. Dependent on X
                                                                2. Depenent on P(Y == y)
                                                                3. Dependent on types of errors (e.g., classifying people as monkey having higher loss)
                                                    2. Separating Hyperplane (SVM)  
                                                2. Bayesian/Probabilistic
                                                    1. Aproximate Maximum Likelihood
                                                        1. Simple: Maximum Likelihood Estimation (MLE) (_Note:_ Vanilla ERM is a special case of MLE with Normal Distribution)
                                                        2. With Latent Variables: Expectation-Maximization (ER Algorithm)
                                                    2. Aproximate Maximum a Posteriori (MAP)
                                                    3. Aproximate Posterior Inference (Is the most complete because gives you complete understanding of parameter estimation but t is the hardest to do due to intractability of the denominator integral in most cases)
                                                        1. Sampling methods: MCMC generally
                                                        2. Optimization methods: Variational Inference generally
                                                    4. Information gain (Decision Trees)
                                            2. Modifiers
                                                1. Quantization-aware training
                                                    1. Loe precision training
                                                    2. Mixed precision training
                                                2. Sparsity-aware training (introduce sparsity gradually over training)
                                    3. Training Artifacts (users can plug in or use defaults)
                                        1. Loss Function
                                        2. Optimizer
                                3. Practical Tips
                                    1. If classification: supposing classes in {A,B,C} add another class D == not in {A,B,C} (then your model can handle unexpected input)
                            4. Training Steps Definition
                                1. Data initalization:
                                    1. Definition of where to get data, specifically where to get training, cross-validation, test datapoints.
                                    2. Random seed for making training batches
                                    3. Dataset Split (Important: after evaluating on cv set, if you want to make changes, need to do the dataset split again to avoid overfitting the cv test!):
                                        1. Typical case
                                            1. Statistically comparable sets (or Balanced split)
                                                1. If small dataset: typically people do random sampling but stratified sampling is better because you can ensure with more confidence statistically comparable sets. E.g., If classification: aim to have same label distribution across splits.
                                                2. If larger dataset: random split will very likely make a balanced split, no worries
                                            2. Sequential data: splits have to follow sequence (e.g., time series data)
                                        2. Worst-case Analysis
                                            1. Hard cases: manually curate the hardest test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) you can think of, only using the data you have (not artifically creating more data) (One approach is to get datapoints of minority groups, another is to get datapoints near decision boundaries)
                                            2. OOD Cases: curate an ou-of-distribution test set that will evaluate how much the model can extrapolate/how much it is overfitting the datset
                                        3. Balanced Analysis: split train/test with same percentage of hard examples (ones were model made mistake, preferebly large mistakes)
                                2. Model initialization
                                    1. Same hyperparameters as old model (most common, when window of data approach is used): uses hot-start from old model's parameters.
                                    2. Different hyperparameters (typically when we are training with more data than before we often want to increase capacity of our models so we increase number of parameters by altering hyperparameters)
                                        1. Hot-start: transfering parameters from old model to new model in some way (hard problem)
                                        2. From Scratch: ignoring old models parameters and do random initialization of parameters. Define random seed for weight initialization.
                                3. Load data features on demand (per batch) from ML Data whareshouse (when you still havent built the data pipeline that store features in the Feature Store) or Feature Store (if you already have it working)
                                    1. Load raw data
                                        1. Most recent data: training the whole model with recent window of data
                                        2. All data: training the model with entire dataset until moment (computationally problematic)
                                    2. If experimentation: do Preprocessing
                                        1. Data wrangling
                                        2. Feature-engineering
                                4. Training
                                    1. Stopagge criteria. Possible criteria:
                                        1. Main
                                            1. Objective: Validation-loss convergence (usually the primary criteria)
                                            2. Constraints
                                                1. Time (cant wait to much to have the model. E.g we might be retraining and a stale model is still operating in production)
                                                2. Resource-consumption (even though convergence treshold wasnt reached, we might want to limit)
                                        2. Extra
                                            1. Validation Loss (e.g., we want an underperforming model to stop before a well performing model with same convergence)
                                            2. Training-loss (for early stopping, to avoid overfitting)
                                    2. Steps
                                        1. Overfitting (fitting noise/doig great at training set error but doing bad at cv set error): keep increasing parameters/degrees of freedom of the model if it does not overfit, same as the question of how to scale the model if more data would arrive.
                                        2. Regularization
                                            1. Modify Loss Function
                                                1. Limit Parameters (Penalty is given if parameters are to high, in the form of a term in the loss function)
                                                    1. L2 Regularization (strong)
                                                    2. L1 Regularization (strong)
                                                2. Limit Confidence: Entropy Regularization (-entropy of the output distribution in the loss function)
                                            2. Modify Training algorithm
                                                1. Early stoppage (strong) (interrupt training in the middle)
                                                2. Dropout (strong) (setting a random set parameters to 0 at each batch)
                                                3. Injecting noise into weights (weak)
                                            3. Modify Data Samples
                                                1. Injecting noise in data samples (weak)
                                                2. Label smoothing: Softening hard-labels (weak)
                        2. For Fine-Tuning FMs (and maybe then doing distillation)
                            1. Fine-tuning
                                1. Data
                                    1. Source
                                        1. ML Data whareshouse
                                        2. Feature Store
                                        3. A file somewhere (give the API endpoint for it)
                                    2. Check schema: check if data schema remains the same (e.g of playstore recommonder system: features might be apps on the playstore, if your model was trained with data that included app x which is no longer present, you need to change the training algo and retrain it). If modified schema: alert programmer (whats, email, slack, or dedicated alerts manager like: keep) and send to (generally stream) Monitoring System; else continue
                                    3. Injection
                                        1. If biased data (basically detecting that we will have Data Distribution shift (Note: sudden shifts are often indicative of data bugs) before it actually happens): Importance Sampling (upsample groups where ratio probDeploy/probTraining is high andsubsample when it is low)
                                2. Pre-processing plug-in (apply some function to data as preprocessing)
                                    1. Manual preprocessing (data wrnagling + feature engineering)
                                    2. If training an Ensemble: Apply Other Models
                                3. Format (Template) Specifications:
                                    1. Input: Prompt Engineering (x --> prompt). How are is knowledge about the task + x going to be transformed into a structured prompt that maximizes some model metric (e.g., a corrected predictive power metric)
                                    2. Output: 
                                        1. Format Enforcement at the LLM: Formatted Training (any y --> restricted space of y's). (Formated Training: making LLMs substitute task-specific ML models. Restricts output to follow a certain format. In practice, what happens is that at each time step, when before any token could be the prediction, what happens is that only a few tokens can be predicted; the probabilities of the allowed tokens are normalized and then you sample just like before. This method makes it possible to train an LLM on any supervised task reliably. Note: the advantage of using an LLM is that you can insert prior knowledge of the task as text to help the model converge to the right place.)
                                        2. Format Enforcement at the consumer app:
                                4. API Configurations: each FM API lets you do API-specific configurations to the behaviour of the FM Service
                                5. Actual Training: FM's API endpoints to use (with all the necessary headers & payload specifications)
                                6. Resultant Model: store inference API endpoint for the model in Model/Prompt Registry along with metadata such as timestamp (e.g., timestamp, who pushed the model, poniter to datasets used for finetuning & more)
                                    1. Problem: since we are finetuning an existing pretrained model, there are a bunch of details of how this pre-trained model was built that we dont know
                                        1. Open source pre-trained models: we might not know the training data and some tricks that were used
                                        2. Not open source pre-trained models: (which might not be open-source): we dont know the trianing data, the architecture neither tricks that were used
                                        This requires that we poke a lot these models to enrich it with metadata that might help us in debugging/extending capabilties later.
                            2. Knowledge Distillation 
                                1. Pick a smaller model class (Note: generally the case, but not necessary for Knowledge Distilation, e.g., you can do the contrary, you can pre-train your model with data simulated with a smaller heuristic model, aka using Weak labels; then you can finetune with real data and be more data-efficient)
                                2. Pick hyperparameters for smaller model
                                3. Build proxy dataset of (x, y_hat) by probing the fine-tuned FM
                                4. Use proxy dataset to train smaller model with a hyperparameter tuning wrapper
                                5. Resultant Model: store resultant model in Model/Prompt Registry along with all necessary model metadata (e.g., timestamp, who pushed the model, pinter to datasets used & more)
                    2. Pipeline Generation: mapping tool-agnostic pipeline specification --> tool-dependent pipeline specification. (Tools: lineapy)
                        1. Configuration
                            1. Data pipelines to generate
                                1. Training Data Pipeline (Batch & Streaming)
                                2. Inference data pipeline (Streaming)
                            2. Target Tech stack
                                1. Language (e.g., python)
                                2. Framework/Libraries (e.g., spark/koalas)
                        2. Steps
                            1. Verification of modifications: only re-do things that were changed in the pipeline file, generally can reuse a lot of the last generated pipeline
                            2. Pipeline Generation
                                1. Optimization: Optimize pipeline file (''Typical data science workloads using NumPy, Pandas and TensorFlow run 23 times slower one thread compared to hand-optimized code (Palkar et al., 18)'')
                                2. Transfer: Produce production pipeline (Tools: (1) Batch Processing: Dask, Ray, spark/koalas, Fugue; (2) Stream processing: Storm, Flink, Kafka Streams, Spark Structured Streaming, Numaflow, KSQL, Beam, Pathway, Benthos, NiFi, Materialize, Decodable, quix streams, spark/koalas Streaming, hazelcast).
                            3. Cashing: Cash last pipelines generated
                10. User as Data Owner
                    1. Data Deletion/Takeout (Right to be Forgotten)
                        1. User requests for his/specific user data to be deleted
                        2. BFF/API gateway stores user deletion retrieval request details in some storage location (e.g., DB, S3, etc)
                        3. User Data Deletion/takeout task is triggered
                            1. Direct data deletion/takeout: request data of user A to be deleted/takedout from all stores.
                            2. Indirect data deletion: for models trianed on user A's data. Some options:
                                1. Actual Indirect Deletion 
                                    1. Differentially private models
                                    2. Retrain models without user A's data
                                    3. Do machine unlearning of user A's data
                                2. Evaluation: trigger task that evaluates if model knows things about user A
                                    1. If model doesnt know: pass
                                    2. Else: alert engineers and trigger plan B task
                                        1. Plan B tasks. Some options:
                                            1. Try same approach with different config
                                            2. Try a different Indirect Deletion approach
                            3. Deploy auto ml/online learning/federated learning services to user devices so that you can maintain de ml service but without sending user data outside of his device.
                    2. Compensation for stored data: a service that periodically queries the Feature Store to calculates how much data of each user is being stored, and composensate users (using their financial data stored in business db (e.g., authorization server's DB)) proportionally to that amount.
                11. Retraining job (Options: 1. A serivce that listens to retrain triggers (Tools: Mindsdb, Chat2DB, postgresml, cube, vespa, EvaDB, Alluxio); 2. Controlled by workflow orchestrator (I think is the best option) (e.g., Virtual ML Workflow Orchestrators: zenml, CLAIMED, Couler, kedro, hamilton, sqlflow; (2) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo), mleap, metaflow, flyte, sematic; Only for Batch Offline Learning (since in online learning retraining will be happening all the time, it needs to be inside Inference Service code)))
                    1. Triggers
                        1. Sources
                            1. Prevention (active): Continual Learning (_Note:_ You might think that Continual Learning can only increase computational cost wrt retraining by Mitigation. However, when you retrain continually, convergence is faster! ''Going from monthly training to daily training gives 45x cost savings and +20% metrics increas -- GRUBHUB (Alex Egg, 2021)'')
                                1. Types
                                    1. Peridic Clock
                                    2. New data in Feature Store: feature store sends ''\# new x featured labelled datapoints are available''
                                2. Threshold dependent on:
                                    1. How Dynamic the process is: if process is not so dynamic, then retraining frequency can be lower
                                    2. How important the ML service is to the applications consuming it
                                    3. Curves
                                        1. corrected predictive power
                                            1. (Business metric) vs (Corrected predictive power) curve
                                            2. (Corrected predictive power) vs (retraining frequency) curve
                                        2. Required Computation: amount of computation that will be required if retraining is decided (e.g., if you are deciding to retraing model A or not, see how many models depend directly or indirectly on outputs of model A, because they will have to be retrained aswell): (Computational cost/time) vs (retraining frequency) curve
                            2. Mitigation (passive): Monitoring actions
                                1. Types
                                    1. Automatic: corrected predictive power dropped, input/output drifts, etc
                                    2. Manual: programmer wants retraining, or allowed retraining after training algo was fixed to accomodate new input schema
                                2. Threshold dependent on:
                                    1. How important the ML service is to the applications consuming it
                                    2. Curves
                                        1. corrected predictive power
                                            1. (Business metric) vs (Corrected predictive power) curve
                                            2. (Corrected predictive power) vs (proxy for predictive power) curve
                                        2. Required Computation: amount of computation that will be required if retraining is decided (e.g., if you are deciding to retraing model A or not, see how many models depend directly or indirectly on outputs of model A, because they will have to be retrained aswell): (Computational cost/time) vs (retraining frequency) curve
                    2. If retraining our own Model: Automatic Training procedure generation (refer to Training procedure specification Interface in MLOps Setup) (which triggers ML Training CI/CD)
                    3. Elif using a FM:
                        1. Automatic Training procedure generation (refer to Training procedure specification Interface in MLOps Setup) (which triggers ML Training CI/CD)
                    4. Reconfigure Monitoring: send/stream to monitoring system a reconfiguration request (directly or through the state store that distributes configurations) to change the reference data distribution it using for detecting Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s in this model. Send a pointer to training data that was used in training this model.
            2. MLOps platform CI/CD
        2. ML Product (works on top of MLOps platform)
            1. ML Frontend (UI/UX) (Tools: Marvin, TypeChat, Vercel AI SDK, text-generation-web-ui)
                1. Development (output is changed repository)
                    1. Principles
                        1. Data Principles
                            1. Customer can delete all his data from our system if he wants
                            2. Customer can choose if he wants us to sell his data or not
                            3. Evade training on troll/malicous actor data (can use other ML service to detect this also)
                        2. Decision Making Principles
                            1. Make it clear it is an ML based decision
                            2. Make the tech limitations clear
                            3. Test different decision making tresholds and choose the best
                            4. Non-ML fallback
                                1. Model says its not shure
                                2. ML service goes down
                                3. Customer disables ML
                            5. Display to user that decision will take a while if ML service says it was not shure so sent to Human Expert to double check
                            In this case the client will have to either send periodic requests to get decision or connect via websocket type connection (best option I think)
                            6. Degree of decision uncertainty returned to user. Knowing the uncertainty can lead to positive and negative things like:
                                1. Can be usefull for different clients to decide their decidion tresholds
                                2. Very usefull for malicous actors to do privacy attacks and steal the model
                            7. Provide access to explanation of the decision
                        3. User Action Principles
                            1. Recognize a potential user action (e.g., changing color of button on user hover)
                            2. Aknowledging user action & stating consequences (e.g., after user provided feedback say that the system will improve by learning from the feedback)
                            3. Incorporate non-ML extensions to address limitations/errors that might occur (e.g., for a model that flags inapproapriate search queries from kids to parents, sometimes the parent sdont know what something means, so you can provide a button where the parent clicks and it searches in google the word)
                    2. Active User
                        1. Prediction Explanations
                            1. Passive
                                1. Natural Language Explanation
                                2. Feature Importance
                                3. Local Predictions around the user datapoint
                            2. Active: User can, interactively, play with model, varying the inputs (The problem is that this can easly make users change their behaviour, thus making data shifts happen more rapidly)
                                1. Real Model
                                2. Surrogate Model
                        2. User ML Feedback (Send to BFF that sends to Prediction Manager that sends to Monitoring Service and Stores in Feature Store)
                            1. If Natural/artifical ground truth from the environment/user: labels to the predictions will be available going forward in time 
                                1. Forecasting tasks: real output can be measure further in time
                                2. Generative tasks: user edits ml output if necessary
                            2. If no Natural/artifical ground truth from the environment/user
                                1. Direct feedback (Note: you should incentivize feedback from user with some kind of reward)
                                    1. Types
                                        1. Problem Identification
                                            1. Corrected predictive power 
                                                1. Only negative feedback (why not positive aswell? because when user is happy he is mreo concerned being happy than answering some feedback question, if he does aswer it will probably be because we wants to get rid of the message)
                                                    1. Model corections: free labels!
                                                    2. Only negative feedback: the user only tells you that the prediciton was bad, it is usefull to guide model predictive power monitoring and data labelling priotization
                                            2. Other problems: user reporting of important problems/weird behaviour (e.g.,  bias). 
                                            3. Problem formalization: (e.g., insted of recommending one movie, customer wanta a set of movies to pick from) this would fit in the Feedback System (call Feeback API)
                                        2. Problem Resolution
                                            1. Suplying fine-grained information. Make it easy for the user to directly supply more information about him that might help with personalized recommendations (e.g., a non common women gets recommended a lot of girl stuff but actually likes boy stuff so she gets angry)
                                2. Indirect Feedback
                                    1. Leveraging Proxy Singnals: mapping post-prediction activity on the platform to proxy labels and then seding publishing them (just like would do in the case of natural labels that you get from the environment later in time)
                        3. If Personalized System: Cold-start (In many tasks we want to learn from user data, but when the user just joined our app he doesnt have data. But we still want to show him some decent stuff, not random things.)
                            1. Ask user for initial help
                            2. Cluster User into some group
                            3. Collaborative Filtering (taking advantage of other users that like the same stuff)
                            4. Heuristics
                        4. User-faced inference config (e.g. for recsys: user can change config to avoid content of a specific topic or to only get recommended content he has never seen before). It will be then the job of the prediction managet to execute the right inference pipeline according to the inference config.
                    3. Communication
                        1. With Application Server
                            1. Request-Response Pattern
                                1. Requests
                                    1. Limit # of requests /user to evade trolls/malicious actors (this can also be done in the BFF/API gateway)
                                    2. Validate request in the Frontend (schema, data types)
                                    3. If request is triggered by user action: implement debouncing (e.g., if a child clicks 10 times in the button "predict" wihtout debouncing we would send 10 requests (wait, but doesnt http need to receive response before? Yes, but browser opens a bunch of tcp connections in parallel and in http2 you can send a bunch of requests in a row))
                                    4. Put a lot of metadata ("E.g. each request needs to come with more metadata that is not needed to call the endpoint (e.g. app version, use case id if the algorithm is used for multiple places, ab test ids, ect"))
                                2. Responses
                                    1. Predictions
                                        1. Model doesnt know --> activate ml fallback
                                            1. Simple ML Model
                                            2. Non-ML Fallback (Heuristics)
                                        2. Some prediction --> normal logic
                                        3. Error (e.g., no internet) --> activate ml fallback
                                            1. Simple ML Model
                                            2. Non-ML Fallback (Heuristics)
                                        4. Model asking for more information --> get more infomormation from user logic
                                    2. After receiving predictions
                                        1. Offline predictions (user dont need them now): cash them (in memory or storage)
                                        2. Online predictions (user needs them now): Do normal logic with it and then delete
                            2. Streaming Pattern (Consuming messages)
                                1. Predictions
                                    1. Model doesnt know --> activate ml fallback
                                    2. Some prediction --> normal logic
                                    3. Missing messages (e.g., no internet) --> activate ml fallback
                                2. After receiving predictions
                                    1. Offline predictions (user dont need them now): cash them (in memory or storage)
                                    2. Online predictions (user needs them now): Do normal logic with it and then delete
                        2. With Frontend Data consumers (through Stream Transport, e.g., Kafka, Mosquitto, RocketMQ, Pulsar, Kinesis, Pravega, Brooklin). Each message will contain:
                            1. Data itself (Appended with unique ID:= append(dataSourceID, uniqueIDGenerator)
                                1. Raw datapoint & Label ((X,Y))
                                2. Just Raw datapont (X)
                                3. More Raw datapont (X') & Reference to the datapoint it is linked to (X that was sent earlier)
                                4. Label (Y) & Reference to the raw datapoint it is linked to (X that was sent earlier)
                                5. Proxy label to raw datapoint X
                            2. Metadata
                                1. Noise (if present) to estimate noise do experiments with data source where you gather the data in aprox the same experimental conditions as the real world data collection condition in one of two ways (which in the end are the same thing): randomized way or expected value way --> always changing some factor that wouldnt be taken int account into you real world data collection, and do it n times.
                                    1. Types of noise
                                        1. Feature Noise (e.g., noise of sensors)
                                        2. Label Noise
                                    2. Estimating noise
                                        1. When you can know the ground truth experimental value (e.g., sensors): do n data gathering trials and see the noise around it. If the noise is biased, send the calibration necessary to unbias it.
                                        2. When you can't know the ground truth experimental value (e.g., asking people their income): do n data gathering trials and see the noise around the expected value (e.g., income data --> you ask)
                                2. Time data was gathered
                                3. Which Device gathered the data
                                4. Data Owners (if any)
                                5. Processing Priority of the datapoint(s) (the more dynamic the features that will be built using these raw datapoints, the more processing priority they get, until a treshold where it doesnt pay off trying to build offline (Training Data Pipeline) super fast changing features; so this job is done online by the inference data pipeline )
                                6. Weak label or strong label (or generalizing: degree of confidence in the label)
                    4. Bugs: bugs in the app as a whole can cause the user to change their behaviour, and this leads to Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s.
                2. Root CI/CD
            2. ML Experimentation (Tools: For builing features & Models (these tools are used wihtin the tasks of the "Experimentation Pipeline Framework/ML Workflow Orchestrator Wrapper"):
                (1) Experimentation Services:
                    (1) All-in-one
                        (1) Platform: Kubeflow, MLFlow, ClearML, Polyaxon, SuperDuperDB
                        (2) Framework: ZenML, CLAIMED, Kedro, Dstack, FuseML
                    (2) Piecewise
                        (1) Experiment Tracking: (1.1) open source: Tensorboard, Aim, MLflow tracking, dvc/fds + CML, Pachyderm, truelens, fasttrackml, MLTRAQ; (1.2) paid: W&B, DagsHub, Comet ML
                            (1) LLM Experiment Tracking: (1) open source: trulens
                        (2) Working with Notebooks: JupyterHub/Marimo + Jupyterlab-requirements + Papermill/Ploomber + pyparallel + parakeet + testbook + nbdev + elyra + jupytext + ReviewNB + NBQA + lmk + lineapy + nbdime + jupysql + jupyter-ai
                        (3) Pipeline builder/Virtual Workflow Orchestrators: just helps build workflow-orchestrator-agnostic pipeline, you then need a backend workflow orchestrator to execute it (Tools: Hamilton, Kedro, ZenML, CLAIMED, Couler, Metaflow, sqlflow). Note: not strictly necessary, since workflor orchestrator already help you with this aswell.
                        (4) Workflow Orchestrator/Pipeline Executor (_Note:_ every fully fledge exeprimentatio tool offers itw own inside) (very important to decouple experimentation stages in an organized manner. 
                            (1) If you have small data you can do computation on the the same machine you code: dvc/fds pipelines, ploomber; true workflow orchestrators (using them locally) such as: General workflow Orchestrators: Argo/Hera, Prefect, Airflow, Dagster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau, dagger; (2) Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro; (3) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo/Hera), mleap, flyte, sematic; (4) Data-driven Workflow Orchestrators: Pachyderm; (5) Notebook-based Wrkflow orchestrators: Ploomber
                            (2) If you have bug data/heavy workloads then you need to decouple the place of computation from the place of writing code and do computation in a dedicated cluster, and for that, you will need a true worflow orchestrator (metioned above))
                        (5) Additional setups: 
                            (1) Private datasets: Pysift
                (2) Distributed Processing Engines: Ray, Spark
                (3) Data Storage Abstraction (to make pipeline independent of actual storage implementations that may vary betwen ci/cd stages): Kedro
                (4) Preprocessing Libraries:
                    (1) Low-level: unstructured, sparrow
                        (1) Numerical workloads: (1) CPU: Numpy; (2) GPU: cupy, Numba, MatX
                        (1) Engines: 
                            (1) Engines: Pandas/Modin/Polars/Dask/cuDF(CUDA's GPU dataframe library)/Daft(non-tabular data (files) distributed dataframe library)
                                (1) Zero-copy data formats (with libraries to use them): Arrow, Iceberg, safetensors, avro, parquet 
                            (2) Single Interfaces to multiple Engines: ibis
                        (3) Data Validation: JSON Schema, Great Expectations, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation, DVT, data-diff, cerberus
                        (4) Profiling: ydata-profiling
                    (2) High-level: (1) General: optimus, scrub, dataprep; (2) CV-specific: DataGradients
                        (1) EDA
                            (1) General: Rath, ydata-profiling, sweetviz, spotlight, TaskWeaver
                            (2) CV: Kangas, fastdup, Voxel51, DendroMap
                            (3) Visualization: matplotlib, seaborn, facets, altair, bokeh, folium, holoviz, nomic, pygwalker
                        (2) Data Wrangling
                            (1) General
                                (1) Data Augmentation: YData Synthetic, Gretel Synthetics, AugLy, nlpaug, TextAttack, audiomentations, albumentations, inflect, snorkel, textaugment, kornia, batchgenerators, synthea
                            (2) CV: 
                                (1) Data Management: supervision
                        (3) Application-specific
                            (1) Time-series: temporian
                        (4) Feature Engineering: anovos, ppscore, featuretools, Feature Engine
                (5) Framework Libraries:
                    (1) Simple Models: Scikit learn
                    (2) Complex Models
                        (1) Powerhorses:
                            (1) Single-machine focused
                                (1) Low-level: Pytorch, TF, JAX, TRAX, AXLearn, MXNet, MNN, Volwpal Wabbit, Burn, candle (rust), MindSpore, DL4J, dlj, BeyondML, mlpack, flashlight, Paddle Paddle, Oneflow, mmengine
                                    (1) Add-ons:
                                        (1) Distributed Computing
                                            (1) Only Distributed Training (Note: single machine with multiple GPUs already requires it): 
                                                (1) Single-machine: 
                                                    (1) Framework-specific: TF Distributed Training, Pytorch Distributed Data Parallel (DDP)
                                                (1) Single-machine or Cluster:
                                                    (1) Your cluster:
                                                        (1) Any model
                                                            (1) Framework-agnostic: Horovod, Analytics Zoo, MosaiCML Composer, spark/koalas MLib, Dask Distributed Training, Petastorm, FedML, Kubeflow Model Training Operator
                                                        (1) LLMs: trlx, Megatron
                                                    (2) P2P: Hivemind
                                            (2) Distributed Training and Inference: Colossal AI, DeepSpeed, Paddle, Ray, Apache Singa, FlagAI
                                                (1) LLMs: BigDL
                                            (3) Only Distributed Inference (Note: single machine with multiple GPUs already requires it)
                                                (1) Your Cluster: NVIDIA Triton, TorchServe, exo
                                                (2) P2P: Petals 
                                        (2) Privacy: 
                                            (1) federated Learning: Pysift, TF federated,  tf federated, flower, nvidia clara, substra, OpenFL, FATE, FedML
                                            (2) Differential Privacy: TF Privacy, google/differential-privacy, Pytorch Opacus
                                            (3) Homomorphic Encryption: SEAL, HELib, tf-encrypted
                                        (3) Fairness: TF Model Remediation, TF Constrained Optimization
                                (2) High-level:
                                    (1) General
                                        (1) Pytorch-based: HuggingFace, Fastai, Catalyst, Pytorch Lightning, Pytorch Ignite, Ludwig
                                        (2) Tensorflow-based: Keras
                                        (3) JAX-based: FLAX
                                    (2) NLP: TextBlob, Spacy, opennlp, flair, Spark NLP, CoreNLP, PaddleNLP, NLTK
                                        (1) Topic Modelling: pyLDAvis
                                (3) Higher-level: AutoML frameworks that you just mess with config
                                    (1) Just Feature Engineering
                                        (1) TPOT
                                        (2) Featuretools
                                        (3) CleanLab
                                        (4) tsfresh
                                    (2) Just Models (NAAS)
                                        (1) ENAS-pytorch
                                        (2) HF Autotrain
                                        (3) HF Autotrain Advanced
                                    (3) Everything
                                        (1) Traditional ML-focused
                                            (1) auto-sklearn
                                            (2) mljar-supervised
                                            (3) PyCaret
                                            (4) SapientML
                                        (2) DL-focused
                                            (1) Ludwig
                                            (2) AutoKeras
                                            (3) NNI
                                            (4) AutoGluon
                        (2) High-level Libraries
                            (1) General: Hugging Face
                                (1) Add-ons:
                                    (1) Distributed Training & Inference: Accelerate, Optimum
                            (2) Specific 
                                (1) Domain-specific
                                    (1) NLP
                                        (1) Chatbot: NVIDIA NeMo, FastChat, Rasa Open Source, Lobe Chat, Superagent, Cheshire Cat, Botonic, Tock, Pipecat, wechaty
                                    (2) CV: OpenCV, SuperGradients, NVIDIA DeepStream SDK, Savant, Icevision, Kornia, FiftyOne, detectron2, Lightly SSL, timm, LAVIS, supervision, ImageAI
                                    (3) Time-series: tsai, darts, kats, Time-Series-Library, gluonts, skforecast, pydlm
                                        (1) Hierarchical Forecasting: hierarchicalforecast
                                    (4) Survival Analysis: lifelines
                                    (5) Audio: audiocraft
                                (2) System-specific
                                    (1) Recommender Systems: NVIDIA Merlin SDK
                    (3) FM-specific Libraries
                        (1) LLM-specific Libraries
                            (1) LLMs (single calls)
                                (1) Data Management & Dataset Construction: RedPajama-Data
                                (2) Model Building: HF Transformers, ml-4m, Ludwig, higgsfield, litgpt, dclm
                                    (1) Pre-trained models: openchat  
                                    (2) Pre-training: unilm
                                    (3) Finetuning: LLaMA-Factory, LMFlow, Simple LLM Finetuner, Axolotl, unsloth, langdrive, mistral-finetune, xtuner, LLM Finetuning Toolkit, OpenPipe, xTuring, LLaMA-Adapter, xTuring, felafax, torchtune
                                        (1) Alignment: NeMo-Aligner, alignment-handbook, 
                                            (1) RLHF: HF trl, trlx, OpenRLHF
                                            (2) DPO: direct-preference-optimization
                                        (2) Knowledge Distillation: DistillKit
                                        (3) AutoFinetuning: LLMstudio
                                        (4) Instruction Tuning: open-instruct, LLaMA-Adapter
                                        (5) Multimodal finetuning: multimodal-maestro, ms-swift
                                        (6) 4 Voice conversion: Retrieval-based-Voice-Conversion-WebUI
                                    (4) Scaling: TorchScale
                                    (5) AutoLLM: autollm, LLMstudio
                                    (6) Exploration: languagemodels
                                    (7) Output decoding: entropix
                                (3) Guardrails (Structure & Evaluation of LLM Outputs):
                                    (1) Server-side Structure Enforcement: Jsonformer
                                    (2) Client-side Structure Enforcement: Guidance, promptify
                                        (1) Prompt Structure Enforcement: 
                                        (2) Output Structure Enforcement: guardrails, outlines, Faster-Outlines, instructor, lm-format-enforcer
                                (4) Prompt Engineering: Promptflow, AgentHub, ChainForge, mirascope, Agenta, AI Test Kitchen, AnySolve, AnythingLLM, betterprompt, Chainlit, ChatGPT Prompt Generato, ClickPrompt, DreamStudio, Dify, DUST, Dyno, EmergentMind, EveryPrompt, FlowGPT, fastRAG, Guardrails, Guidance, GPT Index, GPTTools, hwchase17/adversarial-prompts, Interactive Composition Explorer, Knit, LangBear, LangChain, LangSmith, Lexica, LMFlow, loom, Metaprompt, OpenAI Playground, OpenICL, OpenPrompt, OpenPlayground, OptimusPrompt, Outlines, Playground, Portkey AI, Prodia, Prompt Apps, PromptAppGPT, Prompt Base, Prompt Engine, prompted.link, Prompter, PromptInject, Prompts.ai, Promptmetheus, PromptPerfect, Promptly, PromptSource, PromptTools, Scale SpellBook, sharegpt, SmartGPT, ThoughtSource, Visual Prompt Builder, MLFlow Prompt Engineering UI, spacy-llm.
                                    (1) Automatic Prompt Engineering: YiVal, gpt-prompt-engineer, DSPy, Zenbase Core, AutoPrompt, AdalFlow
                                (5) Local Hosting
                                    (1) Just LLM: Open Interpreter, transformers.js, llama-cpp-python, GPT4All, LocalAI, Ollama, ExLlamaV2, Dalai, BigDL-LLM, ExLlamaV2, koboldcpp, Xorbits Inference, danielmiessler/fabric, chatbox
                                    (2) RAG/Agents: TinyAgent, Jan, khoj, UFO, llama-agent, leon
                                        (1) Can take screenrecord as input: screenpipe
                            (2) LLM Orchestrators (mUltiple calls and context construction (RAG) orchestrated)
                                (1) Types
                                    (1) Agent-less Orchestrators: DSPy, Guidance, mirascope
                                    (2) Agent-full Orchestrators: AutoGen, Llama-index, zep, Langchain, haystack, Semantic Kernel, Dust, IX, MemGPT, BondAI, OpenAgents, SuperAGI, Agent Pilot, griptape, phidata, LLMCompiler, ragapp, CopilotKit, agentkit, gptscript, ten_framework, TaskGen
                                        (1) Multi-Agent Orchestrators: MetaGPT, ChatDev, Langroid, crewAI, ControlFlow, agent-zero, AgentVerse
                                            (1) Agent Communication: Camel
                                        (2) No-code Agent-full Orchestrators: LLMStack, Magick, Rivet, Tribe AI, PraisonAI
                                            (1) Automatic Agent Building: AgentK, ADAS
                                                (1) Financial Agents: openbb-agents
                                        (3) Mobile Agent-full Orchestrators: AppAgent
                                        (4) Web Agent-full Orchestrators: Huginn, LaVague, Agent-E, AgentGPT, skyvern, webllama, robotframework
                                        (5) Data Enginering Agent-full Orchestrators: Sparrow
                                        (6) RAG-specific:
                                            (1) Imperative: Verba, Anything LLM, Neum AI, gorilla, graphRAG, Cohere Toolkit, R2R, clip-retrieval, HippoRAG, ragbuilder, cognita, ragflow
                                            (2) Declarative (AutoRAG): AutoRAG
                                        (7) Graph-based: langgraph, HybridAGI, graphRAG
                                (2) Experimentation: langflow, flowise
                                    (1) Response size estimation: 
                            (3) LLM Explainability
                                (1) Token importance: 
                                (2) Prompt Engineering for explainability:
                                (3) LLM internal analysis: tuned-lens
                                (4) Visualization: inspectus, llm-viz
                        (2) Image/Video Generation Libraries
                            (1) Training: 
                            (2) Finetuning: VADER
                    (4) Distributed Hyperparameter Tuning: NNI, HyperOpt, Optuna, Ray Tune, KatibHyperOpt, Optuna, Ray Tune, Katib
                (6) Visualization: 
                    (1) Models: tensorboard, netron 
                    (2) Prompts: cometLLM 
                (7) Evaluation: Open Source: Phoenix, HF Evaluate, TF Model Analysis, TF Responsible AI Toolkit, Microsoft Responsible AI Toolkit (RAI), Giskard, Learning Interpretability Tool (LIT), PiML, ZenoML, Evidently, AI Verify; Proprietary: W&B, CometML, Kolena, Efemerai, Determined AI, Fiddler, OpenLayer, Arize AI, Etiq, Gentry, Credo AI, Modelblocks
                    (1) General
                        (1) predictive power: plain framework
                        (2) Fairness: TF Fairness Indicators 
                        (3) Privacy: TF Privacy Tests
                        (4) Interpretability/Explainability: SHAP, InterpretML, Alibi Explain, AI Explainability 360, TF What-If, Microsoft Responsible AI (RAI), interpret, Learning  Interpretability Tool (LIT), PiML, Captum, TF Lattice, nnsight, gemma-scope
                            (1) NLP-specific: TF language interpretability, ecco)
                            (2) Transformer-specific: Pythia
                            (3) Automated Interpretability: maia, sae-auto-interp, automated-interpretability
                    (2) Specific
                        (1) NLP: langtest, checklist
                            (1) LLM Evaluation: Chatbot Arena, OpenAI evals, deepeval, Bench, Pheonix, HELM, lm-evaluation-harness, truelens, guardrails, promptfoo, fiddler-auditor, trulens, ChainForge, benchllm, evidently, llm-autoeval, auto-evaluator, LLMZoom, moonshot, Inspect, empirical, alpaca_eval, lone-arena, opencompass, BIG-bench, Eureka ML Insights, artkit
                                (1) RAG Evaluation: deepeval, ragas
                                (2) Image Generation Evaluation: pytorch-fid
                                (3) Hallucinations: hallucination-leaderboard
                                (4) Software Engineering: SWE-bench, Spider
                                (5) Security and Safety: modelbench, Purple Llama, garak, PyRIT
                                    (1) Prompt Injections: rebuff
                                (6) Code Generation: bigcode-evaluation-harness
                                (7) vectorDB: VectorDBBench
                                (8) Science: sciml-bench
                                (9) speech-to-text: speech-to-text-benchmark
                                (110 Agents: AgentBench, agent-arena, OSWorld
                                    (1) WebAgents: WorkArena
                                    (2) Scientific Discovery Agents: DISCOVERYWORLD
                                (11) Reasoning: llm-reasoners
                                (12) Performance: llmperf
                                (13) LLM-powered Evaluation: prometheus-eval, EvalLM, evalgen, cappy, arena-hard-auto
                        (2) CV: efemarai
                1. Experimentation Environments (Tools: open-source: devspace, okteto, telepresence, gitpod, Tilt, coder, che, daytona; managed: github codespaces)
                    1. Types
                        1. Dev: Dataset Construction and Model Building environment
                        2. Eval: Model Evaluation environment
                    2. Guide
                        1. Local: ML Engineer's Logical Machine
                            1. Setup (Tools: envd)
                                1. Build a Dev Image to work on
                                    1. Tools (e.g. poetry)
                                        1. CLI tools
                                        2. GUI tools
                                        3. Libraries
                                    2. Client-side caching (inside your client libraries): caches your most used requests
                                2. Send experimentation/dev image to private image registry
                                3. Run experimentation/dev environment as a Docker/Podman/Contaierd/runc container
                            2. Go in, work & go out of your local experimentation/dev environment (Tools: vscode extension)
                                1. Option 1: k8s cli. Working (executing commands & interaacting with files), via k8s cli, inside a <pod,container> dev environment in k8s (Disavantadge: might not be secure to use open up a git networking connection in your cluster; Advantage: easier & more native)
                                2. Option 2: sync, ssh & mount. Working (executing commands & interaacting with files), via file sync (Tools: bitsail) & SSH, inside a logical machine which has somefolder mounted to a <pod,container>, then execute commands inside <pod,container> using k8s cli
                            3. Support for:
                                1. Authentication (e.g., we need to know which engineer is requesting workloads to be run on the offline ephemeral workloads cluster, because each engineer has a computational budget). This can be done via container environment variable at container start and then having http (or other app protocol) client libraries that support modifying any request to include authetication credentials.
                        2. Remote: Cloud Logical Machine: more specifically, Dev Logical Machine
                            1. Setup (Tools: envd)
                                1. Build a Dev Image to work on
                                    1. IDEs (e.g., VSCode)
                                    2. SDKs (e.g., Android SDK)
                                    3. Tools
                                        1. CLI tools (e.g., git)
                                        2. API tools (e.g., notebook server)
                                        3. Libraries (e.g., pytorch)
                                2. Send experimentation/dev image to private image registry
                                3. Deploy experimentation/dev environment as a kubernetes (using Shipwright, Skaffold/Flux/ketch/kubevela/kubeblocks, Lens/Kubeapps/porter-archive, Rancher, Istio/Kiali/linkerd2/Cilium/Consul, Helm/Kustomize/timoni/JuJu/cdk8s/kpt, yamllint/Carvel/kubesec/kubeaudit/kubeconform/kube-linter/polaris/conftest/Kubescape/Kyverno/OPA/Datree/Kubevious, SchemaHero/Datashim/Velero, Karpenter, Sealed Secrets/Kubernetes Secrets Store CSI Driver/External Secrets, Metacontroller)/OKD/Kubeflow <pod, container>
                            2. Go in, work & go out of your remote experimentation/dev environment (Tools: vscode extension)
                                1. Option 1: k8s cli. Working (executing commands & interaacting with files), via k8s cli, inside a <pod,container> dev environment in k8s (Disavantadge: might not be secure to use open up a git networking connection in your cluster; Advantage: easier & more native)
                                2. Option 2: sync, ssh & mount. Working (executing commands & interaacting with files), via file sync (Tools: bitsail) & SSH, inside a logical machine which has somefolder mounted to a <pod,container>, then execute commands inside <pod,container> using k8s cli
                            3. Support for:
                                1. Authentication (e.g., we need to know which engineer is requesting workloads to be run on the offline ephemeral workloads cluster, because each engineer has a computational budget). This can be done via container environment variable at container start and then having http (or other app protocol) client libraries that support modifying any request to include authetication credentials.
                2. Fully-fledged Manual Offline Evaluation (Engineer & Subject Matter Expert receives notification that model already is in the Model/Prompt Registry and passed through automated evaluation) (_Note:_ if some of these evaluations can be automated (at least to a point where a human is needed only to give an approval) then they can move out to CI/CD) (Note: the more we use the test set and iterative on our model based on it, the more we risk overfitting the test set.  Therefore we should impose a limit of n uses of the test set, where n is a small number. After n, we need a new test set.)
                    1. LLM-specific evaluations (Tools: OpenAI evals, HELM, lm-evaluation-harness, truelens, checklist, guardrails, promptfoo)
                        1. Validation against templates & regex
                        2. Predictive power: measuring distance of outputs wrt test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) targets
                            1. Via distance between embeddings
                            2. Via another LLM
                        2. Evaluation of prompt and output innapropriateness (e.g toxicity, trolling, private data being exposed, intellectual property being robbed)
                        3. Sensitivity Analysis (sensitivity of output quality & size/price)
                            1. Prompt changes (how output changes if prompt changes a little)
                            2. LLM changes (How output changes if LLM is changed or the same LLM but with different config)
                        4. Prompt injection
                            1. Integraty attacks
                            2. Privacy attacks
                            3. Availability attacks (for agents)
                        5. Output size/cost
                        6. Retrieval Power (how well does the retriver of a RAG retrive the adequate set of docs for user inputs)
                        7. Embedding Power (how well does the distance metric betwen embeddings reflect the actual semantic similarity between docs)
                        8. Steerability (evalauting prompt importance models, aka feature importance for LLMs, the goals is to estimate how the LLM can be steered, which type of prompt make it change the output in a certain way)
                        9. Stochasticity: make a bunch of llm calls with the same prompt, and see measure output variability
                    2. General
                        1. Steps
                            1. Aggregate predictive power metrics (e.g., accuracy, recall, fi-score, AUC, etc)
                            2. Error Analysis (seeing in which slices of data there are more errors and try to diagnose why. It is usefull also to go byond current features, and add tags to datapoints, then you can slice by tags also. "What fraction of errors have that tag? Of all data with that tag, which fraction is miscalssified? What fraction of all data has that tag? How much room for improvement is there with data with that tag? (Usually human level performance)"). How to identify slices of data: (Tools: Microsoft Responsable AI (RAI))
                                1. Automatic
                                    1. Decision Tree (maimizes information gain/minimizes entropy on p(E=e|X=x) where e is the error (that is binary for classficaition, and 1 means that the model made the wrong prediction for that datapoint)
                                    2. Search
                                    3. Clustering
                                    4. Bias-driven (seeing if the model presents some social dangerous bias)
                                        1. Analyzing predictions to special subsets of input data (e.g.,  recommending black people a lot of ...)
                                        2. Analying predictions in general (e.g.,  recommending a lot of male ...)
                                    5. Confusion-matrix driven (when you are worried with recall/precision on some slice of the data)
                                2. Manual
                                    1. Heuristics (requires domain knowledge)
                                    2. Visualization
                            3. Inference Profiling (Tools: (1) Local Profiling: pytorch profiler; (2) Cloud profiling: octoml-profile)(Fine grained measure of some resource through stages of the model). _Note:_ results for measuring this resources will vary do to uncertainty in hardware implementation (assuming same <machine, OS, processor used, cgroup>): branch prediction, threads, concurrent processes, process priority, etc. So these should be measured in a statistical sense: drawing a few samples then compute mean a std deviation.
                                1. Time: Fine grained time spent on each stage of inference (time per model operator) (if an operator is a bottleneck you can decide to modify/remove it and do training again)
                                2. Energy: Fine grained energy consumed on each stage of inference (tenergy per model operator) (if an operator is a bottleneck you can decide to modify/remove it and do training again) (Tools: codecarbon, scaphandre)
                            4. Robustness (distribution shift or regions with few datapoints are the problem here)
                                1. Natural (Normal inputs model should cope with)
                                    1. Smoke tests
                                        1. Minimum Functionality tests (take inputs to extremes, obvious cases)
                                        2. Invariance tests
                                            1. Random Perturbation tests (add noise to an input that shouldnt change much the output)
                                            2. Non-random Perturbation tests (usually called Invariance tests, but becomes confusing) (appy non-random perturbation to input that shouldnt change much the output)
                                        3. Directional Expectation tests (change the input in a way that should change the output in a known way)
                                        4. Template tests (generate examples based on a input template. The output is a function of what you put in the template (inclduing cases where output stays the same. E.g., for NLP ''Find me a [CUISINE] restaurant within [NUMBER] miles [LOCATION].''))
                                    2. Long tail (e.g., for images, foggy images, images with finger)
                                2. Adversarial (Malicous Actor tries to make model deliver wrong decision) (Tools: Adversarial Robustness Toolkit (ART), Foolbox)
                                    1. Semantic contraints: human can tell its weird or not
                                    2. Payload contraints: may be present or not
                                3. Out-of-distribution (Data Distribution shift (Note: sudden shifts are often indicative of data bugs)) deployment: training data distribution to production recent production data distribution (get production data from Feature Store)
                            5. Explainability/Interpretability (Tools: (1) General: SSHAP, InterpretML, Alibi Explain, AI Explainability 360, TF What-If, Microsoft Responsible AI (RAI), interpret, Learning  Interpretability Tool (LIT), PiML, Captum, TF Lattice  (2) NLP-specific: TF language interpretability, ecco)
                                1. Feature importance/Attributions (Tools: Tensorflow WhatIf tool is great) (Good way to think about it: the level of importance of a feature X is measured by the entropy decrease between (P(Y=y| and maybe other features) and P(Y|X=x, and maybe other features). This is exactly how feature importance pops out of decision trees naturally, because they are naturally optimizing for this.) _Note: if working with LLMs, this because prompt importance, but prompt importance is a harder because of the very high dimensionality of the input, so adaptations are needed_
                                    1. Problems:
                                        1. Spurious dependency in the dataset (often called spuirous correlation, but this term is not technically correct): overfitting the dataset/biased dataset (e.g., seing that the model classfied a boat image correcly as boat, but not because there was a boat on it, but becuase there was water. This is a problem, because it will likely not generalize to a photo with a boat in the ground)
                                        2. Leakages: if the model is realying heavily on some feature, you might suspect there is a leakage. (e.g., doctors that mark with pen place where tumor is in CT scan, the model will rely heavily on these pen marks or "frequently shared urban legend of an early military AI project trying to detect Russian tanks in pictures with neural networks: The model accurately predicted tanks in held-out test data but utterly failed in production. It turned out that the data (from which both training and test data were drawn) was gathered so that pictures with tanks were taken on sunny days, whereas the other pictures were taken on cloudy days. The model then simply learned to distinguish brightness or weather")
                                        3. Bias: spotting bias indicators (e.g., if model is relying heavily on the gender feature to estimate loan)
                                    2. Methods
                                        1. SHAP (Improved version of Vanilla Shapley) values (there is a SHAP library for this)
                                        2. LIME
                                        3. Vanilla Shapley values (e.g., Integrated Gradients (Tensorflow Integrated Gradients is a great tool)) (_Note:_ computing Shapley values exactly in most cases is computationally intractable because of the number of feature, what is done is an approximation (the you can modify the Shapely accuray/computational cost tradeoff accordig to your needs))
                                        4. Causal Inference (evaluating effect of feature x on target): Avargae Treatment Effects
                                        5. Substituting feature values by random values and seeing the amount of drop in predictive power
                                    3. Actions: remove non-informative features (are defined by low information gain == E_x[Entropy(P^(Y=y|X=x))]/Entropy(P^(Y=y)) where P^(Y=y|X=x) and P^(Y=y) are estimates of P(Y=y|X=x) and P(Y=y) respectively) (e.g., can do SHAP (Improved version of Vanilla Shapley) values on entire dataset)
                                        1. Categorical features: Remove categorical features or Lower dimension of categorical features
                                        2. Numerical features: Remove numerical features
                                2. Sensitivity Analysis: Disecting Inference
                                    1. Easy: naturally interpretable models (e.g., decision trees)
                                    2. Hard: not naturally interpretable models (e.g., for NNs) (Tools: WhatIf from Tensorflow and Responsible AI (RAI) Toolbox are great tools)
                                        1. What if analysis: probing your model to see how output changes when you change input (Visualizations help a lot here)
                                        2. Distill model into a naturally interpretable model (e.g., NN was distalled into Decision Tree during training to facilitate interpretability)
                                3. FM-based Explainability Analysis: using a Foundation model (FM) to explain the preditions of your model.
                            6. Privacy guarantees (Threat model (from most dangerous to less dangerous dimensions))
                                1. Attacker has access to:
                                    1. Partial Data
                                    2. Model parameters
                                    3. API
                                        1. Unlimited
                                        2. Limited
                                    4. Output Probability
                                    5. Training Algorithm
                                2. Attacker's objective is to:
                                    1. Reconstruct features of a specific datapoint: <TODO>
                                    2. Recostruct any datapoints: Differential Privacy Evaluation
                        2. Results: send whole experiment (model/prompt, metadata, evaluation results) to Experiment Tracking System.
                3. Dataset Construction and Model Building
                    1. Data Preparation (Data-Centric AI movement focuses here) (Tools: (1) General ML tools like: optimus, scrub, dataprep, img2dataset, augmentoolkit; (2) CV: DataGradients, fastdup)
                        1. EDA 
                            1. Inject data from ML Data whareshouse, Feature Store, Public Hub or any other Data Source using a Data Discovery & Retrieval Platform (Data Catalog). What is a Data Discovery & Retrieval Platform (Data Catalog)? It is a tool that helps clients get the data they need from multiple data sources. (Tools: (1) Data Discovery tools: superset, elasticsearch, amundsen, datahub, redash, ckan, magda, genie, open data discovery (odd), OpenMetadata, atlas, Chat2DB; (2) Search Engines: (1) search-focused OLAP DB/Engine like: ElasticSearch, OpenSearch, Solr, Sphinx Search, Xapian, Nutch, bm25s, datasketch; AI-powered higher level search engines: Swirl, Devv))
                                1. Guaranteeing reproducibility:
                                    1. Define max number of rows to get row_max. Because new featured datapoints are constantly being inserted in the FEature Store via Streaming, so if we were to inject the entire dataset again we would get more rows
                                    2. If not sequential: define random seed for row selection
                                2. By dataset size
                                    1. For small datasets: inject all data until row_max
                                    2. For large datasets: work with a slice of the entire dataset of size row_max (because of memory concerns & to improve debuggability)
                            2. Pre-EDA: Defining expectations
                                1. Study the real world process we are trying to emulate with an algorithm
                                2. List hypothosis on the data. Some types:
                                    1. Data Quality
                                        1. Duplicates (Tools: dedupe, zingg)
                                        2. Missing data
                                        3. Leakage
                                    2. Feature distributions
                                    3. Potential Dangerous Social Biases/Unfairness issues
                                        1. Group Differences present in real life and dataset
                                        2. Group Differences present only in dataset (biased dataset)
                                    4. Dependencies
                                        1. Feature2Feature
                                        2. Feature2Target
                                    5. Structured data (data that hands us assumptions we can use)
                                        1. Sequential data
                                            1. Feedback Loops
                                                1. Environment-driven (e.g., recsys that user umber of downloads as a feature)
                                                2. User-driven (e.g., users that change their features in order to change their output (they do interventions))
                                            2. Temporal data
                                                1. Seq2Seq
                                                2. Time-Series
                                        2. Other types of structured input (e.g., graphs)
                                3. List Specific Questions
                                4. Causality Analysis: draw Causal Graph of the variables involved (dream is (generally unrealistic): independent features and all causing target) (Tools: Microsoft Responsible AI (RAI) is great).
                                    1. Features should be independent in tabular data: feature dependency (redundance of information leads to useless computational efforts, increases complexity, adds one more dimension that can worsen curse of dimenionality and inject noise)
                                    2. Target should have arrows directly to features and only in one direction
                                        1. Confounder association: which features might be associated throigh confounders with target.
                                            1. Weak association (doesnt survive across datasets): Spurious dependency in dataset (often called spuirous correlation, but this term is not technically correct): some kind of noise noise happens to be a good predictor for target (very dependent) in the dataset (biased dataset) by chance (also arent robust to other datasets of the same task (overfitted the particular dataset))
                                            2. Strong association (survives across datasets)
                                                1. Bad for explanability/interpretability
                                                2. Can deteriorate parameter sensitivity
                                                3. Adds somewhat useless dimension (bad for curse of dimenionality and cimmpute reasons) if the real cause is already a feature.
                                                4. Can act on it on the real world
                                        2. Causal twoway road between target and unobserved variable (generally happens when you gather data in an environment where there is already an agent controlling your target): you feature can lead to causal arrows only in one direction to target. aka your target cannot be cause (directly or indirectly) by some of your features and cause other part of your features, because if this happens it means that you have leakage happening. E.g., predicting umidity of soil in a farm from historical data: model thinks sunny day will make soil umid because farmer always irrigated it. Here the features cause both farmer and target, and farmer causes target. Here, collecting data on how much water the farmer poured is essential to be able to undo the causal effect of farmer on target (and be carefull to not leak this info to training set); or ''a model might learn high blood pressure is good for you because the treatment given when you have blood pressure lowers risk compared to healthier patients with lower blood pressure'' but when deployed the model wont have done any treatment. (_Note:_ this problem in non-trivial in the causality lens, because typicla assumptions on causal graphi is that it is a directed _acyclical_ graph of variables, and this examples has a cycle.)
                                        3. Non-trivial causal paths
                                            1. Association boost: two causal paths lead to same variable in a way that boosts association
                                            2. Canceled Association: two causal paths lead to same variable in a way that associations cancel.
                            3. Core EDA (Exploratory Data Analysis (i.e. becoming the subject matter expert) (_Note_: write notes to yourself along the way, because you will keep coming back to things)) (Tools: Rath, ydata-profiling, sweetviz, spotlight, TaskWeaver)
                                1. Data Validations (Tools: JSON Schema, Great Expectations, pandas profiling, facets, pydantic, schema, pandera, deequ, TF Data Validation)
                                    1. Type of Task
                                    2. Know what each of the datasets features mean
                                    3. See if there is class imbalance problem
                                    4. Evaluate de quality of the dataset (% of missing values, not explained features, absurd values, repeated rows, outliers/anomalies)
                                    5. Look at examples
                                    6. Draw general statistics & visuals (Tools: d-tale)
                                2. Do Dataset split and forget test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant): guarantee training, cv & test set are statistiscally alike (usually just get statisitcs on datasets and compare, but can do fancier stuff also) (Note: shoould have 2 test sets: the first one is used for typical model evaluation and another one is used for ml system evaluation in the background cluster.)
                                    1. Data cleaning necessity: remove duplicates (if not: can end up with same/aprox same datapoint in train and test set which will make it very easy)
                                        1. Exactly duplicates
                                        2. Near duplicate datapoints
                                            1. Close examples (easy ones. e.g., when features are almost equal)
                                            2. Further examples (hard ones. when features seem different but have same semantics. E.g slighly different views of a patients CT scan)
                                    2. Beware of:
                                        1. If datapoints are not independent
                                            1. If sequential data: splits have to follow sequence (e.g., time series data)
                                            2. If grouped data: groups need to be treated as a unit of data and go into either split altogether (e.g., "or example, we may have multiple cancer scans from the same patient and a random split will distribute those scans across training and test data — the model may learn to detect the patient and memorize their cancer status rather than to detect cancer in the image. Even if the model does not detect patients, it may have only learned to distinguish cancer from non-cancer images well for the patients in the dataset, but not for any other patients")
                                        2. If classification: aim to have statistically comparable sets
                                            1. If small dataset: typically people do random sampling but stratified sampling is better because you can ensure with more confidence statistically comparable sets. E.g., If classification: aim to have same label distribution across splits.
                                            2. If larger dataset: random split will very likely make a balanced split, no worries
                                3. Reviewing Data Collection
                                    1. Check carefully outliers/anomalies and see if they indicate your missing to collect important features (because noise comes from partial observability)
                                    2. See data collection can be improved to make x more informative of y (e.g., task of detecting broken phones by using images taken in the factory. If the lighting is low, images can come dark and make it harder to perform the task of classifying broken phones, this is evident when asking humans to perrom the task and seeing their error (larger entropy in p(y|x))
                                    3. See if the training data was collected in the same environment that we want to deploy it on
                                        1. Special case problem: if the data was collected in a special case of the general case you are aiming to handle. E.g, data was collected only on holidays, or from one region of the world, etc)
                                        2. Causal twoway road between target and unobserved variable (generally happens when you gather data in an environment where there is already an agent controlling your target): E.g., predicting umidity of soil in a farm from historical data: model thinks sunny day will make soil umid because farmer always irrigated it, so farmer is being caused by the umidity of the soil and is causing it back.
                                    4. Spot Degenerate Feedback loops: when feature vector X we are gathering depends on a model's previous outputs. E.g.,  of recommender system where reccomnedation shown o top have a much more prob of being clicked than predictions on the bottom or even more (of course) for not shown predictions, the system can start thinking user likes a lot some item and recommend it again (because for example: it has a feature of total times this item was clicked in general, which is one of its best features), but in fact they are just clicking the one the cathes their attention most, and this would transfer to any other item; or crime prediction: where police sees output of ML model and goes to place/time where crime is more likely. But then crime is less likely to occur in this place due to a non-modelled factor: the police being there;
                                        1. Environment-driven (e.g., recsys that user umber of downloads as a feature)
                                        2. User-driven (e.g., users that change their features in order to change their output (they do interventions))
                                    5. See if there are indications of wrong data collection along the way
                                4. Imaginate what you can do, in terms of modelling complexity, with the amount of quality data that you have data
                                5. Check Pre-EDA hypothesis
                                6. Answer Pre-EDA questions
                                7. Plot feature distributions (can show multimodal behaviour and outliers/anomalies aswell), genenrate summray statistics (with confidence bounds if features have explicit noise) and test for normality (some stats tests & models work under normality assumption)
                                8. Do causal inference (evaluating causal effects of features) to identifiy relevant features (Tools: Microsoft Responsible AI (RAI), DeepCausality, Causal ML, dowhy)
                                    1. Goals
                                        1. Causal Effects (when you dont know the ouput y for a datapoint. Actually expected value over all possible Counterfactuals)
                                            1. Avarage Treatment Effects (how changing a feature, changes output on avarage)
                                            2. Individual Treatament Effects (how changing a feature, changes output on a specific datapoint) (This not very common to do in Experimentation, because usually you do this analysis on your dataset, which you have labels on, so you can do CounterFactuals. However, if you dont have labels in some examples, you can do this)
                                        2. CounterFactuals (when you know the ouput y for a datapoint and want to know what woud happen with a different X datapoint): Always concerned with Individual Datapoints
                                    2. Methodology
                                        1. Definition: define causal estimands: P((Y=y)|do(X=x)) that dictates the effect of X on Y
                                        2. Identification: going from causal estiamnd to statististical estimand (without do operator) though causal graph and do-calculus
                                        3. Estimation: estimate statistical estimands through simple Statisitcs/ML Methods
                                9. Search for patterns yourself (''Good data analysis will have a story to tell. To make sure its the right story, you need to tell the story to yourself, then look for evidence that its wrong'')
                                    1. That explain dependent (target) variable
                                        1. Types of tasks
                                            1. Regression
                                                1. To see what features are important: dependence between x_i and y
                                                2. To see how they are important: y == f(x) + noise --> aprox f(x) in your head
                                            2. Classification
                                                1. To see what features are important: Separability (entropy) of classes with respect to x_i
                                                2. To see how they are important: boundaries f(x) == 0 with high separability (entropy) of classes
                                        2. Approaches
                                            1. Hypothesis driven: statistical tests
                                            2. Clustering
                                            3. Plotting
                                            4. Causal analysis: when you are a about to use a feature x_i because it y depends a lot on it, check if there arent hidden factors (Confounders) that cause x and y and generate this supurious correlation in the dataset. This is a case were the distribution of training environemnt is different from production, however, the distribution of data can be the same!This is because we ignored the causal variable in our dataset, which is the one which is changing its occurance. In production this hidden factor might not appear and therefore destroying the dependency/correlation. This can typically reveal a non-ideal data collection environment, absence of important features in the training set, dummy features or label leakage in the dataset assembly (Tools: Microsoft Responsible AI (RAI) is great).
                                10. Validate patterns that you found (look for evidence that rejects your hypothesis. ''One way of doing this is to ask yourself, ''What experiments would I run that would validate/invalidate the story I am telling?'' Even if you dont/cant do these experiments, it may give you ideas on how to validate with the data that you do have'') (Tools: TF Data Validation, JSON Schema, Great Expectations, pandera, soda core)
                                    1. Feature Importance Tests (dropping low signal/noise features). _Note 1_ Feature with noise can hurt the model in these ways: 1. Low data regime: Training algorithm fits noise thinking its signal, wont generalize; 2. If using probabilisitc model, noise of feature will lend itself to noise in our estimator and therefore increasing our error rate; 3. Computational costs; Increase of complexity/Decrease of explanability. _Note 2_ However, if signal decreases complexity (y == f(feature2) aproximates mean(y)) noise in the feature has less and less influence (gets ignored) to the point where a feature is pure noise and then the f(feature1) just get replicates across feature 2 span wihtout incurring any noise in estimated P^(X,Y).
                                        1. Estimate information gain == E_x[Entropy(P^(Y=y|X=x))]/Entropy(P^(Y=y)) where P^(Y=y|X=x) and P^(Y=y) are estimates of P(Y=y|X=x) and P(Y=y) respectively
                                        2. Do SHAP on whole dataset
                                        3. Comapare preditice power with and wihout feature. Probabilistic model compared with its determinisic counterpart (expected value of the probabilisitc one) will always perform worse on the test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant), because it will have its own variance summed with the variance of the empirical p(Y=y|X=x). Always compare non-probabilistic to non-probabilistic or probabilistic to probabilistic. And if comparing probabilisic models, do a handfull of evaluation runs.
                                    2. Inter-Feature Dependence Tests. That removes redundancy of features (features should be independent, but this is almost never the case) (it is important mainly to increase simplicity (less tech debt & better explanability), decrease computational costs, and improve reproducibility & learning stability (if dependent variables are used, the final learned model can be various things (parameters having high variance) depending on the initialization of parameters (if numerical optimization was used) and noise in the dataset (problematic for small/medium datasets where the noise doesnt "cancel itself" and can get mistaken for signal))). Approaches to remove highly dependent features:
                                        1. Use Visualizations: often the first step, before doing it with Statistics or ML (not limited to Pairwise analysis)
                                        2. Use Statistics
                                            1. (Between numeric features) or (between numerical and ordinal features) or (between ordinal features):
                                                1. Non-probabilistic (limited to Pairwise analysis)
                                                    1. Linear relationship (Correlation): Person's r (cannot handle non-linear data, but you can apply to small subsets of data (locally more linear) to work around this)
                                                    2. Monoticity (can handle non-linear relationships if they are monotonic, but you can apply to small subsets of data (locally more montonic) to work around this)
                                                        1. Spearman's rho
                                                        2. Kendall's tau
                                                2. Probabilistic: Mutual information between 2 or more variables (way better than correlation in principle, but in practice requires muchh more computation because for each evaluation of mutual info between a set of variables you need to model the joint ditribution between them) (not limited to Pairwise analysis)
                                            2. Between categorical features: chi-square test for independence
                                            3. Between ordinal and categorical features: e.g.,  Polychoric correlation
                                            4. Between numeric and categorical features: t-test/ANOVA
                                        3. Use ML: fit simple non linear model (input: one variable; output: the other variable) and see corrected predictive power (not limited to Pairwise analysis)
                                11. Seasonalities: see if there is a seasonality in the model behaviour (e.g., model that predicts amount of icecream that will be selled has 3 modes: winter, summer and other months, each mode has a singificantly different behaviour than the other)), and if there is a feature addressing it. If not, it is important to include one (e.g., in a certain group of months or before holiday)
                                12. Build a heuristic model & discuss with actual subject matter experts (Tools: human-learn, sklearn-expertsys)
                                13. Plan Manual Data Engineering
                                14. Bundle EDA Results into a standardized report/dashboard/notebook and send to ML Monitoring System (Then anyone can understand the dataset and remove doubts)
                        2. Manual Preprocessing (Data Engineering) (Go back to EDA when necessary) (The arficat of this step is a function capable of receiving batch datapoints and transforming them to suitable featured datapoints). _Note 1:_ impirtant to define preprocessing/data engineering as a pipeline of tasks & cash the outputs of these tasks somewhere, because you will be tweaking little stuff & re-running it a lot (you dont need to rereun upstream tasks that were not changed). _Note 2_ tasks will be interacting with a lot of external storage data (e.g., S3) so its important to cash this data in the VMs themselves (entire data wont fit into machines, so you need to cash some parts of it). You should put the data in your persistent sotrage (e.g., distributed file system (e.g., HDFS, Gluster), hosted distributed storage service (e.g., Hive), managed object storage (e.g., S3, Swift)) (good workflow orchestrator integrate with them) as your first task. (Tools: TF Transform, sklearn pipelines)
                            1. Raw Data Wrangling
                                1. Inject data from ML Data whareshouse  if data is not already in-memory
                                2. Change data types
                                3. Joining tables, Groupbys
                                4. Cleaning (Tools: Cleanlab, cleanvision, HF Evaluate)
                                    1. Missing values
                                        1. Types (there is a big spectrum between Dependent on features and Completely Random diictated by the correlation)
                                            1. Dependent on features --> can be modelled
                                            2. Missing because of some hidden feature of the datapoint (non realistic, out of range, etc) --> we need to use previous value in some way
                                            3. Completely Random --> cant be modelled
                                        2. Approaches
                                            1. Drop datapoints with too much missing values
                                                1. Advantages: Simplifies computation, avoids making non-realistic datapoints
                                                2. Disavantges
                                                    1. Dependent on features: biasing data
                                                    2. Completely Random and/or Missing because of some hidden feature of the datapoint: Overfitting or dont capture complex patterns
                                            2. Drop features with too much missing values
                                                1. Advantages
                                                    1. Dependent on features: removes confounding
                                                    2. Completely Random and/or Missing because of some hidden feature of the datapoint: simplifies computation, improves on curse of dimensionality
                                                2. Disavantages
                                                    1. Completely Random and/or Missing because of some hidden feature of the datapoint: Lose important predictive information
                                            3. Do feature inputation for missing values
                                                1. Modelling missing values
                                                    1. Advantages: make very realistic datapoints, leveraging relationships between features
                                                    2. Disavantages: computationally intensive
                                                2. Sampling from marginal distribution of the feature
                                                    1. Advantages: reasonably realistic datapoints
                                                    2. Disavantages: less acurate than modelling missing values and more computationally intesive than using espected value
                                                3. Using expected value of the marginal distribution of the feature
                                                    1. Advantages: reasonably realistic datapoints, very computationaly friendly
                                                    2. Disavantages: injects strong bias, because you are always putting same value
                                            4. Treat missing value as data (e.g., adding a category if is categorical feature or using a specific fixed value if it a numerical feature)
                                                1. Advantages:
                                                    1. Missing because of some hidden feature of the datapoint: can extract usefull information from them
                                                2. Disavantages
                                                    1. Dependent on features: Injecting confounding
                                                    2. Completely Random: Injecting noise
                                                    3. Missing because of some hidden feature of the datapoint: needs carefull design and strong domain knowledge
                                    2. Anonymization of PII data when posssible
                                    3. Leakages: check for existing leakages and, if present, remove the corresponding features  (e.g x-ray images with radiolgist pen marks exaclty where the problem is, or ID of the datapoint giving away the target because of the non-random way the dataset was generated)
                                        1. Twoway road to target is forbidden: you feature can lead to causal arrows only in one direction to target. aka your target cannot be caused (directly or indirectly) by some of your features and cause other part of your features, because if this happens it means that you have leakage happening
                                    4. Filtering (can use heuristcs (e.g keywords) or an ML Model 4 this)
                                        1. By Inapropriate content (discard single datapoint)
                                        2. By Outliers/anomalies (discard single datapoint)
                                        3. By Troll user in some time window (discard all datapoints with User A in the specific time window the datapoint B from user A was found on)
                                    5. Mislabelled datapoints
                                        1. Using no prior information
                                        2. By Bad Labeller in some time window (discard all labels that he did)
                                6. Transforming (e.g log scaling for exponential data)
                                7. Changing format (e.g for images, change PNG (4 color channels) to JPEG (3 color channels))
                            2. Wrangled Data validation [same routines as inference with the exception that here datapoints can have labels or not (natural labels from the environment or artifical ones provided by the user), in inference never have]
                                1. Schemas
                                2. Descrptive statistics (should not be that diffferent between batches)
                                3. Ranges
                            3. Feature Engineering (making modelling easier for your model by doing som of the heavy pattern recognition work yourself) for feature generation [same routines as inference with the exception that here datapoints can have labels or not (natural labels from the environment or artifical ones provided by the user), in inference we never have]
                                1. Manual Analysis (After offline manual analysis)
                                    1. Drop
                                        1. Features
                                            1. Non-informative features (are defined by low information gain == E_x[Entropy(P^(Y=y|X=x))]/Entropy(P^(Y=y)) where P^(Y=y|X=x) and P^(Y=y) are estimates of P(Y=y|X=x) and P(Y=y) respectively)
                                            2. Highly dependent on other features
                                            3. Non-Crucial Low entropy features (feature than mostly have one value, with the exception of a few, and only looking at these few, target has high entropy. (_Note:_ for numerical features, the entropy of the feature must be calculated before normalization (because normalization extinguishes the notion of how well distributed the datapoints are relative to the native feature space))
                                        2. Datapoints
                                            1. Removing outliers/anomalies
                                            2. Class with much more examples than another class
                                    2. Modify existing features
                                        1. General Categorical encoding
                                            1. One-hot encoding
                                            2. Hashing
                                            3. Embedding
                                            4. Label encoding
                                        2. Text-specific encoding
                                            1. N-grams
                                            2. Tokenizer + Learned Token Embedding
                                        2. Discretizing: binning features (generally numerical) into categories
                                    3. Create new features as a function of existing features (will have cdependent features problem if you dont drop the features you used to build the new one)
                                2. ML methods
                                    1. Dimensionalty Reduction (PCA (will create tottally new orthogonal features (each one as a function of existing festures) & SVD are the main methods))
                                    2. Automatic Feature Generation
                            4. Featured Data validation [same routines as inference with the exception that here datapoints can have labels or not (natural labels from the environment or artifical ones provided by the user), in inference never have]
                                1. Schemas
                                2. Descrptive statistics (should not be that diffferent between batches)
                                3. Ranges
                            5. Store (Cash) final data
                                1. Store in files
                                    1. Tabular data: .csv
                                    2. non-tabular data: same format that we got it (.pdf, .html, ...)
                                2. Store in DB
                                    1. Tabular data: Structured DB (e.g., Postgres)
                                    2. Non-tabular data: Unstructured DB (e.g., MongoDB)
                            6. Productionizing (_Note:_ productionization of new feature set can only be triggered together with a new model productioninzation)
                                1. Changing Feature Generation: Data Pipelines CI/CD
                                2. Changing directly featured dataset: Data Sources CI/CD. This is done for things that are very engineer dependent and can't be easily put into feature generation code (e.g., outlier removal).
                                3. Changing Model & Monitoring & other dependent systems to support new feature set
                                    1. Changing Model: Go to section "3. Modelling" and follow from there
                                    2. Changing Monitoring: change monitoring sytem repo and/or prodution config
                    2. Modelling
                        1. Get data (Tools: dlt) from either:
                            1. If using features you just made with "Manual Preprocessing": 
                                1. Files stored somewhere
                                2. Some DB Server
                            2. Else: Feature Store
                        2. Model P(X). Reasons:
                            1. Quantify Bias: the expected divergence from ideal distribution
                                1. True-bias. If your model generate samples (generative model P(X=x,Y=y) is necessary): Divergence from true P(X) (_Note:_ very hard to do because we dont know true P(X))
                                2. Uniform-bias. If your model labels samples (always dicriminative model P(Y=y|X=x)): divergence from Uniform distribution (Before normalizaiton!)(_Note:_ easy to do because we only need to look at features individually but need to define ranges for them (even if they theoritically dont have bounds) to do the analysis)
                            2. Can use to transform a discriminative model P(Y=y|X=x) into a generative model P(X=x,Y=y) if necessary)
                        3. If using a base/pre-trained model: get these pre-trained models and validate them
                            1. Types of model Dependencies
                                1. Transfer Learning
                                    1. Fine tunining(internal or external models already trained for a similar task and that can transfer knowledge to our current task)
                                    2. Hot-start (using the parameters of a similar model we' as already built to speed up learning)
                                2. Emsembling: stacking. Getting pre-trained models that we going to use to generate inputs to our model.
                            2. Validations: validate pre-trained models
                                1. Reproduce results: validate all the results the model claims it has
                                2. Model evaluaiton: do your own more detailed evaluation. Explained in "Manual Offline Evaluation".
                        4. Training & Debugging
                            1. Setup/Configure Ephemeral Workload Cluster if necessary (Tools: Ansible or Terraform/Terragrunt/terramate, Pulumi, Crossplane)
                            2. Manual Training procedure specification (Refer to Manual Training procedure specification interface in MLOps Setup)
                                1. If using LLMs: prompt engineering. (_Note:_ prompts will also be included in experiment tracking: API endpoints, prompt file (that escribes the computation needed to build the prompt from input), formats used, test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) performance, LLM previously fine-tuned or not, etc).
                                    1. Manual Prompt building using heuristics
                                    2. Automatic Prompt building
                                        1. Prompt Templates (Tools: promptify) 
                                        2. LLM-generated data & prompts (Tools: automatic_prompt_engineer)
                                        3. Learned prompts (treating prompts as parameters in the LLM) (Tools: OpenPrompt)
                                        4. Genetic Algorithms on Embedding Space
                            3. ML Training CI/CD (deploy := send model to Model/Prompt Registry & get its ID, if no more bugs & it is a new best model) (will trigger evaluation when training is done)
                                1. Model Debugging (if there are erros in training or evaluation, you will keep coming back here). While bugs:
                                    1. Programmer identifies errors using ML Training Monitoring tool
                                    2. Programmer corrects ML training procedure specification (which triggers ML Training CI/CD)
                    3. Lightweight Manual Offline Evaluation
                        1. Aggregate predictive power metrics
                        2. Error analysis
                        3. Predictive Power on slices
                        4. Inference time
                    4. Post-processing (after learing the model)
                        1. Post-Training Feature Selection (Ablation Study with Feature Importance guidance) (Good way to think about it: the level of importance of a feature X is measured by the entropy decrease between (P(Y=y) and P(Y|X=x). This is exactly how feature importance pops out of decision trees naturally, because they are naturally optimizing for this.))
                            1. Approaches
                                1. Black-box (e.g., SHAP (Improved version of Vanilla Shapley) or Vanilla Shapley Values through Integrated Gradients) (Model Independent)
                                2. White-box (e.g., decision trees already give you expalability/interpretability out-of-thebox) (Model dependent)
                            2. Steps
                                1. Get model from Model/Prompt Registry
                                2. See which features are less important in the dataset
                                3. Remove the x less important or with importance below a treshold
                                    1. Categorical features: Remove categorical features or Lower dimension of categorical features
                                    2. Numerical features: Remove numerical features
                            3. Results: Change Model and Update model in Model/Prompt Registry with feature importance information
                        2. Feature Staleness Analysis (Most models use features that depende on last g data where g is a time gap. However, if g is small it is hard to make these features availabe in the feature store before the inference request arrives, so then we have to compute it in real-time. Therefore we want to push this g to be as big as possible wirhout degradating too much our ML metrics on the test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant))
                            1. Plotting (ML metric vs g)
                            2. ML metric threshold
                            3. Optimizing a custom evaluation function dependent on ML metric & g
                        3. Ensembling Models (if you want to, remember than this brings more computational costs, latency and technical debt)
                            1. To improve corrected predictive power
                                1. Acess Model/Prompt Registry FrontEnd or Docs and look 4 models with good performance and with different underlying assumptions (diverse set of models)
                                2. Request these models from the Model/Prompt Registry
                                3. Ensembling approaches. (_Note:_ if there is a meta-model it should be a simple model)
                                    1. No learning
                                        1. Typical Bagging: simply doinf a majority vote scheme on top of muultiple models.
                                    2. Petrained models are used and learning occurs
                                        1. Mixture of experts (MoE): each submodel was trained only on a specific slice of the input and the meta-model receives as input their predictions and also the features. The meta-model ony "routes" the prediction to the right model according to the region that input is in.
                                        2. Stacking: each submodel is trained on the entire dataset and is a working model on its own. The meta-model learns which models are better on which regions and also receives as input the features. (Typically used for NNs). _Note:_ important to have variaty between submodels, meaning variatiy of estimation approaches (e.g., by having them to be different family models such a SVM, DT & NN combo.)
                                        3. Boosting: train a new model to predict the error of your previosu model them sum the predictions of both (this is only usefull if the MSE of the new model is lower than the MSE of the previous one).
                                    2. No pretrained models are used and all parameters are learned during ensemble training. Feels like just learning a normal non-ensemble model:
                                        1. Model Dependency-aware Bagging: baaging, but all models are learned together, and making the models less dependent as posssible (optimize for variety of outputs) is part of the optimization.
                                4. Ensembling checks
                                    1. Check that the models are not very dependent (the less dependent the better, but there is a limit to this while mainting good prredictive power to each of them)
                                5. ML Training Procedure generation (which triggers ML Training CI/CD)
                                6. Do step ''8. Post-Training Feature Selection''
                            2. To improve retraining (optional): divding the model into 3 separate models. 1 for static feature, 1 for slow changing features & another for fast changing features, with a meta-model on top of those 3. The advantage of this is to retrain a smaller model. High frequency retraining would only be one to the model of fast changing features and the meta-model, low requency retraining to the model that uses slow changing features. The static feature model would never be retrained. _Note_ however, in my opnion this added complexity & probable corrected predictive power loss is not worth the little retraining advantages.s
                        5. Manual Offline Evaluation again, sending experiment to Experiment Tracking System again
                        6. Building pre/post processors
                            1. IO Filters/Guardrails (e.g., innaproporate content identification, summarization eval, LLM-as-judge, etc): heuristics to filter either undesired inputs from reaching model our undesired outputs of the model from reaching users. Because many times the model is not aware of a larger context, model fails & trolling/attacking attempts.
                                1. Write your filters as python files
                                2. Test them against failure modes of your model
                                3. Export them as filter files (language agnostic, efficent format, that just describes the necessary computation (similar to model file))
                                4. Store them in Model/Prompt Registry in the tab of a specific task (task covers all model versions) or specific model (specific model version).
                            2. Semantic Cache
                        7. Manual Offline Evaluation again, sending experiment to Experiment Tracking System again
                        8. Fallbacks
                            1. ML Fallbacks: simpler ml model
                            2. Non-ML Fallbacks
                                1. Write heuristic code to serve as a non-ml-fallback
                                2. Export as a computational graph/IR file (language agnostic)
                                3. Store in Model/Prompt Registry for a specific task or speicic model
                        9. Productionizing
                            1. Scaling the model to entire dataset
                                1. Put (if already not inside) above steps inside a pipeline/function that accepts configurations for scale (e.g., hyperparams, cloud machines, etc)
                                2. Define appropriate training configuration for larger scale
                                3. Apply pipeline/function to larger entire dataset with large scale training configuration
                            2. Generate "production-going experiment" to the experiment tracking system, these are the only type of experiments that are used for Inference Service ci/cd.

            4. MLOps platform Development (output is push or PR that triggers MLOps platform (level 1) CI/CD)
                1. Dev-ML-Platform-Experimentation Environment (Tools: devspace, okteto, telepresence, gitpod, Tilt, coder, che)
                    1. Local: ML Engineer's Logical Machine
                        1. Setup (Tools: envd)
                            1. Build a Dev Image to work on
                                1. IDEs (e.g., VSCode)
                                2. SDKs (e.g., Android SDK)
                                3. Tools (e.g., Microservices Framework like Spring)
                                    1. CLI tools
                                    2. GUI tools
                                    3. Libraries
                            2. Send experimentation/dev image to private image registry
                            3. Run experimentation/dev environment as a Docker/Podman/Contaierd/runc container
                        2. Go in, work & go out of your local experimentation/dev environment (Tools: vscode extension)
                            1. Option 1: k8s cli. Working (executing commands & interaacting with files), via k8s cli, inside a <pod,container> dev environment in k8s (Disavantadge: might not be secure to use open up a git networking connection in your cluster; Advantage: easier & more native)
                            2. Option 2: sync, ssh & mount. Working (executing commands & interaacting with files), via file sync (Tools: bitsail)& SSH, inside a logical machine which has somefolder mounted to a <pod,container>, then execute commands inside <pod,container> using k8s cli
                    2. Remote: Cloud Logical Machine: more specifically, Dev Logical Machine
                        1. Setup (Tools: envd)
                            1. Build a Dev Image to work on
                                1. IDEs (e.g., VSCode)
                                2. SDKs (e.g., Android SDK)
                                3. Tools
                                    1. CLI tools
                                    2. GUI tools
                                    3. Libraries
                            2. Send experimentation/dev image to private image registry
                            3. Deploy experimentation/dev environment as a kubernetes (using Shipwright, Istio/Kiali/linkerd2/Cilium/Consul, Helm/Kustomize/timoni/JuJu/cdk8s/kpt, yamllint/Carvel/kubesec/kubeaudit/kubeconform/kube-linter/polaris/conftest/Kubescape/Kyverno/OPA/Datree/Kubevious, SchemaHero/Datashim/Velero, Karpenter, Sealed Secrets/Kubernetes Secrets Store CSI Driver/External Secrets, Metacontroller)/kubeflow cli <pod, container>
                        2. Go in, work & go out of your remote experimentation/dev environment (Tools: vscode extension)
                            1. Option 1: k8s cli. Working (executing commands & interaacting with files), via k8s cli, inside a <pod,container> dev environment in k8s (Disavantadge: might not be secure to use open up a git networking connection in your cluster; Advantage: easier & more native)
                            2. Option 2: sync, ssh & mount. Working (executing commands & interaacting with files), via file sync (tools: bitsail) & SSH, inside a logical machine which has somefolder mounted to a <pod,container>, then execute commands inside <pod,container> using k8s cli
                2. Experimentation tool: needs repo-based experimentation tool (Tools: open source: Aim, MLflow tracking, dvc/fds + CML, Pachyderm, truelens, fasttrackml, MLTRAQ; (1.2) paid: W&B, DagsHub, Comet ML)
        3. Monitoring
            1. Do ML Monitoring
            2. Do General Monitoring
    3. Iterate
        1. Things go wrong. Can change:
            1. Project scoping and/or
            2. Data collection and/or
            3. Model and/or
            4. Data pipelines and/or
        2. ML Cost not worth it. You can: (This involves infrastrcture, training, tuning and inference costs)
            1. Stop doing ML and/or
            2. Change Project scoping and/or
            3. Simplify system architecture and/or
            4. Change cloud provider
</details>

<details>
    <summary><b>Production ML needs</b></summary>

### :alien: __Production ML needs__

Given the assumptions, what entities (Companies, Government, etc) want for their ML systems is described below. This list extends what they already want for their software systems, which is adressed by Software Engineering and DevOps/DevSecOps. Therefore, non ML-related needs (solution of problems) are not adressed here, but are very important. Most of the day-to-day failure modes of these systems are actually due to non ML-related problems, that Software Engineering and DevOps/DevSecOps adresses. But, ML-related problems can have large impact and be more difficult to solve.

#### :dart: __High corrected predictive power__ (ML research community's traditional focus)

17. __Corrected predictive power__

* 17.1 __Vanilla corrected predictive power (evaluation metrics)__
    * 17.1.1 __Supervised Learning__
        * 17.1.1.1 __Label metrics__
            * 17.1.1.1.1 __Empirirical__ (using test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)s. _Note:_ tests sets shouldnt be made by drawing random datapoints, they shloud be the last datapoints (indexed by time) available, because then you factor in distribution shifts)
                * 17.1.1.1.1.1 __Classification__
                    * 17.1.1.1.1.1.1 __Accuracy__
                    * 17.1.1.1.1.1.2 __Recall__
                    * 17.1.1.1.1.1.3 __Precision__
                    * 17.1.1.1.1.1.4 __F1-Score__
                    * 17.1.1.1.1.1.5 __AUC__
                * 17.1.1.1.1.2 __Regression__
                    * 17.1.1.1.1.2.1 __L1 Loss__
                    * 17.1.1.1.1.2.2 __L2 Loss__
                * 17.1.1.1.1.3 __Ranking__
                    * 17.1.1.1.1.3.1 __Mean Avarage Precision__
            * 17.1.1.1.2 __Theoretical__ (probabilistic, that uses performance on training set and model complexity)
                * 17.1.1.1.2.1 __AIC__ (Akaike Information Criterion) from frequentist probability
                * 17.1.1.1.2.2 __BIC__ (Bayesian Information Criterion) from bayesian probability
        * 17.1.1.2 __Generative metrics__
            * 17.1.1.2.1 __Image metrics__
                * 17.1.1.2.1.1 __Groud-truth-independent__ (there isnt only one right output image)
                    * 17.1.1.2.1.1.1 __Inception Score (IS)__
                    * 17.1.1.2.1.1.2 __Frenchet Inception Distance (FID)__
                * 17.1.1.2.1.2 __Groud-truth-dependent metrics__ (only one right output image)
                    * 17.1.1.2.1.2.1 __Compare representions__
                        1. __Image embedding__
                        2. __NN layer__
                            1. __Learned Perceptual Image Patch Similarity (LPIPS)__
                    * 17.1.1.2.1.2.2 __Structured Similarity Index Metric (SSIM, MSSIM)__
                    * 17.1.1.2.1.2.3 __Peak Signal-to-Noise Ratio (PSNR)__
            * 17.1.1.2.2 __Text metrics__
                * 17.1.1.2.2.1 __Groud-truth-independent__ (there isnt only one right output text)
                    * 17.1.1.2.2.1.1 __Diversity__
                        1. __Self-BLEU__
                    * 17.1.1.2.2.1.2 __Toxicity__
                    * 17.1.1.2.2.1.3 __Halucination__
                * 17.1.1.2.2.2 __Groud-truth-dependent__ (only one right text)
                    * 17.1.1.2.2.2.1 __Compare representions__
                        * 17.1.1.2.2.2.1.1 __Embeddings__
                            1. __Word Embeddings__
                                1. __Word Mover's Distance (WMD)__
                            2. __Text Embeddings__
                        * 17.1.1.2.2.2.1.2 __NN layer__
                            1. __BERTScore__
                    * 17.1.1.2.2.2.2 __ROUGE__
                    * 17.1.1.2.2.2.3 __BLEU__
                    * 17.1.1.2.2.2.4 __METEOR__
    * 17.1.2 __Unsupervised Learning__
        * 17.1.2.1 __Likelihood__
        * 17.1.2.2 __Supervised Learning evaluation metrics__ (when doing Self-Supervised Learning)
* 17.2 __Set Custom Objectives__ (More than a traditional objective function on a single task)
    * 17.2.1 __Single Task__ (loss function as a function of input and/or type of error)
        * 17.2.1.1 __Proritazation of certain errors over others__ (e.g Covid screening: no false negative (patients with covid should not be classified as no covid, e.g Fingerprint unlocking: no false positive (unauthorized people should not be given access))
        * 17.2.1.2 __Prioritazation of certain inputs over others__ (e.g focus on paying users in churn prediction, focus on patients with underlying conditions in cancer prediction, focus on "easy" inputs the user expects the model to not make any mistakes.)
    * 17.2.2 __Multi-Task__ (multiple models, one big multi-task model, transfer learning, meta-learning, generative models, self-supervised learning as pre-training for multiple tasks. Generative Models are in a sense the generalization of discriminitive supervised learning, because they model the joint distribution p(X == x, Y == y), instead of modelling the conditional p(Y == y|X == x), therefore can be used for predicting any missing data instead of only missing labels
* 17.3 __Use MultiModal data__ (e.g.,  images, audio & text together)
* 17.4 __Leverage unlabelled data: semi-supervised learning__ (together with labelled data, give take advantage of unlabelled data): Unsupervised (includes Self-supervised learning) approaches can be leveraged to improve supervised learning by doing representation learning (includes distance metric learning). With good representations it is easy (approaches linear relantionship) for a simple model supervised to work on top of it. We humans are only able to explain this last simmple model layer generally, the complicated representation learning stuff happens under the hood and we cant explain it.
* 17.5 __Incorporate Domain Knowldge (as inductive biases/assumptions expressing contraints of the real world process) not just in features__ (to converge learning to the right regions in the param space without absurd amount of data)
    * 17.5.1 __Types__
        * 17.5.1 __Processing Priors/Assumptions__ (restricting the set of admissable functions, to enable convergence to the right places in the parameter space (in a reasonable training time and/or dataset size. E.g., Convolution or attention))
        * 17.5.2 __Extra Information (Non-euclidean Input)__ (putting domain knowledge in input (Every input can be seen as a graph (Tensor (images/video) is a special case of a graph with homegeneous connection, usually fixed size and with 2 patial atributed for each node (<x,y> coordinates))))
        * 17.5.3 __Causality Priors/Assumptions__ (e.g., expliciting which variables cant be caused by others. Target variable is not always cause by the features, sometimes we are dealing with iverse problems (e.g tryinng to reconstruct 3d model by image)
    * 17.5.2 __Sources__
        * 17.5.2.1 __Science-informed Knowledge__ (Traditionally, System Identification, today Scientific ML also)
        * 17.5.2.2 __Expert Knowledge__ (Traditionally, Expert Systems or Bayesian Priors, today handcrafted architecture/learning algo also)
        * 17.5.2.3 __Games__ (Need to follow Explicit Rules) (Traditionally Planning, today Learning also)
* 17.6 __Levarage extra data__
    * 17.6.1 __Extra features__ (Some inputs to your model might have some sidealong extra features that can usefull for the prediction, but incorporating them in training is not trivial. Usually these get discarded, are treted as features with too much missing values in the dataset. E.g., if model is not confident it could ask user to give more information. Obs: if you want to model the joint P(X,Y) you shouldnt be discarding datapoints with missing features.)
    * 17.6.2 __Extra labels__ (Other types of supervision signals) (e.g., of tralating system, where user can give ffedback if tranlation was good or not (binary). The system wasnt trained on this supervision signal, but shurely can take advantage of it, if engineered correctly)
* 17.8 __Estimate any missing variable__ (any variable can be though of as target and the rest as features)
* 17.9 __Structured Prediction: Prediting Outputs that dependend on each other (entire output does not live in Euclidean Space)__ (e.g., predicting sequences or trees; e.g., ''Recommend movies to a user who watches 70% comedy, 30% action. What happens if you estimate that the most likely movies to be watched are all comedies? DO you recommend them? No, need to correct for our 70% comedy 30% action proxy.)
* 17.10 __Deal with Noisy Data__
    * 17.10.1 __Noisy Features__
    * 17.10.2 __Noisy Labels__
* 17.11 __Deal with imbalanced datasets__ (when one class has much more datapoints than another, its actualy special case of ''More Homeogenous Behaviour'')

#### :bomb: __Solve Surrounding Problems__ (Now getting more attention from the ML research community)

_Note_: all the solutions to problems below would have to maintain performance high enough vanilla corrected predictive power (e.g., custom evaluation function could have fsigmoid on acc (more acc after a certain point doesnt help much, but if acc drops below this certain point it impacts negatively a lot))

1. __High Speed__
    * 1.1 __ML Workloads__
        * 1.1.1 __Low Training time__ (how long takes to train due to type of model. (Some factors: model, code, infrastructure, hyperparameter tuning, optimization algorithm)
        * 1.1.2 __Low Tuning time (# of Tuning runs*Training time)__ (how long it takes on avarage to tune hyperparameter so that it gets the corrected predictive power mentioned (need to compute average over developers or datasets)). _Note:_ The same algorithm can have more than 1 Tuning Times, e.g.,  if our DT with default parameters has 80% acc, and we tune it during 10 tuning runs to get 82% acc, we can report model1 := default tree and model2 := 10_run_tree and then see which we think it more useful.
        * 1.1.3 __Low Inference time__ (how long takes to give output to client. (Some factors: model, code, infrastructure, data pipelines)
        * 1.1.4 __Low Annotation (can be more than just labelling) time__ (annotation (can be more than just labelling) has to be fast to provide you with more training data/time, but especially to provide you with ground truth over already made inferences, to monitor how well the model is doing in production)
    * 1.2 __General Fast Code__
        * 1.2.1 __Fast Programming Language__
        * 1.2.2 __Fast & Parallel Hardware__
            * 1.2.2.1 __Fast: mainly clock speed__
            * 1.2.2.2 __Parallel__
                * 1.2.2.2 __# of Cores__
                * 1.2.2.2 __Types of Processing Units Available__
        * 1.2.3 __Optimized Code wrt the Hardware__
            * 1.2.3.1 __Custom Code__
                * 1.2.3.1.1 __Vanilla Efficient__
                * 1.2.3.1.2 __Parallel-like__
                    * 1.2.3.1.1 __Concurrent__ (when suited)
                    * 1.2.3.1.2 __Truly Parallel__ (when suited)
                        * 1.2.3.1.2.1 __Vectorized (Exec model: SIMD)__ (when suited)
                        * 1.2.3.1.2.2 __GPU-like (Exec model: SIMT)__ (when suited)
                * 1.2.3.1.3 __Async__ (when suited)
            * 1.2.3.2 __Interactions between Frameworks/Libraries__

2. __High Security__ (avoid weird shit)
    * 2.1 __Integraty__
        * 2.1.1 __Avoid Poisoning Attacks__ (Troll provides bad behaviour as input as potentially output (e.g.,  users trolling chatbot). Or malicous actor poisons third party data/model you are using indirectly. (especially relevant to online learning)
            * 2.1.1.1 __Data: Direct Poisoning__
                * 2.1.1.1.1 __Adversaries__ (malicous actor, usining the ML Service, modifies his inputs with a speicific weird attribute, the model learn spurious dependency with that attribute, and in inference the malicous actor can send any input with that attribute and he will get the desired output))
                    * 2.1.1.1.1.1 __Input ramains semantically reasonable__ (human cant tell its weird)
                    * 2.1.1.1.1.2 __Input doesnt remain semantically reasonable__ (human cannot tell its weird)
                * 2.1.1.1.2 __Trolls__ (Trolls provide non-realistic or inapropriate training data, non semantically equivalent)
            * 2.1.1.2 __Data and/or Models: Indirect Poisoning__(malicous actor poisons third party data/model you are using indirectly (pre-trainined models & public datasets))
                 * 2.1.1.2.1 __Input ramains semantically reasonable__ (human cant tell its weird)
                 * 2.1.1.2.2 __Input doesnt remain semantically reasonable__ (human cannot tell its weird)
        * 2.1.2 __Be Robust__
            * 2.1.2.1 __Natural__
                * 2.1.2.1.1 __Out-of-distribution cases__ (Cases the system did not see much or didnt see at all (out-of-distribution or out-of-sample) in training. E.g., for images, foggy images, with finger on it, etc)
                    * 2.1.2.1.1.1 __Domain Adaptation__ (When the model was trained in an environment x and deployed in environment y (the two environments share the core principles in common, but they have some differences. E.g., autonomous vehicle trained on simulated environment and deployed in real world))
                    * 2.1.2.1.1.2 __Tail Cases__ (People can also refer to them as Edge Cases. Naturally difficult for the model to learn and perform well) (cases on the tail of the joint distribution, probabably a lot of them didnt appear on the training set, hard to learn the patterns regarding them)
                    * 2.1.2.1.1.3 __Noise__ (When noise get added to an input. E.g.,  environment perturbs sensor information, and adds noise to the input signal)
                * 2.1.2.1.2 __Decision Boundary Cases__ (People can also refer to them as Edge Cases. Naturally difficult for the model to learn and perform well) (cases that are near the true decision boundaries on the feature space. SOmetome even hard to human, because they have things in common with various other parts of the feature space that have different dependent variable assignments (for classification: different labels, for regression: different local functions))
            * 2.1.2.2 __Avoid Wrong Output Attacks__ (An adversary changing intentionally the inputs might make it misbehave? Generally means that the attacker should have access to the model, which is rarely the case. Cases this applies are in code that runs in the client side like in your your browser or mobile phone (e.g., antivirus or model running as federated learning. Another option is the attacker probing a lot your model  (_Note_: giving prob outputs is not secure, giving the ucertainty in semantic bins is better) and stealing your model, this can be avoided by request limiters/client but will impact power users and if it doesnt require an account, attacker might just use multiple clients.))
                * 2.1.2.2.1 __By attack intent__
                    * 2.1.2.2.1 __Output has to be wrong and known a priori__ (e.g., making person identification system identify an attacker as a specific person from the organization)
                    * 2.1.2.2.2 __Any wrong output is sufficient__ (e.g., making tesla vehicle crash) (This is much easier, and in some cases can be done without model knowledge or probing. E.g., changing image background. If the attacker knows roughly the trianing dataset, it can try several out-of-distribution candidate)
                * 2.1.2.2.2 __By input contraints__
                    * 2.1.2.2.2.1 __By semantic contraints__
                        * 2.1.2.2.2.1.1 __Input ramains semantically reasonable__ (human cant tell its weird. E.g., image of a bird still seems the same to the naked eye, but non-random perturbations (invariance tests) are added to it to make the model misbehave. Usually perturbations are treated as a vector as have max norm.)
                        * 2.1.2.2.2.1.2 __Input doesnt remain semantically reasonable__ (human can tell its weird. E.g., image just seems random.)
                    * 2.1.2.2.2.2 __By payload contraints__
                        * 2.1.2.2.2.2.1 __Payload-constrained input__ (attacker needs to put certain payload content in the input, wants the system to get the decision wrog but also something more. E.g.,  evading malware detection from machines and humans while producing usefull malware)
                        * 2.1.2.2.2.2.2 __Unconstrained input__ (attacker has no restrictions regarding input (besides being semantically reasonable), just wants to make the system get the decision wrong. E.g.,  making CV system of autonomous vehicle make wrong decision by using weird fabricade stop signs)
                * 2.1.2.2.4 __By method__
                    * 2.1.2.2.4.1 __Model-based (Vanilla Optimization)__ (attacker needs to hands on the model or estimate the model to then do gradient ascent on the input (change input to increase confidence of the estimator))
                    * 2.1.2.2.4.2 __Hill Climbing (Combinatorial Optimization)__ (instead of doing gradient ascent on the input, just do basic genetic algorithm or heuristics)
            * 2.1.2.3 __Avoid Explability/Interpretability Attacks__ (analogous to Robustness Attacks, but instead of wanting the model to output the wrong prediction, here the attacker wants the expalability/intepretability tool to output wrong explanation/interpretation and model still makes wright prediction)
            * 2.1.2.4 __Avoid Watermark Attacks__ (carving the input as such to make the model not output a correct watermark or extracting the watermark algorithm the model is using as to make fake watermarks on human-generated data)
            * 2.1.2.5 __For Generative Models: Avoid Dagerous Synthesis__
                * 2.1.2.4.1 __Avoid Inapropriate Outputs__ (e.g., spam, incentivizing going agaist law, slurs, pornography, etc)
                * 2.1.2.4.2 __Avoid Prompt Injection__ (similar to SQL injection, the ouput makes your system misbehave in a very programmed manner planned by the attacker, so that it do dnagerous stuff in your backend)
        * 2.1.5 __Avoid Dangerous Misuse__ (e.g., asking LLM how to make a bomb, or websites with pirate content)
    * 2.2 __High Privacy__ (aka Confidentiality)
        * 2.2.1 __Attacks__
            * 2.2.1.1 __ML Attacks__  (can a malicious agent infer private data from people by probing the model and/or knowing something of the model and/or candidate datapoint? Generally means that the attacker should have access to the model, which is rarely the case. Cases this applies are in code that runs in the client side like in your your browser or mobile phone (e.g., antivirus or model running as federated learning). Another option is the attacker probing a lot your model (_Note_: giving prob outputs is not secure, giving the ucertainty in semantic bins is better) and stealing your model to then do the attacks on unlimited probing (this can be avoided by request limiters/client but will impact power users and if it doesnt require an account, attacker might just use multiple clients)
                * 2.2.1.1.1 __Model-based (Vanilla Optimization)__ (white-box attack: attacker needs to know the model's parameters or estimate them, to then do gradient ascent on the input (change input to increase confidence of the estimator))
                * 2.2.1.1.2 __Coonfidence-based (Combinatorial Optimization)__ (black-box attack: attacker doesnt need to know nothing about the model, instead of doing gradient ascent on the input, just do basic genetic algorithm or heuristics unti he finds an input which gives high output confidence)
                * 2.2.1.1.3 __Explanation-based__ (black-box attack: uses model explanations of predictions to uncover sensitive dataset information)
                * 2.2.1.1.4 __Output-based__ (black-box attack: crafts input to make model output sensitive information (most common in Generative Language Models))
            * 2.2.1.2 __Traditional Attacks__
                * 2.2.1.2.1 __Data leakage attacks__ (attacker leaks sensitive data from companies servers)
                * 2.2.2.2.2 __De-Anonymizing data__ (ananymized dataset is public and attacker has access to another leaked dataset. Attacker can match datasets to de-anonymize the data)
        * 2.2.2 __Right to be forgotten__ (users not wanting data about them in companies servers and/or user data privacy compliance rules)
        * 2.2.3 __Overcome Data silos__ (data from different organizations are not integrated to produce a better model that could benefit all of them)
            * 2.2.3.1 __Privacy Regulations__ (e.g., hospital cant share patient data with other organizations)
            * 2.2.3.2 __Competitive Advantage__ (Companies dont want to give their data advantage to others)
    * 2.3 __Intellectual Property (IP) Attacks__ (can a malicous actor steal your model by probing it or directly extracting it from an edge deployment). This often is a first step to model-based Robustness or Privacy Attacks

3. __High Max Throughput to Users__ (how many requests/time or user can the system handle? Obs: handle must be defined in terms of a metric)
    * 3.1 __Avoid Availability Attacks__ (e.g., DoS attacks)
    * 3.2 __Have a large-enough system__

4. __Explanability/Interpretability__ (what-if situations, deriving formulas, feature importance/attribitions, visualizations, causality etc)

5. __Minimize cloud/premise costs /time or /request__ (training and inference)
    * 5.1 __Origin of costs__
        * 5.1.1 __IaaS__ (you arrange you cluter & infrastructure, and then do whataver you want with it)
            * 5.1.1 __Hardware__
                * 5.1.1.1 __Types of hardware__
                    * 5.1.1.1.1 __Machine Specification__ (Vertical Scaling) (Usually virtual machines)
                        * 5.1.1.1.1.1 __Memory size__
                        * 5.1.1.1.1.2 __Disk size__
                        * 5.1.1.1.1.3 __Processors__
                            * 5.1.1.1.1.3.1 __Number of cores__
                            * 5.1.1.1.1.3.2 __Type of processor__
                        * 5.1.1.1.1.4 __Clock-speed__
                    * 5.1.1.1.2 __Infrastrcuture Hardware__
                        * 5.1.1.1.2.1 __Layer 4 load balancers__
                        * 5.1.1.1.2.2 __Internet speed (cables, router, ISP, etc)__
                * 5.1.1.2 __Amount of hardware__ (Horizontal Scaling)
                * 5.1.1.3 __Window of time that you need the hardware__
            * 5.1.2 __Resource consumption__
                * 5.1.2.1 __Processor (CPU/GPU generally) usage__
                * 5.1.2.2 __Memory usage__
                * 5.1.2.3 __Networking usage__
                * 5.1.2.4 __Storage usage__
        * 5.1.2 __PaaS (Managed Services): platform usage__ (you still code but just interact with the service, the service itself is managed for you. E.g., storage services like S3, or ML services like Sagemaker)
        * 5.1.3 __SaaS: service usage__ (you dont code, just use the system through UI. E.g., salesforce)
    * 5.2 __Types of costs__
        * 5.2.1 __Software Infrastructure costs__
        * 5.2.2 __Experimentation costs__
        * 5.2.3 __Final model building costs__
            * 5.2.3.1 __Training costs__
            * 5.2.3.2 __Tuning costs__
        * 5.2.4 __Inference costs__
  
6. __Uncertainty Estimation__
    * 6.1 __Knowing topics you don't know about__
    * 6.2 __Outputing an y distribution that reflects your uncertainty about the output (Model Calibration)__
    
    (knowing how much you are sure about a decision, output probabilities actually reflecting the unncertainty)

7. __Deal with Constraints when developing for Edge Devices__
    * 7.1 __Hardware Contraints (Inference)__
        * 7.1.1 __Heterogeneous computing paradigms__ (very different architectures, may or may not have OS)
        * 7.1.2 __Less compute power__ (processors, clock-speed etc)
        * 7.1.3 __Limited energy__ (due to computation & networking)
        * 7.1.4 __Small memory__
        * 7.1.5 __Low-speed or no internet__
    * 7.2 __Software Contraints (Training)__
        * 7.2.1 __Lack of ML Libraries__

8. __Fair Behaviour__
    * 8.1 __More Homogeneous behaviour__
        * 8.1.1 __More Homogeneous corrected predictive power (across slices of data)__ (when the model does not have uniform acc across the input space (which is the default, considering the joint distribution is probabably not uniform and therefore the optimization problem will lead to uneven accuracies. However performing badly in few slices of inputs may be very significant in production.)) Some solutions: ''How to identify slices: 1. Heuristics (requires domain knowledge); 2. Error analysis (patterns that misclassified examples follow); 3. Automatic Slice finder: Exhaustive/beam search, Clustering, Decision tree''. Generally what we see is a mixture of the behaviours below.
            * 8.1.1.1 __Model Problems: Avoid Dangerous Social Bias__ When the model has non-uniform behaviour based on features the user cannot change (usually non-causal features).
                * 8.1.1.1.1 __Level 1: Misfit-based (having reducible error) Unfairness:__ statistically, the model does not do the right thing (typically when we dont have enough data in some regions)
                    * 8.1.1.1.1 __Avoid Consistent Errors in regions of P(X,Y): Statistically Biased Model__
                        * 8.1.1.1.1.1 __Consistent errors for X protected features regions__ (Are the signed errors of the model consistent? E[f_hat(x) - f(x)] == E[f_hat(x)] - E[f(x)] in a specific region of interest. Specifically in the context of Dangerous Social Bias , does the model show statistical bias in protected\_feature == protected\_value regions?) (Slices are protected\_feature == protected\_value; (e.g., gender == women) and statistical bias is in a dangerous direction (e.g., predicting often that women are cooks)
                            * 8.1.1.1.1.1.1 __Necessarily biased data collection mechanism__ (if you had infinite dataset, it would be biased wrt true P(X,Y) in a region) any model fit will end up biased
                            * 8.1.1.1.1.1.2 __Not necessarily biased data collection mechanism__
                                * 8.1.1.1.1.1.2.1 __Necessarily biased dataset: due to high sampling distribution of the empirical distribution__ When the sample size is smaller, expected value (of the target) has higher variance. This high variance can go either way: 1. Non-Dangerous: Biasing Minority further away from discrimination concerns; 2. Dangerous: biasing Minority in the direction of discrimination concerns. Can solve by: 1. debiasing data; 2. increasing sample size.
                                    * 8.1.1.1.1.1.2.1.1 __Interpolation: fitting with similar training inputs on both sides (seen some cases wrt some dimension)__ model matches new data with similar data it has seen in training, that is around the current datapoint: simlar values in some dimensions (it interpolates). This can lead to consistent types of errors on specific slices of data. E.g., if the data has y == f(x) has a valey but we didnt get data from the valley. The main solution is to unbiase the data.
                                    * 8.1.1.1.1.1.1.2.2 __Extrapolation (totally new cases wrt some dimension): fitting with similar training inputs on one side only__ Consistent Behaviour, but wrong. The ground truth function is consistent but in another way. E.g., when compressing models by pruning this often happens
                                * 8.1.1.1.1.1.2.2 __Not Necessarily biased data__
                                    * 8.1.1.1.1.1.2.2.1 __Suboptimal Fit__ Due to stochasticity in training (parameter initialization & optimizer) directs parameters towards a biased local optima (lets say in your parameter initialization, males (SEX == 1) are coincidently already with the right fit and receiving more weight (say, weight == 6) to explain target variable than females (SEX == 0) (say, weight == 2) . But the data doesnt say this, the datapoints are literally the same, all with Y == 6, however, there are much more male (SEX == 1) datapoints than female (SEX == 0), so optimization favours lowering the error on males. This causes situations where the optimizer encounters a local minima that seems very good, however, the point fitted to females (SEX=0) is still below the actual datapoints and where the fitted point should actually be)). One solution is to give the model more freedom to find better optima by overfitting first and then regularizing (done in majority of ML projects) or to do several initializations.
                        * 8.1.1.1.1.2 __Consistent Errors for Y protected class__ (e.g., if classification: model uncertainty for Y == 3 is consistently more optimistic than for Y == 1; or if regression: model error for 300 < Y < 400 is consistent (biased))
                            * 8.1.1.1.1.2.1 __Necessarily biased data collection mechanism__ (if you had infinite dataset, it would be biased wrt true P(X,Y) in a region): any model fit will end up biased
                            * 8.1.1.1.1.2.2 __Not necessarily biased data collection mechanism__
                                * 8.1.1.1.1.2.2.1 __Necessarily biased dataset: due to high sampling variability of empirical distribution__ When the sample size is smaller, expected value (of the target) has higher variance. This high variance can go either way: 1. Non-Dangerous: Biasing Minority further away from discrimination concerns; 2. Dangerous: biasing Minority in the direction of discrimination concerns. Can solve by: 1. debiasing data; 2. increasing sample (dataset) size.
                                    * 8.1.1.1.1.2.2.1.1 __Interpolation: fitting with similar training inputs on both sides (seen some cases wrt some dimension)__ model matches new data with similar data it has seen in training, that is around the current datapoint: simlar values in some dimensions (it interpolates). This can lead to consistent types of errors on specific slices of data. E.g., if the data has y == f(x) has a valey but we didnt get data from the valley.
                                    * 8.1.1.1.1.2.1.2.2 __Extrapolation (totally new cases wrt some dimension): fitting with similar training inputs on one side only__ Consistent Behaviour, but wrong. The ground truth function is consistent but in another way. E.g., when compressing models by pruning this often happens
                                * 8.1.1.1.1.2.2.2 __Not Necessarily biased data__
                                    * 8.1.1.1.1.2.2.2.1 __Suboptimal Fit__ Due to stochasticity in training (parameter initialization & optimizer) directs parameters towards a biased local optima
                    * 8.1.1.1.2 __Avoid Higher error rate in regions of P(X,Y): Statistically Varianced Model__
                        * 8.1.1.1.2.1 __Avoid Higher error rate for X protected features regions__ (Are the absolute errors of the model inconsistent? Var[f_hat(x) - f(x)]. Specifically, in the context of Dangerous Social Bias , does the model have a higher error rate on some protected\_feature == protected\_value_?) This usually happens due to way less data on the X pretected feature region with worse performance
                            * 8.1.1.1.1.2.1 __High Capacity Model__
                                * 8.1.1.1.1.2.1.1 __Out-of-Distribution Extrapolation (totally new cases wrt some dimension)__ Bad Complex (but usually better than random) Behaviour (is the model doing seemingly random guessing in some kinds of inputs? Usually due to having not seen nothing like this input in its training data.)
                                * 8.1.1.1.1.2.1.2 __Overfitting (seen some cases wrt some dimension):__ model is so complex that it fits noise
                            * 8.1.1.1.1.2.2 __Low Capacity Model__
                                * 8.1.1.1.1.2.2.1 __Skewed Capacity (seen some cases wrt some dimension):__ if uniform-biased data (diverges significantly from uniform distribution): capacity gets directed to high density regions;
                                * 8.1.1.1.1.2.2.2 __Underfitting (seen some cases wrt some dimension):__ roughly uniform data Simple Behaviour when actually it should be more complex.
                                * 8.1.1.1.1.2.2.3 __Extrapolation (totally new cases wrt some dimension):__ simple Behaviour when actually it should be more complex.
                        * 8.1.1.1.2.2 __Avoid Higher error rate for Y protected class (Commonly referred as Imbalanced data)__ (e.g., if diagnosis someone with cancer will have higher error rate of being detected than someone with brain injury, might not be acceptable). This usually happens due to way less data on the class with worse performance (Imbalanced data)
                            * 8.1.1.1.2.2.1 __High Capacity Model__
                                * 8.1.1.1.2.2.1.1 __Out-of-Distribution Extrapolation (totally new cases wrt some dimension)__ Seemingly Random Complex Behaviour (is the model doing seemingly random guessing in some kinds of inputs? Usually due to having not seen nothing like this input in its training data. The model then depends entirely on the initilization and optimization side-effects)
                                * 8.1.1.1.2.2.1.2 __Overfitting (seen some cases wrt some dimension):__ model is so complex that it fits noise
                            * 8.1.1.1.2.2.2 __Low Capacity Model__
                                * 8.1.1.1.2.2.2.1 __Skewed Capacity (seen some cases wrt some dimension):__ if uniform-biased data (diverges significantly from uniform distribution): capacity gets directed to high density regions;
                                * 8.1.1.1.2.2.2.2 __Underfitting (seen some cases wrt some dimension):__ roughly uniform data Simple Behaviour when actually it should be more complex.
                                * 8.1.1.1.2.2.2.3 __Extrapolation (totally new cases wrt some dimension):__ simple Behaviour when actually it should be more complex.
                * 8.1.1.1.2 __Level 2: Coarse-grained-based (having irreducible error) Unfairness:__ statistically, the model does the right thing; but does not necesserily do the right thing to individual instances (when we dont have access to causal features, so we use non-causal features that proxy them). _Note:_ user cant know if we are or are not using these causal features. But they can complain in both scenarios because the outcome is the same: differences in expected Y for different groups. The thing is: when using causal features (or removing protected feature info) you can just show them that if get a datapoint that is member of of group perceived to be subject of unfairness and change the other features it will also change output or if you change the protected feature it will not change output
                    * 8.1.1.1.2.1 __Group Differences: avoid Non-uniform predictions in X regions__ Uniform-biased (Bias wrt to uniform distr.) P(X,Y) in protected feature regions. Basically when reality states that that P(Y=y|ProtectedClass=protectedValue) is not uniform, but there is pressure to make the model treat it as uniform. This should not be a problem, considering the you are doing the best job with the features you are working with, have but ends up being, because in the eyes of the user, you should have more detailed features that would enable you to not make a more fine-grained judgment, based on causal features. In practice, it is often super hard or impossible to get these causal features (e.g., the ones that substitute gender) and programmers end up using non-causal features that help achieve good accuracy on the test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant), but this is not the ideal option in the ML sense also; considering the test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) is not a very precise measure of your model in production, since there will be Data Distribution shift (Note: sudden shifts are often indicative of data bugs) (in particular, input/covariate shift) and your non-causal features might not be robust to it; also there can be feedback loops. So in the end it is a tradeoff between test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) corrected predictive power vs external fairness pressure + data distribution and feedback loops robustness. E.g., model that uses gender feature to predict if a person is a nurse or CEO, since most CEOs are mena dn most nurses are wommen, the model learns that gender is a hihly informative features, which is very important for it. But people complain about these differences, because you should have more knowledge of the person to make this decision, istead of using gender as an important feature; or model that uses ''marrietal status'' as a feature to estimate loan, then user figres this out and separates from his wife just to get a better loan.
                    * 8.1.1.1.2.2 __Instance Differences: avoid different Y when changing protected feature value__ (E.g., only changing gender of an instance shouldnt change it recommendations. 2 ways to do this (in both ways, gender is metadata): 1. the model does not use (explicitely and implicetly) gender information; 2. the model onsly uses causal features (can infer gender information with it))
            * 8.1.1.2 __Monitoring Problem: Avoid High variance predictive power estimate__ the more heterogeneous predictive power is, higher the variance of our production predictive power estimate (with labels or no labels)
        * 8.1.2 __More Homogeneous Predictive coverage__ (e.g.,  recommender systems performs well on test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) but does not recommend any eletronic products, and this pisses off eletronic vendors)
    * 8.2 __Meritocratic Behaviour__ (This is only a problem when ''More Homogeneous behaviour'' was treated to severely, increasing the error rate of the model. Distance to optimal classifier can be to big, because you start putting groups in front of individuals. Measures: 1. Iinframarginality: distance of classifications from classifications of the bayes classifier. In practice, the bayes classifier is aproximated by the best model (ignoring more homogeneous behaviour; 2. Underranking: worst-case displacement of an individual's true rank after group-fair (''More homogeneous behavior'') reranking))

9. __Deal with Dynamic World__
    * 9.1 __Core problems__
        * 9.1.1 __Structural change in Model__
            * 9.1.1.1 __Task change__
                * 9.1.1.1.1 __Task Redefinition__ (need to redifine the ML task. e.g from regression to classification)
                * 9.1.1.1.2 __Task Addition/Removal__
            * 9.1.1.2 __Schema change__ (input our output data changes schema). (Some Solution ideas --> 1. Input schema change: can happen very often, especially adding/removing features.(1.1 Adding/removing features: do retraining or handle missing data/dicard data at inference; 1.1.1 Adding Features: can train another model that only receives new feature as input as predicts target, than ensemble this new model with the old; 1.2 Tranforming features: retrain or do pre-processing until a natural retrain occurs; 2. Output Schema change: 2.1: Adding/removing target dimensions: you typically fix training algo I/O hyperparameters and retrain; 2.2: Tranformationing target: retrain or do post-processing until a natural retrain occurs))
        * 9.1.2 __Decision thresholds shift__ (e.g.,  suddely need to be more than 9.17% confident to give decision)
        * 9.1.3 __Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s__ (when P(X,Y) in deploy changes wrt training. In practice though, recent time window of data is compared to last time window of data in which model was performing well, not training data) (2 independent shifts and one dependent one) (can be cyclic/seasonal and time dependent (seasonal or converging))
            * 9.1.3.1 __Process (Concept) Shift (independent shift)__ (usually called concept drift, reflected in P(Y|X)) (where the actual process that generates the data of interested changed)
                * 9.1.3.1.1.1 By type:
                    * 9.1.3.1.1.1.1 __Algorithmic shift: change in ideal X --> Y function__ (this is the shift in the real-world algorithm/function mapping X to E[P(Y|X)] according to our assumptions/hypothesis)
                    * 9.1.3.1.1.1.2 __Noise shift: change in P(Y|X) noise__ (when unmodelled variables change their distribution (can also change process in a way that one compensate the other, but is much more rare), in our model this is just reflected as noise in P(Y|X) and P(Y))
                * 9.1.3.1.1.2 By way of happening:
                    * 9.1.3.1.1.2.1 __Abrupt__
                        9.1.3.1.1.2.1.1 __Step: long-lasting shift (should implement changes to account for new distribution)__
                        9.1.3.1.1.2.1.2 __Impulse: short-lasting shift, that goes back to normal (should treat as anomaly)__
                    * 9.1.3.1.1.2.2 __Gradual__
                    * 9.1.3.1.1.2.3 __Periodic__
            * 9.1.3.2 __Input (Covariate) shift (independent shift)__ (usually called covariate shift, reflected in P(X)) (input data distributions should be roughly equal in training and inference. If model starts seeing data it saw few times of never saw can make bad decisions). Some non-trivial cases where covarite shift occurs: when compensating imbalanced training datasets or in active learning systems.
                * 9.1.3.2.1 __Raw data shift__ (this is the real input shift and what most people thinnk about when they refer to input shift. Happens when the input to real world process shifted its distribution.
                    * 9.1.3.2.1.1 __Upstream process shift__ (i.e. the process that generate this input (which we are not modelling) shifted)
                    * 9.1.3.2.1.2 __Confounder shift__
                        * 9.1.3.2.1.2 __Confounder distribution shift__
                            * 9.1.3.2.1.2.1 __Hard shift (a lot of values of confounder go to prob=0)__ (when your training environment is not reflecting of future production cluster, but you dont have data distribtion shift, you have new or deleted variables. This is usually caused by relying on non-causal features, and a confounder (that you dont have in your dataset) dissaperas in production or a new confounder appears in production)
                            * 9.1.3.2.1.2.2 __Soft shift__
                        * 9.1.3.2.1.2 __Confounder algorithmic shift(s)__ (when function that generates X has changed)
                * 9.1.3.2.1 __Data Engineering issues__ (raw data has not shifted))
                    * 9.1.3.2.1 __Data pipeline bugs__ (more often than not, bugs in our data pipeline, that transforms raw data to featured data whihc we then feed to our model, cause input shift in the featured data)
                    * 9.1.3.2.2 __Schema changes__
                        * 9.1.3.2.2.2 __Schema logical change__  (''e.g., your model was using years for the 'age' feature, but now it uses months, so the range of this feature values has drifted'')
            * 9.1.3.3 __Output (Target) shift (dependent shift)__ (usually called label shift, reflected in P(Y)). Think of this one as the dependent shift (more intuitive). If there is no shift in process or input, then there will be no shift in output.
    * 9.2 __Side effects: Composition of models__
        * 9.2.1 __Input shift__ when the world changes, typically you retrain your model A, but a lot of models might be depedent on A, directly or indirectly; so you need to rettrain them aswell because there will be input shift for them (this is very computationally intensive, thus speed and costs become major issues, Furthermore, humans might be acostumed to a certain model behaviour, and would need to adjust themselves to a new model behaviour))
        * 9.2.2 __Overreliance on upstrem errors__ some downtream models might be taking advantage of non-generalizable singals/errors of upstream models, when these errors are corrected, downstream models become worse. E.g., cuteness detector that uses output of cat detector and ball detector as features, if cat detector makes mistakes of classifying dogs as cats this is beneficial to the cuteness classifier, because the feature it actually wants is not cat detection, but cute animal detection; but when these errors are fixed, it loses this beneficial information. Some solutions to be robust to upstream models: cross validate cuteness detector using data generated by some other cat detector, traing downstream model with a mix of data from multiple cat detectors.

10. __Faster deployment cycles__
    * 10.1 __Monitoring__. What to monitor:
        * 10.1.1 __MLOps platform (Processing Pipelines + Specialized Systems)__
        * 10.1.2 __Inference Services__
    * 10.2 __Debugging__
        * 10.2.1 __Auditability/Metadata Management__
            * 10.2.1.1 __Data lineage__
            * 10.2.1.2 __Track related Artifacts (main I/O files)__ (intermediate outputs)
            * 10.2.1.3 __Roll-back to older versions__
        * 10.2.2 __Less Technical debt__ (better to maintain, scale and extend)
            * 10.2.2.1 __Low Complexity__ (more complex is harder to maintain)
            * 10.2.2.3 __Dependencies__
                * 10.2.2.3.1 __Dynamic Dependencies__(like computing on top of outputs of other model)
                * 10.2.2.3.2 __Amount of Dependencies__
            * 10.2.2.4 __Organization/Clarity__
            * 10.2.2.5 __Code Reuse__
            * 10.2.2.6 __Rich Ecosytem__ (is the solution implemented have a lot of resources online, is well known, has a lot of libraries that do it, pretrained easy to use models, docs)
            * 10.2.2.7 __Reproducibility__ (ability to reproduce results easily, remove sources of non-determinism in building Artifacts (main I/O files))
            * 10.2.2.8 __Good Integration with existing/legacy systems__ (making compatible with the way the company is already doing things)
            * 10.2.2.9 __Design Patterns__
        * 10.2.3 __Quality Assurance__
            * 10.2.3.1 __Testing__ (done empirically, increases your confidence that the program is right)
            * 10.2.3.2 __Verification__ (done with formal math, guarantees the program is right)
        * 10.2.4 __Easy of use__
            * 10.2.3.1 __Interface__ (e.g., nice UIs and/or CLIs)
            * 10.2.3.2 __Documentation__
            * 10.2.3.3 __Client languages support__
            * 10.2.3.4 __Similarity to/Usage of popular tools__
            * 10.2.3.5 __Human Effort__ (e.g., amount of hyperparameter optimization necessary)
            * 10.2.3.6 __Level of Knolowledge Required to Operate__ (e.g., expert knowledge for EDA)
    * 10.3 __Re-deploying__. What to re-deploy:
        * 10.2.1 __MLOps platform (Processing Pipelines + Specialized Systems)__
        * 10.2.2 __Inference Services__
        * 10.2.3 __Model files__

13. __Less Carbon footprint__
    * 13.1 __Motivations__
        * 13.1 __Regulations__
        * 13.2 __Branding__
    * 13.2 __Context__
        * 13.2.1 __Training__
            * 13.2.1.1 __Training from scracth__
            * 13.2.1.2 __Retraining__
        * 13.2.2 __Inference__
    * 13.3 __Influential Factors__
        * 13.3.1 __Energy Consumption__
        * 13.3.2 __Hardware Location/Type of Energy Supply__

15. __Good Scalability__
    * 15.1 __Model scalability (scaling laws)__
        * 15.1.1 __Scaling of:__
            * 15.1.1.1 __Corrected predictive power:__ corrected predictive power scalability with respect to data: a system that gets much better given more data is prefered over a system that does not improve that much. (e.g., If a system automatically increases hyperparameters it can capture more pattern in data, can perform better for a system with fixed hyperparameters which ends up not being able to overcome underfitting). Is is also very usefull to estimate how much new data you need to get a specific boost in predictive power.
            * 15.1.1.2 __Resource Consumption__
                * 15.1.1.2.1 __Resources__
                    * 15.1.1.2.1.1 __Time__
                        * 15.1.1.2.1.1.1 __Training Time__
                        * 15.1.1.2.1.1.2 __Inference Time__
                    * 15.1.1.2.1.2 __Energy__
                    * 15.1.1.2.1.3 __Money__ (Cost)
                * 15.1.1.2.2 __Influential factors__
                    * 15.1.1.2.2.1 __Core training: parameter updates__
                        * 15.1.1.2.2.1.1 __Hot-Start__
                        * 15.1.1.2.2.1.2 __Optimizer__
                        * 15.1.1.2.2.1.3 __Regularization strategy__
                    * 15.1.1.2.2.2 __Hyperparameter tuning__
                        * 15.1.1.2.2.2.1 __Hyperparameter initialization__ (decide which hyperparameters to start hyperparameter tuning in a way that doesnt take long to find a good setting: aka initialize hyperparameters near overfitting point)
                            * 15.1.1.2.2.2.1.1 __Theoretical: Similarity & Dimension Analysis__ use model trained on slice of the entire dataset as a miniature of entire dataset. Apply scaling methods to its hyperparameters to avoid doing to much hyperparameter tuning with the whole training dataset.
                            * 15.1.1.2.2.2.1.2 __Empirical: Model Size vs Dataset Size curve__ (useful to be able to reach overfitting point fast)
                        * 15.1.1.2.2.2.2 __Sensitivity analysis__ how fragile is the model (talking about corrected predictive power generraly) with respect to hyperparameter perturbations. In other words, if max_hyperparameters(predictivePower(modelA)) == max_hyperparameters(predictivePower(modelA)) - e where e is a smalll value; we would prefer model A if it has mostly stable accuracy accross hyperparameters space as oposed to mode B with varying a lot accuracy. Because the computational cost of finding the point where model B superseeds model A is not worth it when we have to scale our models.
                    * 15.1.1.2.2.3 __Inference Method__  
                        * 15.1.1.2.2.3.1 __Straighforward Computation__ (e.g traditional classifiers)
                            * 15.1.1.2.2.3.1 __Verification of Conditions__ (e.g., SVMs or simple DTs)
                            * 15.1.1.2.2.3.2 __Formation of Abstractions (Features)__ (e.g., NNs)
                            * 15.1.1.2.2.3.3 __Aggregation__ (Ensembles)
                        * 15.1.1.2.2.3.2 __Information Retrieval__ (e.g KNN)
                        * 15.1.1.2.2.3.3 __Optimization or Search__ (e.g., problem solving tasks where you have learn a heuristic function and need to optimize the input to it; or aproximating bayesian posterior though variational inference)
                        * 15.1.1.2.2.3.3 __Sampling__ (e.g., aproximating bayesian posterior though MCMC methods)
                        * 15.1.1.2.2.3.4 __Analytical__ (e.g., calculating exact bayesian posterior by using conjugate prior; or exact message passing algorithms)
                    * 15.1.1.2.2.4 __Post Processing__
                        * 15.1.1.2.2.4.1 __Calibration__
                        * 15.1.1.2.2.4.2 __Compression__
        * 15.1.2 __With respect to:__ (# of features is not here because given a dataset, a company wont add so many features afterwards)
            * 15.1.2.1 __Dataset Size: # of datapoints__
            * 15.1.2.2 __Size of Model (# of Parameters)__ (you would thinks that scaling models is only a connsequence of scaling data, because if not it would cause overfitting, thus it is the same as ''Dataset Size''; but in the double descent regime, scaling models with fixed data is beneficial)
            * 15.1.2.3 __Latency requirement__ (measures the ability a model has to make better inferences if it has more time to "think")
    * 15.2 __System scalability__
        * 15.2.1 __Availability scalability__. Availability of the system with respect to increasing traffic: a system that can remain available/fast when receiving more traffic (and downscale afterwards when not needed that much resources).
            * 15.2.1.1 __Provision more Software__ (when hardware resources are not saturated)
                * 15.2.1.1.1 __Privision more software infrastrcuture__ (e.g., software reverse proxies, cluster entrypoints)
                * 15.2.1.1.2 __Provision more replicas of microservices__ (usually provision more pods in Kubernetes/OKD) (when hardware resources are not saturated yet)
            * 15.2.1.2 __Provision more Hardware__ (when hardware resources are saturated on all namespaces or instances burn)
                * 15.2.1.2.1 __Virtual Hardware__
                    * 15.2.1.2.1.1 __Allocate more hardware for pods & VMs__
                    * 15.2.1.2.1.2 __Allocate more hardware for logical clusters__
                * 15.2.1.2.2 __Physical Hardware__
                    * 15.2.1.2.2.1 __Privision more hardware infrastrcuture__ (e.g., hardware reverse proxies, routers, CDNs)
                    * 15.2.1.2.2.2 __Provision more compute resources__ (usually more AWS EC2 instances of some type (_Note:_ the type of instance depends on the workloads, so the autoscaling function should look into the deploy specification of the container images that need to be deployed) & S3 storage)
        * 15.2.2 __Data Engineering scalability__. Ability to handle (generate, store, annotate and process) inscreasingly larger data.

16. __Wise training data selection__ (Model that chooses datapoints he wants to be trained on, to get examples in situations he is not shure or difficult situations, will get better performance than a system that doesnt choose and receives random data)
    * 16.1 __Data Collection__ (gathering more data)
        * 16.1.1 __Improving corrected predictive power__
            * 16.1.1.1 __Prevention__
                * 16.1.1.1.1 __Getting datapoints in data-scarce spaces of the feature space__ (places the model has not had the opportunity to identify patterns (so performance will be random or according to a prior))
                * 16.1.1.1.2 __Getting hard datapoints: 1. Regression: in high frequency regions; 2. classification: Decision boundary regions__ (naturally difficult cases)
                * 16.1.1.1.3 __Loss-Function/Evaluation aware data collection__ (if the loss function/evaluation depends on the input, there are certain regions that will require more attention than others)
                * 16.1.1.1.4 __Getting data which generates disagreement between components of emsemble (models)__ (leveraging the power of different approaches)
            * 16.1.1.2 __Mitigation__ (Data Flywheel) (Get error-prone data (types of inputs its geeting wrong in the CV set, test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) or in production) (but then either you relly on proxy label signals or you need a human to evaluate the outputs of the model))
                * 16.1.1.2.1 __Getting error-prone datapoints__ (from real world)
                * 16.1.1.2.2 __Generating error-prone datapoints__ (artificially)
        * 16.1.2 __Improving Causal Understanding__ (Get data that lets you calculate causal effects (random-like))
        * 16.1.3 __Making corrected predictive power more Homogeneous__ (Get more data in regions where you dont have much, but should have more, according to the P(X,Y) you are modeling)
    * 16.2 __Data Ordering (in Epoch)__ (just learning from data in a different order)

20. __Act in an Environment__ 
    * 20.1 __Deal with degenerate feedback loops__ (when a model's predictions affect X data gathering for the same/another model) (_Note_: this is a consequence of agent sequential decision making tasks being modelled as independent one-time prediction task, aka RL done as Supervised Learning) which causes non iid X inputs.
        * 20.1.1 __Definition: X_t/Y_t dependent on Y_{t-1}__ Data is model dependent (if you give historical training data, you must give the model also). In time, small difference in output prob of two classes leading to large differences in output prob for each class. (E.g.,  of recommender system where reccomnedation shown o top have a much more prob of being clicked than predictions on the bottom or even more (of course) for not shown predictions, the system can start thinking user likes a lot some item and recommend it again (exploitation), but in fact they are just clicking the one the cathes their attention most, and this would transfer to any other item)
        * 20.1.2 __Patterns__
            * 20.1.2.1 __[X_t/Y_t] monotonically [increases/converges] (to some class/range of values):__ (e.g., for classification in recommeder systems: In time, small differences in output prob of two classes leading to large differences in [X data/prob] for each class, since X_t is aprox Y_t)
            * 20.1.2.2 __[X_t/Y_t] sigmoidlly (in S) converges:__ (e.g., for regression for stock trade: if the system is very powerful, when it sees that a stock it starting to go down, it can quickly sell a few of it, but then since there is now a lot of supply and little demand, the proces go down further, and then the system wants to sell more ... plummiting the prices until the system is out of stocks)
            * 20.1.2.3 __[X_t/Y_t] oscillates (between some classes/ranges of values):__ (e.g., discontinous controllers for dynamical systems)
        * 20.1.3 __Context__
            * 20.1.3.1 __Single model__ (typical scenarios nowadays)
            * 20.1.3.2 __Multiple models__(when multiple agents (models) are interacting in the same envrionment and affects each others data colllection. Its is sometimes called Canibalization when one model improves a proxy business metric but decreases other, e.g.,  a frontend of a mobile app that makes uses of  different models to provide value to users in different ways, but these compete with each other for users attention. As a result some models will see increase in engagement on their pages, and will see a decrease.)
        * 20.1.4 __Driver__
            * 20.1.4.1 __Environment-driven__ (e.g., recsys that user umber of downloads as a feature)
            * 20.1.4.2 __User-driven__ (e.g., users that change their features in order to change their output (they do interventions))
    * 20.2 __Do Interventions: Causal Inference:__ model impact of Interventions on target p(Y=y|do(X=x), maybe other variables)) & doing Counterfactuals p(Y=y'|do(X=x'),X=x,Y=y, maybe other variables)
        * 20.2.1 __Paradigms__
            * 20.2.1.1 __Using only Observational Data from one dataset__ (needs assumptions to discover causal graph in reasonable time)
            * 20.2.1.2 __Using Observational Data + Causal Domain Knowledge from one dataset__ (typical case)
            * 20.2.1.3 __Using Observational data + Experimental data + Domain Knowledge from one dataset__
            * 20.2.1.4 __Using Observational data + Experimental data + Domain Knowledge from multiple datasets__ (Causal Data Science)
        * 20.2.2 __Goals__
            * 20.2.2.1 __Causal Effects__ (when you dont know the ouput y for a datapoint. Actually expected value over all possible Counterfactuals)
                * 20.2.2.1.1 __Avarage Treatment Effects__ (how changing a feature, changes output on avarage)
                * 20.2.2.1.2 __Individual Treatament Effects__ (how changing a feature, changes output on a specific datapoint) (This not very common to do in Experimentation, because usually you do this analysis on your dataset, which you have labels on, so you can do CounterFactuals. However, if you dont have labels in some examples, you can do this)
            * 20.2.2.2 __CounterFactuals__ (when you know the ouput y for a datapoint and want to know what woud happen with a different X datapoint): Always concerned with Individual Datapoints
        * 20.2.3 __Methodology__
            * 20.2.3.1 __Definition:__ define causal estimands: P((Y=y)|do(X=x)) that dictates the effect of X on Y
            * 20.2.3.2 __Identification:__ going from causal estiamnd to statististical estimand (without do operator) though causal graph and do-calculus
            * 20.2.3.3 __Estimation:__ estimate statistical estimands through simple Statisitcs/ML Methods
</details>

<details>
    <summary><b>Methods</b></summary>

### :syringe: __Methods__

#### :warning: __Methods disclaimer__

Not all methods are listed here, we are shurely missing some important ones. There is so much research that it's hard to keep up.

__Legend: [Problems it tries to solve | Field | _Name of the Area or Method_]__

* __6, 17.1, 2.1, 8, 9, 16, 17.4, 20.2, 7, 4, 14, 17.5, 21, 17.3 | ML Methods | _Model-related_ (scope: building models with ready dataset)__
    * 2.1.2, 17.1, 15.2.2, 8.1.1, 6 | ML Method | _Data Augmentation_
        * 2.1.2, 17.1, 15.2.2 | ML Methods | _Predictive-power-driven_
            * 2.1.2, 17.1, 15.2.2 | ML Methods | _Manual Augmentation:_
                * 2.1.2, 17.1, 15.2.2 | ML Methods | _Apply output-invariant transformation to data_
                * 2.1.2, 17.1 | ML Methods | _Error-driven_ (gather data on modes of failure of the system)
                * 2.1.2, 17.1 | ML Methods | _Decision-Boundary driven_ (gather data on decision-bounndary regions)
                    * 2.1.2, 17.1, 6 | ML Methods | _Estimating difficulty of examples: Variance of Gradients_
                    * 2.1.2, 17.1 | ML Methods | _Estimating decision boundary regions by drawing <input, output> samples and see if output varies a lot in certain regions_
                * 2.1.2, 17.1 | ML Methods | _Noise-driven: Noise estimation_ (improve annotation (can be more than just labelling) on regions with large estimated noise (irreducible error))
            * 2.1.2, 17.1, 15.2.2 | ML Methods | _Automatic Augmentation_
                * 2.1.2, 17.1, 15.2.2 | ML Methods | _Non-hypothesis driven: Sampling from P(X) models_
                * 2.1.2, 17.1, 15.2.2 | ML Methods | _Non-hypothesis driven: Using LLMs_
                * 2.1.2, 17.1, 15.2.2 | ML Methods | _Hypothesis driven: Model Patching_ (given two subgroups of a class, one with less examples that is performing badly in the test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant); you want to increase examples in this under represented subgroup. _Model Patching_ lets you generate examples of this subgroup transforming examples of the other subgroup. E.g., frog immages with finger arent being classified correctly as frog, so a human might get frog images and put finger on top of it to generate more training examples of this kind. But this is not ideal, because it requires real-world data gathering and is limited to only some kinds of things we can control. Ideally this would be done artifically by changing pixels, so thats besically what  _Model Patching_ does)
        * 8.1.1 | ML Methods | _Bias-driven_
            * 8.1.1 | ML Methods |  _Making totally new datapoints to improve equal representation_
            * 8.1.1 | ML Methods |  _Chaninging protected feature values in datapoints to improve equal representation_

    * 9.1.3 | ML Methods | _Dangerous Data Shift Detection_
        * 9.1.3 | ML Methods | _Identify Seasonalities_ (so that you can train with more data)
        * 9.1.3 | ML Methods | _Only trigger alerts/notifications for Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s when effective capacity of the model under some context (given C == c)/amount of shift ratio is high_

    * 6, 8, 9, 16, 20.2, 17.4, 2.1.2, 17.1 | ML Methods | _Semi-Supervised Learning_
        * 17.1, 2.1.2 | ML Methods | _Self-Distillation/Self-Training_ (algorithm uses its own predictions (the most confident ones) to get more label training data)
        * 6, 8, 9, 16, 20.2, 17.4, 2.1.2 | _Unsupervised Learning_ (includes Self-Supervised Learning)

    * 17.1, 17.2.1.1, 17.2.1.2, 8,  | ML Methods | _Custom loss function_
        * | ML Methods | _Compatible models_ (model after retraining doesnt make errors on instances where it got right before retraining) (enables you to retrain model A, and avoid having to retrain models dependent on model A (at least to some degree))
        * 17.2.1.1, 17.2.1.2, 8.1.1 | ML Methods |_Overcoming biased data>Model-based_
        * 17.1, 2.1.2 | ML Methods | _Focal Loss_ (Give more weight to difficult samples. Needs the model to be calibrated in-training)
        * 17.2.1.2, 8.1.1 | ML Methods | _Putting custom x and p(y) dependent terms in the objective functions_
    
    * 6, 2.1.2, 9.1.3, 17.1 | ML Methods | _Robust ML_
        * 6, 2.1.2, 17.1 | ML Methods | _Uncertainty Estimation (includes OOD Detection)_
            * 6, 2.1.2 | ML Methods | _Use Dropout at Inference_ (do inference with random dropout multiple times, then get a distribution on prediction. Same idea as bootstrapping)
            * 6 | ML methods | _Model calibration_ (probability of class actually reflecting its uncertainty. E.g., for binary classification: ''Among all samples predicted POSITIVE with propa 80%, 80% of them should be POSITIVE''. One method is temperatuure scaling on models outputs)
            * 6, 17.1 | ML Methods | _Selective Classification_ (absteining in inputs you are not shure of the output, to gain more acc, a.k.a trading coverage for acc. Adding another output option for the model which is basically "I dont know" or variants of this. The model would learn to say I dont know for things it is not good at because the loss of picking "I dont know" would be smaller than picking it worngly)
            * 6, 17.1 | _Bayesian Methods_ (1. Get posterior over parameters: Parameter estimation: prior P(teta) and posterior on parameters P(teta|X,Y); or get MLE estimate 2. Inference: if have posterior over parameters: use E_teta[P_teta(Y|X)]); else use simply P_mleTeta(Y|X))
            * 6, 2.1.2, 17.1 | ML Methods | _Data Augmentation>Decision-Boundary driven_ (the more close to a decision boundary the more uncertain model should be)
            * 6, 2.1.2, 17.1 | ML Methods | _Noise estimation_ (fitting function and estimating noise on top of it. E.g., in bayesian regression we jsut assume constant variance generally, but we could model as a function of x, namely var_fx(x))
            * 6 | ML Methods | _Supporting Model that predicts when Main Model will fail_ (maps <x,^y> --> will_fail where will_fail is in {0,1})
            * 6 | ML Methods | _Using Ensembles_ (draw n bootstrapped datasets from original dataets, fit a model to each, and then get the distribution of predictions)
        * 2.1.2.2.2.2, 2.1.2.2.2 | ML Methods | _Adversarial Training_ (training model with artificial examples that would make it mishbehave. Penalize the model in the loss function if it changes output, after adding invariace noise or if it doesnt respect real world contraints))
        * 2.1.2.2.2.2, 2.1.2.2.2 | ML Methods | _Certified Defenses_ (e.g., Randomized Smoothing)
        * 2.1.2.1.1, 6, 9.1.3, 17.1 | ML methods | _Robust Statistics/Estimation_ (Hard outlier removal using assumptions on P(X))
        * 2.1, 17.1, 6, 9.1.3 | ML Methods | _Anomaly Detection_ (e.g., one method is to apply perturbation in the direction of peak class prob regions (normal points generally have substantial entropy decrease, while anomalies generally dont; or train NNs to do auxiliary task (e.g predicting how the input was rotated)))
        * 2.1, 17.1, 9.1.3, 17.10.2 | ML methods | _Confident Learning_ (Removes examples with noisy labels. _Note:_ requires _Model Calibration_. Tool: cleanlab)
        * 6 | ML Methods | _Conformal Prediction_ (set of algorithms devised to assess the uncertainty of predictions produced by a machine learning model)
        * 6 | ML Methods | _Using None Class in Classfiers_ (e.g., if you are predicting cats or dogs, noneClass := not(CATS or DOGS))
        * 17.1, 2.1.2 | ML Methods | _Correct-N-Contrast_ (making NNs produce similar representations for two datatapoints with same label, to avoid suprious correlations and increase robustness)
    
    * 6, 9, 16, 17.4 20.2, 17.6, 17.8 | ML Methods | _Generative Models_
        * 6, 9, 16, 17.4 20.2 | ML methods | _Density Estimation_ (Generative modeling with explicit P(X) or P(X,Y))
        * 17.4, 17.6, 17.8, 20.2, 16| ML Methods | _Implicit Generative Models_
    
    * 4, 17.5 | ML Methods | _Knowledge-enhanced ML_
        * 4, 17.5 | ML methods | _Neurosymbolic AI_ (Symbol mannipulation + ML)
        * 4, 17.5 | ML methods | _Scientific ML_ (Using ML to learn scientific processes or make them faster, generally putting Scienntific priors/assumptions on it)
    
    * 6, 17.1, 6, 20.1 | ML methods | _Ensembles_
        * 17.1, 6 | ML methods | _Boosting_ (model_t+1 trained with samples weighted according to model_t errors (datappints model_t made errors get higher weight), can even use meta features of model_t like calibration or amount of data in the region. Final model is a weighted combination (weight get smaller as t increases proprtionally to size of errors) of all models) (if uses for bias detection, generally coupled with _Input Enrichment_)
        * 6, 17.1 | _Aggregation_
            * 6, 17.1 | ML methods | _Same model, different dataset: Bagging_ (''1. Sample with replacement to create different datasets; 2. Train a classifier with each dataset; 3. Dar distribution of predictions'') (Generally only improves unnstable methods such as NNs and Decision Trees and degrades stable)
            * 6, 17.1 | ML methods | _Same dataset, different model: Arguing Machines_ (The simplest form of ensembling in my opinion: 1. Train a handull of models from different families; 2. Run inference through all of them all of them; 3. Draw a distribution of predictions)
        * 17.1 | ML methods | _Stacking_ (What we generally think of when we say ensembles, a meta model which inputs are the ouptus of many weak models)
        * 20.1 | ML Methods | _Intra-Model Ensembles_ (e.g., for recommendation: one model uses just positional encoding & another uses rest of features to predict click-rate, they both present at training time. At inference only the one with rest of features is used or both are used with but running inference k times with random positional encodings and getting expected value of target)
    
    * 17.2.2, 17.3, 6, 8, 9, 17.4, 26, 17.6, 9.1.1.1.2 | ML methods | _Overcoming small data_
        * 17.3 | ML methods | _Multimodal ML_
        * 17.6 | ML Methods| _Missing data techniques_
        * 17.2.2, 17.3, 17.4, 26, 9.1.1.1.2 | ML methods | _Transfer learning_ (includes Meta Learning, Lifelong Learning & X-shot (Few-Shot/One-Shot) Learning)
        * 17.3 | ML Methods | _Contrastive Learning (includes Distance Metric Learning)_ (learns distance between inputs)
    
    *  8.1.1, 17.1, 4 | _Overcoming biased data_
        * 8.1.1, 17.1, 4 | ML Methods | _Data-based_
            * 8.1.1 | ML Methods | _Data Augmentation>Bias-driven_
            * 8.1.1, 17.1, 4 | ML Methods | _Sampling Methods_
                * 8.1.1, 17.1 | ML Methods | _Importance/Temperature/Weighted Sampling_ (downsample most frequent regions and upsample less frequent regions according to a samplingProb == I(frequencyOfThings) with the objective of getting a uniform distribution (before normalizing!))). Can even learn the Importance Function by optimmizing some custom loss function (e.g., loss funnction defined on a test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) with a different distribution)
                * 8.1.1, 17.1, 4 | ML Methods | _Sampling Methods 4 Low-Dimensional Data_
                    * 8.1.1, 17.1, 4 | ML Methods | _UnderSampling: Tomek Links_ (''1. Find pairs of close samples of opposite classes; 2. Remove the sample of majority class in each pair'')
                    * 8.1.1, 17.1 | ML Methods | _Oversampling: SMOTE_ (''Synthesize samples of minority class as convex (not(linear)) combinations of existing datapoints and their nearest neighbors of same class'')
            * 8.1.1 | ML Method |_Removing protected category info from X_
                * 8.1.1 | ML Method | _Removing features that leak a lot of protected category info (Entropy(P(ProtectedCategory=pc|RestofX=nx)) == Entropy(P(ProtectedCategory=pc)) - e; where e is small_ (that explicitly identifies protected features or features that have high dependence with protected features from the featured dataset)
                * 8.1.1 | ML Method | _Fair representations: removing all protected category info (P(ProtectedCategory=pc|NewX=nx) ~= P(ProtectedCategory=pc)_ (transforming old X into new X set of features which give no information on pretected categories)
            * 8.1.1 | ML Method | _Jury Learning_ (adding feature vector that represents labelers as input, thus learning the mapping <X,labeller> --> Y. With this you can then simulate inferences for any given type of labeller, giving you the ability to avarage over all types of labellers without having the problem of one type of labeller being much more present than another (biased set of labellers) and thus not getting the real avarage; you can also select for types of labellers at any given infrence)
            * 8.1.1 | ML Methods | _Post-processing_ (''apply a transformation F(.) such that the transformed score satisfy some metric definition'')
            * 8.1.1 | ML Method | _Apply Weak Supervision (Annotation (can be more than just labelling) Fuction) in Low Data Regions_

        * 17.2.1.1, 17.2.1.2, 8.1.1 | ML Methods | _Model-based_
            * 17.2.1.1, 17.2.1.2, 8.1.1 | ML Methods | _Changing Loss Function/Optimization Objective_  
                * 17.2.1.1, 8.1.1 | ML Methods | _Error Risk Incorporation_ (Find estimator that minimizes expeted risk of predicting y'\_hat instead(y'): argmin y'\_hat (E_{y'_hat}[R(y_hat, y')] where y' is a specific class). E.g R(Y == monkey|X == human) must be high) (May even combina a MLE and a Risk Estimation in an Ensemble and treat Risk as labels (have to gather this data from people))
                * 17.2.1.1, 8.1.1 | ML Methods | _Putting (Distance(y'\_hat, y') where y' is a specific class) dependent terms in the objective function_
                * 17.2.1.1, 17.2.1.2, 8.1.1 | ML Methods | _Constrained Optimization_ (e.g., contraining that the classifier should have at least 90% recall entire dataset or just a slice) (TensorFlow Constrained Optimization (TFCO) is a great tool for this)
                * 8.1.1 | ML Methods | _MinDiff_ (Penalizes the model during training for differences in the distribution of scores between the two sets)
                * 8.1.1 | ML Methods | _Counterfactual Logit Pairing (CLP)_ (To ensure that a model's prediction does not change between "counterfactual pairs" (where the sensitive attribute referenced in a feature is different). For example, in a toxicity classifier, examples such as "I am a man" and "I am a lesbian" should not have a different prediction. Penalizes the model during training for output differences between counterfactual pairs of examples)
                * 17.2.1.1, 17.2.1.2, 8.1.1 | ML Methods | _Cost-sensitive learning (Confusion Matrix methods)_ (''The loss caused by instance x of class i will become the weighted average (expected value) of all possible classification errors of instance x'')
            * 17.2.1.1, 17.2.1.2, 8.1.1 | ML Methods | _Strong priors_
    
    * 2.1.2, 17.1, 4, 9 | ML methods | _Explainable/Interpretable ML_ (e.g explainng the model by playing with input and seeing how predictions change) The goal is to find human-understandable explanations that you can use to predict the model's behaviour in new inputs & understand features influences to develop trust in the model
        * 2.1.2, 4, 9 | ML Methods | _Input enrichment_ (giving semantic labels to inputs to improve interpertability over semantic space)
        * 17.1, 4 | ML methods | _Generalized Additive Model (GAM)_
        * 2.1.2, 17.1, 4, 9 | ML methods | _Explanation training_ (training the model with ground truth explanations of x --> y mapping to that it can learn to outut explanations_hat together with y_hat)
        * 2.1.2, 17.1, 4, 9 | ML methods | _Explanation monitoring_ (compare embeddings of ground truth explanation to explanation_hat)
        * 2.1.2, 17.1, 4, 9 | ML methods | _Symbolic Regression_
    
    * 1, 3, 5, 15.1.1.2, 10, 10.2.2, 13 | ML Methods & Software Engineering | _Resource Efficient ML_
        * 1.3, 3, 5, 15.1.1.2, 10, 10.2.2, 13 | ML Methods & Software Engineering | _Only Inference_
            * 1.3, 3, 5, 13 | ML Methods & Software Engineering | _Model compression_ (lower inference time and/or memory usage) (Includes _Knowledge Distillation_)
            * 1.3, 5, 15.1.1.2, 10, 10.2.2 | ML Methods | _Model Soup_ (substitute an ensemble for a model which weights are simply the avarage of the submodels of the previous ensemble)
            * 1.3 | ML Methods | _Effortfull Inference_ (receive as input the amount of effort it should allocate to a prediction, this will shape the corrected predictive power/efficiency tradeoff. More effort means the client is willing to accept waiting more time, but wants more accuracy; less effort means the client is not willing to wait, so accucary can b traded for a lower inference time) (e.g., This already could be done easily with Gradient Boosting Models)
        * 1.1 | ML Methods & Software Engineering | _Only Training_
            * 1.1 | ML Methods & Software Engineering | _Gradient checkpointing_ (instead of storing all activations to then cimpute the gradient, only store last activations (in the backwards pass) and compute next activations when neccessary. Ends up with more computation but requires way less memory)
            * 1.1 | ML Methods & Software Engineering | _Neural Differential Equations (NDEs)_ (eliminating necessity of storing activations, because they can be calculated analytically)
        * 1.1, 1.3, 5, 15.1.1.2 | ML methods & Software Engeneering | _Training & Inference_
            * 1.1, 1.3, 5, 15.1.1.2 | ML methods & Software Engeneering | _New Architectures_
                * 1.1, 1.3, 5, 15.1.1.2 | ML Methods & Software Engineering | _Attention-based: Pale-Shaped Self-Attention_ (making self-Attention more efficient)
                * 1.1, 1.3, 5, 15.1.1.2 | ML Methods & Software Engineering | _Convolution-based: Temporal Shift Module_ (making Spatio-Temporal Convolution more efficient)
            * 1.1, 1.3 | ML Methods | _Sampling Sub NN at Inference_
                * 1.1, 1.3 | ML Methods | _Using Dropout to build Sparse Model: remove weights_ (do inference with random dropout, check cv metrics (e.g., accuracy & inference time), if it is good keep it, and do genetic algorithm to find best dropout pattern wrt your cv metrics (e.g., accuracy & inference time). After some runs you  cna build a cv metric estimator with the data you got before, speeding up next cycles. Then use sparse computation techniques to immprove efficiency)
                * 1.1, 1.3 | ML Methods | _Using Ablation to build Simpler Model: remove features_ (do inference with random feature setting to 0, check cv metrics (e.g., accuracy & inference time), if it is good keep it, and do genetic algorithm to find best feature removal pattern wrt your cv metrics (e.g., accuracy & inference time). After some runs you  cna build a cv metric estimator with the data you got before, speeding up next cycles. Then remove the paths of these features from your model to compress it)

    * 17.1 | ML Methods | _Regularization Methods_
        17.1 | ML Methods | _Label Smoothing_
    
    * 8 | ML Methods | _Evaluate Acc on subsets of test data_ (_Note_: maybe incorporate deviation from even acc across slices of data in the objective function)
    
    * 20.1, 9.1.3, 17.1 | ML Methods | _Interactive ML: Learning from Feedback, RL & Bandits_
    
    * 8, 17.1, 4 | ML Methods | _Adressing Hidden Stratification in classes_ (in classification, some might have prediction sub-paths that are loosely coupled, so facilitating the model to decouple processing can make it converge to better places/faster)
    
    * 1.1, 17.1, 17.5 | ML methods | _Bayesian Optimization_
    
    * 4, 9, 17.1, 20.1 | ML Methods | _Program Synthesis_
    
    * 20.1 | ML Methods | _Randomization_ (DO pure exploration in the first n decisions. Take random decisions during this period to get iid data.)
    
    * 8 | ML Methods | _Train decision tree to predict loss & use leaves as clusters (subgroups where model has aprox equal predictive power)_
    
    * 17.9 | ML Methods | _Structured Prediction (including seq2seq models)_
    
    * 17.5.2 | ML Methods | _Geometric DL (including Graph NNs)_
    
    * 9.1.1.2 | ML Methods | _Open-set Recognition_
    
    * 16.2 | ML Methods | _Curriculum Learning_ (not giving examples randomly, giving them from lexx complex (easy) to more complex (difficult))
    
    * 16.2 | ML Methods | _Software Verification_ (proving a formal math specification of a program given the program)
    
* __All | ML Methods & Software Engineering | _System-related_ (scope: more than just building models with ready dataset)__
    * All | ML methods & Software Engineering | _MLSys & MLOps_ (Open source or cloud-provided)
    
    * 1.1, 1.3 | ML methods & Software Engineering | _Ditributed ML_
        * 1.1 | ML methods & Software Engineering | _Ditributed training_ (_Data Parellelism_ and _Model Parellelism_ are often employed together in huge models)
            * 1.1 | ML methods & Software Engineering | _Data Parellelism_ (when model fits into Memory) (splits batch into many machines and then: synchronous option: wait for all machines to send gradients, avarages gradients over subbatch gradients of each machine, and all machines do same batch parameter update; or asynchronous option: when some machine has its subgradient ready, already send to all machines do same subbatch parameter update originated from a specific machine) (includes Federated Learning)
            * 1.1 | ML methods & Software Engineering | _Model Parellelism_ (when model doesnt fit into Memory) (divide model into n parallel sequential processing steps (but then each machines will need information contained on other machines do their their computation (part of their input), and also part of their output they dont use, intead, they have to pass to other machines because it will be input to them. This is done efficiently via inter-processing communication frameworks like MPI))
        * 1.3 | ML methods & Software Engineering | _Ditributed inference_ (when model is too big for memory --> e.g using 2 micrcontrollers on the edge)
    
    * 1, 2.2, 3, 13, 8, 1.3, 4, 5 | ML Methods & Software Engineering | _Resource Efficient ML_
        * 1, 2.2, 3, 13 | ML methods & Software Engineering | _Specialized hardware_ (e.g.,  GPUs, TPUs, Graph-based PUs)
        * 1, 2.2, 3, 13 | _Hardware-Software Co-design_ (e.g., using FPGAs)
        * 1.3, 7, 13 | ML methods & Software Engineering | _Specialized compilers_ (Tools: (1) ML Compilers like: TVM, XLA, voltaML (lib wrapper over multiple compilers/runtimes), Adlik, AITemplate, Hidet; LLM-specific Compilers like: MLC LLM)
        * 1.3, 13 | ML methods & Software Engineering | _AutoScheduler_ (Discovering near-optimal path to execute computational graph (model) with ML)
        * 1.3, 7 | ML methods & Software Engineering | _Tiny ML_ (Ml on ultra-low-power edge devices (typically micrcontrollers))
        * 1.3 | ML methods & Software Engineering | _Inference by Query instead of Computation_
            * 1.3 | ML Methods & Software Engineering | _Conditional KNN_ (have e cache of predictions, when receiving new datapoint A for inference see if there are datapoints in the cache B which makes abs(B-A) < eps; if true: just interpolate the predictions; else: do normal inference)
            * 1.3 | ML Methods & Software Engineering | _Pre-materialized Predictions_ (compute all possible predictions ans store them in a DB, then ininference just query the DB of predictions)
    
    * 2.2.1.1, 2.2.1.2, 2.2.3, ML Methods & Software Engineeering | _Privacy Preserving ML_
        * 2.2.1.1 | ML Methods & Software Engineering | _Differential privacy_ (adding noise in a way that any specific datapoint cant be deduced)
        * 2.2.3 | ML Methods & Software Engineering | _Multi-Party Computing (MPC)_ (f(x1,x2, ..., xn) from n parties is done, without any party or a facilitator knowing the xi from another party)
        * 2.2.3 | ML Methods & Software Engineering | _Homomorphic Encryption_ (do computation with encrypted data and decrypt afterwards)
        * 2.2.3 | Software Engineering | _Smart Contracts (Blockchain)_
        * 2.2.1.2, 2.2.3 | Software Engineering | _Hardware Support for Privacy_ (Hardware Enclaves (Trusted Execution Environments (TEEs)) or Hardware Security Modules (HSMs) (data is only decryypted at the processor and/or in protected memory regions)
    
    * 1.1, 17.5.1, 17.5.2.2, 17.4, 17.3, 2.1.2, 4, 8, 9, 5, 16, 13, 15.2.2 | ML Methods & Software Engineering | _Data-Centric Methods_
        * 1.1, 17.5.1, 17.5.2.2, 17.4, 17.3, 2.1.2, 4, 8, 9, 5, 16, 13, 15.2.2 | ML Methods & Software Engineering | _Training_
            * 1.1, 17.5.1, 17.5.2.2, 17.4, 17.3, 2.1.2, 4, 8, 9 | ML Methods & Software Engineering | _Increase Supervision Signals_ (lets give more supervision signals instead of one label)
                * 1.1, 17.5.1, 17.5.2.2, 17.4, 17.3 | ML methods | _Weak Supervision_ (labels fabricated by labbeling functions (domain-expert's heuristics) that can be a function of domain knowledge and proxy label signal. Can even learn a meta model that receives as input <x,y_proxy> and outputs y, then <x,y> can be fed to the main model) (It is a great way of injecting domain knowledge as a prior, especially usefull for low data regions that we know should have more data)
                * 2.1.2, 4, 8, 9 | ML Methods | _Input enrichment_ (giving semantic labels to inputs to improve interpertability over semantic space. Especially usefull for non-tabular data or when data is very raw)
            * 1.1, 2.1.2, 5, 16, 13 | ML methods | _Active Learning_ (model asks for datapoints in regions he wants to improve estimation)
            * 9.1.3, 17.1 | ML Methods | _Retraining on new data_
        * 9 | ML Methods & Software Engineering | _Inference_
            * 9 | ML Methods | _Data Distribution Classifier_. (Trained to classify input data as training data or inference data (if it has corrected predictive power, evidence of training/serving input distr skew)
    
    * 2.1.1, 2.1.2.2, 2.2.1.1, 3 | ML Methods & Software Engineering | _Secure ML_
        * 2.1.1, 2.1.2.2, 2.2.1.1, 3 | Software Engineering & ML Methods | _Inteligent system vulnerability and attack attempts identifier_
        * 2.2.1.1 | ML methods & Software Engineering | _Requests limiters_ (Limiting number of requests to Inference Service /user or /time)
        * 2.2.1.1 | ML methods & Software Engineering | _Not giving model output probabilities to client_
    
    * 1, 2, 3, 4, 6, 7, 8, 9, 10, 10.2.2, 14, 15 | ML Methods & Software Engineering | _Tools_
        * 9, 10, 14, 10.2.2, 3, 15.2, 2.2.1.2 | Software Engineering | _DevOps/DevSecOps tools_ (Open source or cloud-provided)
        * 1, 2, 4, 6, 7, 8, 10.2.2, 14, 15.1 | ML methods & Software Engineering | _ML Frameworks_ (TF/TFX, Pytorch, JAX/FLAX, etc)
        * 9, 10, 10.2.2, 14, 3, 15.2 | ML methods & Software Engineering | _All-in-one cloud solutions (usally called managed solutions)_ for MLOps and DevOps (Google cloud, Sagemamaker etc)
    
    * 14 | ML methods & Software Engineering | _Green AI_ (worried about carbon footprint of large models' energy consumption)
    
    * 9.1.3 | ML methods & Software Engineering | _Online learning_ (1. repeat a hanfull of times: receives a datapoint, does inference; 2. Do parameter update on his last micro batch)
    
    * 1, 7, 10, 10.2.2, 13, 14.6, 15.1.2 | ML methods & Software Engineering | _AutoML_ (automating process of building ML systems. Includes Neural Architectural Search (NAS) and Learning Optimizers)
    
    * 1.3, 7.1.2, 7.1.5 | Software Engineering | _WebAssembly_ (runs faster than javaascript. Lower level language (like byte code) that browsers interpret now in additional to js. Can run heavy worloads like ML now in a resonable time)
    
    * 2.1.2, 4, 9, 17.5.3, 16.2, 9.1.3, 2.2.1.1 | ML methods | _Causal Inference (includes Data Collection & Causal Discovery)_(use causality priors/assumptions do then infer the causal effect of some variable on other & do counterfactual resoning)
    
    * 8.1 | ML Methods | _Gather personality/cultural test data__ (1. use this data for datapoints instead of protected class features; 2. or just gahter enough data for an experiment (in case you cant get this data or is unnecessarily computaionally more intensive), to formalize causal effect of feature on target and get rid of bias complaints)_

    * 9.1.3 | ML Methods | _Active Surrogate Estimators_ (a metho for monitoring reditive power when labels are scarce)
</details>

<details>
  <summary><b>Future</b></summary>

## :clock130: __Future__

### __Core ML progress drives ML Systems progress__

Progress in core ML research (Corrected predictive power focused) will only continue, leading to more and more complex models, training and inference strategies. These methods will need MLSys capabilities/efficiency and MLOps support to be implemented succesfully in production. Plus, the majority of ML nowadays is vanilla supervised learning, this is already not the case in Research. In Research more complex modelling is taking place and there is a lot of activity in poorly explored in production areas like: Self-Supervised ML, RL, Geometric ML, Kernel Methods, Causal Inference, Graphical Models, etc.

:fountain: _Research to Production Cascade_ is, roughly, like this:

1. __Research__ (Academia & Tech Giants) breakthrough
2. __Frameworks__ (Pytorch, TF, JAX/FLAX, etc) adopt method
3. __New production problems arise__
4. __MLOps tools are adapted/created to support method__
5. __ML Engineers use these tools to build production ML systems__

### __ML Adoption Trends__

_Notice the following trends:_

1. The # of ML-core (that need ML to survive) companies is increasing;
2. Non-ML-core companies are entering and/or relying more on ML;
3. Dataset sizes are increasing for aproximately all Companies.
4. Companies are adopting Generative AI which depends on very large models.

_These show, respectively, that:_

1. The market for ML Systems getting larger and small improvements in these Systems can have major ROI for these types of companies;
2. ML problems are getting higher priority across the board;
3. ML is getting more powerfull (think of better accuracy) inside these companies by default, thus getting greater importance.
4. ML problems are getting harder to solve due to increasing complexity on the ML side and performance & scalability requirements imposed by Generative AI.
</details>

<details>
  <summary><b>Path to AGI</b></summary>

## Path to AGI

With a good enough copilot, a recursive self-improvement feedback loop can be started where the Copilot indirectly improve's itself indefinetely:

1. Good enough ML Engineering Agent
2. Loop
    1. ML Engineer Agent is finetuned to build:
        1. ML Researcher Agent: develops ML research
        2. ML Revisor Agent: revises ML research developed by the ML Researcher Agent
        3. ML Tool Builder Agent: builds MLOps tools based on approved ML research done by the ML Researcher and revised by the Revisor Agent
    2. ML Engineering Agent is finetuned with new ML research and MLOps tools

</details>

<details>
  <summary><b>Major Challenges in building  <i>Freelunch</i></b></summary>

## __Major Challenges in building *Freelunch*__

Besides building the product, there are some other challenges involved that need to be solved:

1. __Adoption is tricky__

__Question:__ How to make companies/ml teams adopt our tool? We are not making a narrow plug-and-play tool that teams can try very easily without having to commit too much for it; we are making a tool in which teams have to change a lot the way they are doing things to adopt us, which becomes riskier for them. 

__Aswer:__ Having great docs with examples is a prerequisite. __The key differential I think can be a Youtube Channel with two playlists: (1) explaining MLOps visually; (2) building end-to-end MLOps platforms using *Freelunch*. These 2 types of contents would be very usefull and currently dont exist on Youtube.__ It's also important like: having a blog with relevant MLOPs content; making open MLOps learning material (whitepaper, book, course, etc); hosting and attending MLOps events; hosting and being part of hackathons and workshops; interacting with the community via all main channels (twitter, linkedin, slack/discord, hacker news, reddit, email, youtube, medium, dev community, zoom, instagram, product hunt, press); shameless posts. asking for the communities' feedback; podcasts; meet the team/office hours; engaging in discussions; posting memes; encouraging people starting with MLOps to gain practical expertise by contributing to *Freelunch* OSS.

2. __We don't suffer the problem (currently)__

__Question:__ We are making a product that helps building and improving MLOps platforms. How are we going to do this, if we dont have this problem ourselves? 

__Answer:__ We use Design Partners. Design Partners helps us shape our product providing feedback and in return they get guaranteed future discount on the paid offering.
</details>

<details>
  <summary><b>Strategy</b></summary>

## :chart_with_upwards_trend: __Strategy__

Things we need to solve in order to build this company.

* __Organize Team__
    1. Who does what?
    2. Bring in more Co-founders?
    3. Amount of work expectations, do we take some day off?
    4. Equity/Vesting paperwork done in the beggining
    5. How do we organize ourselves about milestones and communicate?
    6. Are we going to live together?
    7. Are we going to move to another place/country?
    8. When will we hire people?
* __Build product__
    1. __Define our Roadmap__: e.g., prototype in x days --> register company, domain, name and logo --> raise in y days --> get design partners in w days --> mvp in z days --> open source the mvp in zeta days --> define specific job positions and start hiring 
    2. __Protype and MVP concerns__
        1. Which are the milestones (with deadlines)?
        2. Which tech stack will we use?
        3. What will it have?
        4. How we evaluate sucess?
    3. __If Open Source__
        1. When will we Open Source our code?
        2. How much of it will be Open Sourced?
        3. We need to always be top contributors to the repo. Should we hire top contributors? How will the hiring process be?
        4. Which licence will we use? Confluent Community Licence seems like a good one, because it protects against other Clouds offering the same SaaS under their name and competing against our SaaS. AGPL is good because oblies people that you use the code to make it AGPL aswell.
* __Customer feedback__
    1. __Interviews__
        1. Which people we will interview? How Often? Remote Interviews?
        2. Which questions will we ask? Are they going to be recorded?
        3. Write hypothesis (expectations) of how we think people will say, to test our hypotheses
        4. How do we act on difference between expectations and what people said?
    2. __Design Partners__
        1. Design Partner Guides
            1. [Commonpaper](https://commonpaper.com/blog/design-partner/)
            2. [Andressen Horowitz](https://a16z.com/a-framework-for-finding-a-design-partner/)
            3. [F2VC](https://www.f2vc.com/blog-posts/the-design-partner-journey)
            4. [Entree Capital](https://entreecap.com/blog/11-tips-for-effective-design-partnerships)
            5. [Venture Inside](https://ventureinside.com/from-solving-your-problem-to-solving-our-problem-the-value-of-design-partners-6890b5483619)
            6. [Anecdotes](https://www.anecdotes.ai/post/mastering-design-partnerships)
            7. [Garuda](https://garuda.substack.com/p/design-partner)
        2. Design Partner Contracts 
            1. [Commonpaper](https://commonpaper.com/standards/design-partner-agreement/)
    3. __Production feedbacks__
        1. How can users contact us? Email? Whatsapp? Calling? In-app messaging?
        2. Is there going to be a product feature specific to get feedback?
* __Analytics: Metrics/KPIs__
    1. __Operations__
        1. How will we get, store, and visualize them?
        2. Will we use free tools like Google Analysitcs? open source tools? Custom solution? Paid ones?
    2. __What to track__
        1. Financial metrics: revenue plot, burn plot, runway & default dead/alive; MRR & net dollar retention plots; gross margin plot
        2. User metrics: active users plot, retention plot
        3. OSS-specific metrics: stars plot, forks plot, issues plot, downloads plot, PRs plot, contributors plot, PRs p/contributor plot, watchers plot, slack members plot, citations in other repos plot, traffic/doc visitors plot, company/external contributor ratio plot.
        4. Design partners
* __Aquire users/Sales/Marketing__
    1. How we talk about/pitch the Company for different types of people?
    2. Are we going to pay for Ads? At which point?
    3. How do we screen fast companies that will not close a deal?
    4. How to make first contact with potential customer? Cold email?
    5. Do we approach competitor users?
    6. Define an register company's name, web domain and visual identity (name, main logo, standard logo variations). Requiremts: easy to say and spell for (even for non-native english speakers), nothing relevant appears when you google it, no similar name in the ML space, avalaible <name>.ai domain. Preferences: name that can link to a fun logo.
    7. Will we have a blog?
* __Legal/Accounting__
    1. Incorporate the company. Will we hire a company for this? Will be incorporated in whihc country?
    2. Create bank account and credit card for the company
    3. Get an accountant. Which company will we hire to do the accounting?
    4. Register company name, logo, buy the web/http domain & email domain
* __Funding/Help__
    1. When will we try to get funding?
    2. From which people we want funding?
    3. How much equity will we give?
    4. At which valuation?
    5. How will we use the money?
    6. Apply to/Check Out: 
        1. Equity-free Accelerators: [Arch Giants](https://lootstrap.com/arch-grants), [Tech Founders](https://lootstrap.com/techfounders-equity-free-startup-accelerator), [Lighhouse Labs](https://lootstrap.com/lighthouse-labs-equity-free-startup-accelerator), [Fit 4 Start](https://lootstrap.com/fit-for-start-equity-free-startup-accelerator), [Merk Accelerator Darmstadt](https://lootstrap.com/merck-accelerator-darmstadt/), [Grand Central Tech](https://lootstrap.com/grand-central-tech/), [INiTS](https://lootstrap.com/inits-innovation-into-business/), [K-Startup Grand Challenge](https://lootstrap.com/k-startup-grand-challenge), [Google Launchpad Accelerator](https://lootstrap.com/google-launchpad-accelerator/), [Hot DesQ](https://lootstrap.com/hot-desq), [European Data Incubator](https://edincubator.eu/), [Spinlab](https://www.spinlab.co/), [Startup Hub](http://startuphub.pl/), [The Stable Fund](https://stable.fund/), [Catalyst NYC](https://futurelabs.nyc/programs/catalyst-nyc/), [Orange Fab](https://www.orangefab.com/for-startups/), [Grid 110](https://www.grid110.org/), [Cary Cofounders Lab](http://cofounderscapital.com/free-lab/), [Innovators Program](http://www.innovatorsprogram.co/), (Arch Grants Global Startup Competition)[https://archgrants.org/competition/], (MassChallenge Texas)[https://masschallenge.org/programs-texas], (The summer institute)[https://www.summerinst.com/], [Startup Boost Toronto](https://startupboost.org/toronto), [German Accelerator](https://www.germanaccelerator.com/).

        2. Equity-full Accelerators: [Y Combinator](https://www.ycombinator.com/), [AI2 Incubator](ai2incubator), [NVIDIA Inception](https://www.nvidia.com/en-us/startups/), [Next AI](https://www.nextcanada.com/next-ai/), [Microsoft for Startups Founders Hub](https://www.microsoft.com/en-us/startups?rtc=1), [Fdounders Factory Accelerator](https://foundersfactory.com/accelerator/), [Digital Catapult](https://www.digicatapult.org.uk/expertise/technologies/artificial-intelligence/), [The Hive](https://hivedata.com/), [AI Fund](https://aifund.ai/), [43North](https://lootstrap.com/43north)[Open AI Converge](https://openai.fund/news/introducing-converge) & [Stability AI's AI Grant](https://aigrant.org/), [Campus Founders - AI Founders](https://campusfounders.de/startups/aifounders/?utm_campaign=aif&utm_source=mlopscommunity&utm_medium=email), [AI Startup Incubator](https://www.suincubator.ai/), [Rockstart Accelerator](https://incubatorlist.com/rockstart-accelerator/), [Google for Startups Accelerator](https://incubatorlist.com/google-for-startups-accelerator-canada/), [Product 10x Build](https://incubatorlist.com/product10x-build/), [SKL.vc](https://incubatorlist.com/sklvc/), [Zero-to-one](https://incubatorlist.com/zero-to-one/), [Founders Factory](https://foundersfactory.com/), [Cubo Itaú](https://cubo.network/go/processo-selecao?hsLang=pt-br)
    7. Check out these:
        1. [Funding guide for OSS startups](https://jordansegall.substack.com/p/so-how-many-stars-is-enough-a-data)

</details>

<details>
  <summary><b>Resources</b></summary>

## :books: __Resources__

Some resource sources that you might find usefull, in the context of ML Systems and Startups.

1. __ML Systems__
    1. __ML Engineering__
        1. __My AI folder:__ (Go to "Computer Science & Engineering\CS - Applied CS\Artificial Intelligence (AI)\Machine Learning (ML)\ML (Child)\_ML Engineering")
        2. __Stanford Dawn__ (ML systems group where MLPerf was born)
        3. __RISE Lab__ (ML systems lab from Berkeley)

        4. __ML System benchmarking__ (Go to "Computer Science & Engineering\CS - Applied CS\Artificial Intelligence (AI)\Machine Learning (ML)\ML (Child)\_ML Engineering\Benchmarking ML Systems")
    2. __AutoML__
        1. __My AI folder:__ (Go to "Computer Science & Engineering\CS - Applied CS\Artificial Intelligence (AI)\Machine Learning (ML)\ML (Child)\Beyond Narrow ML\AutoML")
        2. __Catalyst Lab__ (CMU lab that studies automation of ML Systems)

2. __Startups__
    1. __My AI folder__ (Go to "\__Miscellaneous\MISC - Startups")
    2. __Y Combinator__ (Youtube channel mainly)

</details>

<details>
  <summary><b>Subideas</b></summary>

## :mailbox_with_mail: __Subideas__

These are ideas that can be embedded within *Freelunch*.

1. __Scientific ML Framework:__ [Uli](D:\Google%20Drive\Outroscomputadores\Meulaptop\AI2\__Miscellaneous\MISC%20-%20__My%20Stuff\My%20Ideas\Current\Non-profits\Projects\Developer%20Tools\AI\Non-diff%20ML%20Framework\Uli.html)

2. __Efficient Code AI:__ use AI to identify inneficient pieces of code & make it more efficient. One idea:

    1. Automatically __extract formal specification from tests__.
    2. __Detect inneficient parts of code__ (can learn a heuristic function for doing this search).
    3. __Remove inneficient parts of code__.
    4. __Translate code to Automatic Theorem Prover (ATP) (e.g., Lean) Code__.
    5. __Sample ATP Suggestions__ (will treat removed code as an subalgorithm (function) it has to find, for which its formal specification is inferred from the whole code's formal specification and the rest of the code. Note: if A is a potential solution function, A cannot be inside another possible solution B, because then B would be doing redundant computation, so ATP needs to avoid this types of sumb solution generation.)
    6. __Search for Optimal Suggestion:__ search within possible functions, the one with optimal (minimal) expected run-time
    7. __Compile from ATP Language to target language efficient code.__
        1. __With Hardware Acceleration__
            1. __Different hardware:__ source code to hardware-accelerator language (e.g.,  GPU (CUDA or OpenCL) and FPGA (VHDL or Verilog))
            2. __Same hardware:__ source code to hardware-levereger languages (e.g.,  OpenMP (parallelization by multi-processing) & OpenMPI (parallelization by multi-threading))
        2. __With No Hardware Acceleration__ (Just improving the code, using the same hardware & language)
</details>

<details>
  <summary><b>Attachments</b></summary>

## __Attachments__

1. __*Uli*: <tt>Uli.md</tt>__

    __Uli is the first declarative and kubernetes-native neurosymbolic programming framework__: learn entire software systems instead of just models; can be seen as an AI-native build system or general computational workflow optimization tool. You code how you think the system should behave, leaving out the pieces you don't know with hints; *Uli* takes care of learning these missing pieces leveraging your hints. Employs highly parallel and distributed implementations leveraging accelerators (e.g., GPU) when available, without the user having to know anything about parallel, concurrent and distributed programming. 

    __Makes AI programming feel like traditional programming, a.k.a. software 2.0.__ Think of *JAX* but on derivate-free, symbolic, probabilistic, AutoML/AutoControl, ILP/Program Synthesis and MLOps steroids. __Build architectures with "System 1" and "System 2" capabilities__. Important to highlight that __you can continue using your favourite libraries and APIs__ (including databases, ML/LLM frameworks, logic engines, math solvers, etc) within the codebase.

    Uli lies in between the fields of: Neurosymbolic Programming, Graphical Models, Automated Machine Learning & Control, Compilers/Decompilers, Optimization and MLOps.

    __In nutshell:__ *Uli* __(1)__ staticaly analyzes the codebase, __(2)__ converts into a computational format that specifies what is learnable, what is not-known environment, hints, etc; __(3)__ Makes optimizations to the compututational format on order to facilitate learning or acchieving some user-defined goal (e.g. interpretability); __(4)__ choses the most suitable learning algorithms; __(5)__ outputs a learned codebase + helper artifacts (e.g., model files, metrics, plots, traces, etc).

    If you want more control, you can always lower down to our low-level API that gives a lot of configuration options over how Uli works under the hood.

    *Uli* __can replace__ probabilistic programming languages, logic programming languages, logic-probabilisitc programming languages, differential programming frameworks, optimization libraries and model-system integration overhead. 

    *Uli* provides an easy way for encoding domain knowledge, being data efficient, leveraging your current codebase and integrating ml models with your overall system.

    __Core:__ Need to set the optimization goals. This can be done via multiple signals such as: setting scalar variables (objetive/loss) to minimize, reward to maximize, constraint to satisfy, text feedback to improve, text similarity with a baseline, image similiraty with a baseline

    __Examples of hints:__ variables thet rely on other variables to be already learned, variable default or prior, training data, testing data, places to fill with ml models (traditional ML or DL) along with requirements of certified guarantees (e.g., adversarial robustness), places to fill with (synthesized (from symbolic or NN dynamical system) or learned) control theory controllers and state estimators, places to fill with communication policies, places to fill with differential equations and their solutions (using ml model or symbolicly), places to fill with logic programs, places to fill with synthesized programs (+ provide DSL and regularization factor), places to fill with (synthesized or learned) logic engines (e.g., logical NNs), places to fill with equation programs (a program which solves an synthesized or learned implicit equation to arrive at its outputs) places of your code to subtitute with surrogate models, which are native random variables (which would be a rv even if its inputs werent rv's), variables (note: variables can be strings also) you are satisfied with a point estimate, causal relationships, equality/inequality constraints involving variables or objects (data strctures) (e.g., adversarial robustness contraint: if add noise delta_e to input, output can't change; an algebric or differential equaiton involving variables w and d; safety-rregion of the input where the output cannot be below certain threshold, etc), functions that can be relaxed to continuos form, annotations attached to python objects (can be natural language comments, properties, operation constraints (e.g., program synthesis that will produce a specific function needs to use a specific DSL), heuristic code and/or links to multimodal documents) on pieces to learn, quality requirement of the learned piece of code, will spend more computation/time on more important pieces, set of variables to do bayesian network strcture learning or causal discovery between them, learn a pytorch/tf/etc model independently with pytorch/tf/etc or use *Uli* to learn it together with the whole neurosymbolic system, control requirements for a controller to be learned, if a dynamical system to be learned can be assumed linear or not, shared environments between agents, if data is noisy or not, logic-probabilistic hints: whats a predicate and its associated probability, defining an object (data strcture of variables) as the argmax of a variable, places to do quantization, learning operators (e.g. surrogate models for differential equation solving), learning physical laws and differential equations, were you are storing RL episodes (list of (observation, action, communication, reward, feedback) tuples), noise of a a variables based a a function of its dependencies, target hardware to do a model2lib compilation, data to for models to unlearn, sensitive variables (e.g., features should be fair in the sense that they are not significantly informative of sensite variables), which functions define Markov Random Field (MRF) factors, differential provacy requirement, equation solving, symbolic parts to simplify first, etc.
        
    *Uli* lets you __build with ease complex AI architectures:__ e.g., non differentiable, multi-task, with multiple models, with memory, with loops, recursions, with embedded AutoML/AutoControl engines, with embedded numerical solvers, with embedded formal machinery (program synthesizers, theorem provers, logic/type theory engines, logic parsers (e.g., from natural language to FOL), ILP solvers, math solvers, program generalizers, program refactorers, library/concept learners, interactive specifiers/autoformalizers (e.g., from natural language to math problem specification or to an intermiadiate between natural language and logical statments; or from antural language to a heurisitc labelling function), planners, etc), solution templaters (e.g., natural language template to be filled with math solver output), pretrained models, prompt generators, hierarchical feedback loops, finetuning, etc

    *Uli* lets you __use any AI paradigm:__ e.g., logic (forward: predicates -> conclusion and inverse (satisfiability): conclusion -> predicates), supervised learning, finetuning, unsupervised learning, constrastive learning, self-supervised learning, reinforcement learning, graphical models, multi-agent, online learning, lifelong learning, interactive machine learning (e.g. RLHF), learning differential equations, learning agents, advanced agent paradigms (swarms, self-impriving agents, etc), advanced differential optimization (e.g., learning the optimizer, second-order methods, etc), learning to search, meta-learning, model routing, learned hierarchical reward functions, non-parametric learning, learning model guidance laws for sterability, constrained optimization, constraint programming, automatic differentiation, numerically estimating gradients, text "gradients", derivative-free optimization, discrete optimization, causal optimization, hierarchical/nested optimization, LLM-powered optimization, bayesian optimization, interactive program synthesis, control theory requirements, system identification, structured machine learning, advanced NNs (e.g., bayesian NNs, NNs with implicit layers, dynamic NNs, kolmogorov-arnold networks, graph NNs, gorup-equivariant NNs, cyclic NNs, interpolating NNs (where missing training data is seen as latent variables), physics-informed NNs, etc), logical rule learning/relational learning/automated knowledge base construction and resoning over it, automated conjucturing and theorem proving, premise/fact data-driven conflict resolution, hierarchical planning, causal discovery, causal inference, non-euclidean ML, human-in-the-loop, learning to cache/surrogate, learning ensembles, AutoML, learning to compress models, learning to decouple concepts in NNs (i.e., make latent varibales more meaningfull), fuzzy, annotated and differentiable logic, programatic labelling, spectral methods, active learning (error-driven, data-converage-driven, hypothesis-driven, etc), automated experiment design, embedding learning (hierarchical embeddings, graph embeddings, context-modulated embeddings, etc), gradient boosting, bagging, dynamic loss functions (e.g., decreasing local proxy loss importance as training goes), dynamic architecture (e.g., after cv loss goes down some treshold, substitute heuristic piece of code/logic for ml model or program sythesis with the piece of code/logic as a prior (e.g., via generating synthetic data or via a code/logic direct mapping to networks weights)), self-diagnosis/meta-cognition, probabilisitc-logic programming, advanced losses (soft formal/semantic losses, incentivizing multiple models to be independent via a penalty term in their loss functions, etc), mixture of experts/model routing, learning embeddings, learing to generate synthetic data, approximate reasoning, disentanglement learning, learning to name and describe semantic features (could be distributed among several neurons, need to identify the semantic fature vector in activaion space to see in which direction that if we change activations the semantic feature increases the most), learning semantic feature directions in activation space, neural predicates: knowledge bases with neural pecerption modules attached to facts (e.g., giraffe image detector attached to the giraffe concept), Hierarchical Control, Robust Learning, KR/Logic-specific Embedding Models, Logical Abduction, Abductive learning, Bootstrapping and Self-Play, Clustering, Soft Unifications, Learning from Logic Program Templates (learning right predicates and probabilities), probabilsiitc logic, fuzzy logic, probabilistic soft logic, certifiable learning, unlearning, weak supervision/(possible learned) data programming (emsembling for synthetic data generation: (1) 
    bayesian pretrain with small annotated dataset; (2) bayesian training without annotations with "y" as a latent variable and using parameter prior of previous step), model order reduction (same goal of knowledge distillation, but without the need to make data and train a model and with more focus on theoretical guarantees), active inheritance (filtering synthetic data via heuritics or critiq models), using small supervised data to evaluate learned latent variables, static analysis, Neural Architecture Search (NAS), aproximate inference, model merging, dimensionality reduction, Strctured Equation Models (SEMs), Certified/Verified Learning, Sampling Methods, LLM-as-Judge, Multi Task (e.g., multiple task heads, GANs, Generator-Verifier joint learning, etc), Learning Objetive Functions, etc. 

    __Support for:__ multiple programming languages (python,julia,cpp,rust,java,go,scala,javascript/typescript), multiple data formats (e.g., input-output pairs, human task flows, unstructured knowledge, unstructured feedback, knowledge base, causal experiments (interventions)), multiple data sources: (e.g., filesystem, memory, UI, your own code, synthetic data generator, samples from a posterior distribution, environment simulator, proxy reward function or supervised task, etc), ML libraries (e.g., Keras, Pytorch, Tensorflow, HuggingFace, Deep Speed, Lightning, scikit-learn, SciML, Ludwing, etc), ML model formats and runtimes (e.g., ONNX, GGML, pickle, coreML, pt, pb, etc) and libraries making use of binaries.

    __Under the hood:__

    1. *Uli* maps your system into a single program (and at the end maps back)
    2. Since in many cases you want to do multiple probabilisitc inferences besides you forward program pass, *Uli* lets you write virtual function calls (calls to functions that dont exist yet) that give you access to learne variables or their learned distributions/sampling from the distribution, given some other fixed dependencies (i.e., ~ P(W|V,S...) where W is the variable of interest and V,S are fixed dependencies). Virtual functions can also be for other things:
        (1) Causal inference: in this case the function is one the following forms: (1) causal effect: ~ P(W|do(V), do(S), T, U...); (2) counterfactual: ~ P(W_counterfactual|W, do(V), do(S), T, U...) or (3) conditional independence 1 if W|V given T, U ..., 0 else
        (2) Control: inpulse/step/custom response of a specific closed loop system, controllability/observability, etc
        (3) Verification: 
            (1) Variable-based: user provides a First-order-logic symbolic statement involving variables/objects (e.g., equantion, inequation, variable/objects can reach this value, etc) + maximum verification time and receives: (1) true, false + counterexample, unkown + similar statement that can be verified; and (2) proof (if formal verification was used) or experiment (if numerical methods were used), Note: statement can involve latent and observed variables.
            (2) Program-based:
                (1) Complexity: user can query the computational and communication complexity of the program or specific function.
                (2) Resource Usage: user can query memory/disk/network/cpu usage of a program or specific function.
        (4) Differentiation: f'(.) of any function f(.)
        (5) Object computation graph: enables runtime access of a the computation graph that makes a object. The computation graph also can include causal arrows.
        (6) Dynamic DSLs
            (1) Object/function/class creation: create these at runtime and attach them to a DSL.
            (2) Get all info about a DSL on runtime and iterative over the DSL's members
        (7) Analyses
            (1) Estimate of optimal size of dataset given a model and vice-versa (think of generalization error bound using VC dimenison, but an approximate version for when its not possible to analytically calculate)
            (2) Parameter Identifiability/Observability Analysis for hypothetical measurement paradigms
            (3) Parameter Uncertainty Estimation (parameters can get uncertain due to: bayesian inference, truncation errors in solvers, numerical errors, limited data + probabilistic process or stochasticity of optimizers. The Parameter Uncertainty cna be useful for: tuning and sensitivity analysis)
        (8) Estimate metric m vs size s (e.g., predictive power vs model size with fixed dataset) plot and use the plot to return estimates for large-scale values
        (9) NNs extracted symbolic knowledge: use access an iterative over symbolic expressions or logic predicates extracted from the NN
        (10) Inference APIs for all (or just selected ones) the NNs: you can use inference APIs for all the NNs that you used in the codebase
        (11) Access to freezed parts of learned models as functions (e.g., for transfer learning or custom verification purposes)
        (12) Model interpretability: 
            (1) Measures of interpretability of a specific ml model.
            (2) Automated interpretatability (e.g., using LLMs)
        (13) Model Evaluation: evaluation metrics of a specific ml model
        (14) Custom Futures: you can set the output of a function to be a future. In this way you get a virtual function providing this output for you to use downstream. Of course, its actual implementation will have to wait for the future to be ready (Ray and async programming in general do this).
        (15) Model order reduction
    3. Gives you an UI where you can see the data coverage of your variables (i.e., how many dataoints used to learn each variable)
    4. Interactively tries to make your program more optimization/learning-friendly, intepretation-friendly or GPU-friendly when possible (e.g., (1) find aprox. equivalent differentiable (equiv. to non-differentiable) or find pieces of your program that resemble known AI techniques (e.g., transforming a code piece that resembles an HMM into an HMM, using SAT/SMT for a constraint stasfaction part of the program, etc); (2) extraction: symbolic regression (find aprox. equivalent symbolic (equiv. to ML models) pieces), logic extraction (extracing facts and rules within some logic system) or decision-tree extraction; (3) substituting non-GPU libraries by equivalent GPU libraries; (4) relaxing dicrete optimization parts to continous and differentiable aprox. equivalents (e.g., program piece NN to focus on searching at another program piece); (5) aproximating non-convex optimization functions to convex ones; (6) making bayesian pieces tractable (e.g., using conjugate priors or tractable probabilisitc circuit for a certian query class); (7) Relax ILP to Differentiable/Neural ILP; (8) relax a piece of code to a NN with a similar strcture and get the NNs parameters either via a direct code->NN equivalence mapping get the NN parameters, or via training the NN on synthetic data generated by the code. Then extend the netowrks with initalized parameters to enable learning differnt program strctures; (9) relaxing dicrete program synthesis to continuos/differentiable program synthesis); (10) re-parameterization of variables unders contraints in terms of unconstrained values (e.g., Energy-based models and softmax models); (11) make slight modifications in order to take advantage of efficient algorithms (e.g., pseudolikelihood instead of likelihood); (12) relax discrete variables to continuos ones; (13) adding gradient shortcuts (to avoid gradient vanishing) and normalizing parameters (to avoid gradient explosion).
    5. Chooses the best optimization/Serch/Abudction strategies leveraging your priors, data and hints, and also foundation models to guide search (in the case of missing pieces that need to be symbolic programs)
    6. Uses constrained optimization and other methods to handle imposed constraints (expressed as programs or equations), and uses regularization to avoid overfitting (penalizing large programs in case of program synthesis or parameter vectors in the case of ml models)
    7. Does Program Synthesis, Automated Machine Learning (learning model architectures and parameters from data), Automated Control + Automated Game Theory where needed (leraning controllers, state estimators and communication polcies from symbolic programs of the dynamical systems, observers, state objectives (setpoint or state sequence) or reward function), Equation and SMT solving where necessary
    8. If interactive approaches are used, user needs to be part of the learning process (r.g., RLHF or Interactive Synthesis)
    9. Uses advanced distributed optimization/learning algorithms over a ray cluster.
    10. Used best aproximate inference strategies for building your virtual functions
    11. Uses a VLLM to name latent variables:
        1. Discover which variables are (probably) actually generalizable and which as supirious dependencies by looking at their activations for right and wrong predictions (more likely to be suprious dependency if the nn relies on it a lot and the prediction is wrong)
        2. Get names for variables that are actually generalizable
            1. Feed armin and argmax observed input and ask for difference, then parse output into a name
            2. 2 options for higher-level latent variables:
                1. Do the same thing
                2. Feed its actual inputs (other latent variables), wich are already named, to an LLM, translating it into code and asking for the LLM to name the latent variables; then parse the output into a name
    12. Multi-step build:
        1. General builds: 
            1. Build 1: builds to be tested, observable and easily editable (e.g., provides a visualization of the data flow of the system, where its diffferentiable and where its not etc; run tests locally (*Uli* sets up the deployment on a virtual kubernetes cluster) to see if there aren't runtime errors; builds decision tree to code where engineer can see and edit what the model is doing. Of course large models arent built to code, code for interpreting/observing/editing them is compiled.).
            2. Build 2: builds to be executable and maintainable (e.g., decision tree in onnx format, where it can be easily loaded and update with new data).
                1. Build executables and/or libraries
                2. Build model files + runtimes (aupport for JIT) + some functions pre-compiled (optional)
        2. Custom Builds: lightweight builds for isolated parts of the system, e.g., building virtual function for data/model size estimation, etc
    13. Maps the learned single program back to your system

    __The output will be:__ a codebase with posterior/optimized system parameters, posterior/optimized model hyperparameters, posterior/optimized parameters and posterior/conditional/optimized latent variables (given a set of observed variables it is associated with) learned from data. Uli also allows you to interpret (learned or synthesized) variables (via visuzlization of the inputs that maximize it) so you can build more symbolic code around it. And if you retrain your models or resynthesize your code, Uli guarantees that you that the latent variable that you learned wont dissapear (this is done by making a virtual latent variable which is a function of all new latent variables. This funtion is simple weighted combination and is trained using synthetic data from the old model where the latent variable worked well (assumes latent variable already is monosemantic and has semantic meaniing discovered). Whynot just copy the latent variable function from the old model? Because it doesnt allow us to take dvantage of the new improved version of the latent variable.)

    __Uli also offers a GUI__ that lets you see (in real time) the system architecture, the learnable pieces, the leaarning process, automatic actions done by Uli on top of it, etc; side-by-side with your code. Any change in code, instantly updates the GUI.

    __Uli also offers a VSCode Linter/Formatter/syntax highlighter__ to help you code and avoid problems at compile-time.

    Uli also supports ML model compression, calibration, interpretation and evaluation.

    Uli has a has a plugin ecossystem for optimizers. People can easily add their own custom optimizers to power *Uli*'s backend.

    __Related work:__

    - Conferences, Labs and Grants: [NucLeaR](https://nuclear-workshop.github.io/aaai2024/), [Neurosymbolic AI for Science and Code](https://www.neurosymbolic.org/)

    - Papers: [Neurosymbolic AI: the third wave](https://arxiv.org/pdf/2012.05876), [DreamCoder](https://arxiv.org/abs/2006.08381), [Stich](https://arxiv.org/pdf/2211.16605), [Deep Implicit Layers](https://implicit-layers-tutorial.org/), [Globally Robust NNs](https://arxiv.org/pdf/2102.08452), [Learning Differentiable Programs with Admissable Neural Heuristics](https://arxiv.org/pdf/2007.12101), [Differentiable Learning od Logic Rules for Knowledge Base Reasoning](https://arxiv.org/pdf/1702.08367), [Learning Explanatory Rules from Noisy Data](https://arxiv.org/pdf/1711.04574), [A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms](https://arxiv.org/pdf/1901.10912), [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://arxiv.org/pdf/2309.08600), [End-to-end Differentiable proving](https://papers.nips.cc/paper_files/paper/2017/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf), [Lean Dojo](https://leandojo.org/), [Learning Symbolic Rules for Reasoning in Quasi-Natural Language](https://arxiv.org/pdf/2111.12038), [Drivinf Formal Theorem Provers with Informal Proofs](https://arxiv.org/pdf/2210.12283), [A Transformer-based Approach to Premise Selection](https://arxiv.org/pdf/2303.04488), [Lifelong Learning as Program Synthesis](https://arxiv.org/pdf/1804.00218), [Bridging Machine Learning and Logical Reasoning by Abductive Learning](https://proceedings.neurips.cc/paper_files/paper/2019/file/9c19a2aa1d84e04b0bd4bc888792bd1e-Paper.pdf), [DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning](https://arxiv.org/pdf/2108.12370), [Differentiable Synthesis of Program Architectures](https://proceedings.neurips.cc/paper_files/paper/2021/file/5c5a93a042235058b1ef7b0ac1e11b67-Paper.pdf), [Neural Architecture Search without Training](https://arxiv.org/pdf/2006.04647), [Quantitative Testing with Concept Activation Vectors(TCAV)](https://arxiv.org/abs/1711.11279), [Neurosymbolic Reinforcement Learning with Formally Verified Exploration](https://gavlegoat.github.io/papers/revel-neurips2020.pdf), [Logic-LM](https://arxiv.org/abs/2305.12295), [Automated machine learning: past, present and future](file:///C:/Downloads/s10462-024-10726-1.pdf), [Deep Implicit Layers](http://implicit-layers-tutorial.org/), [symbolicAI](https://arxiv.org/pdf/2402.00854), [Scalable Nested Optimization for Deep Learning](https://arxiv.org/pdf/2407.01526)

    - Tools [lbjava](https://github.com/CogComp/lbjava), [saul](https://github.com/CogComp/saul), [DomiKnowS](https://github.com/HLR/DomiKnowS), [ABLkit](https://github.com/AbductiveLearning/ABLkit), [Trace](https://microsoft.github.io/Trace/#top), [PiShield](https://github.com/mihaela-stoian/PiShield), [Lean Interactive Theorem Prover](https://lean-lang.org/), [cvc5](https://github.com/cvc5/cvc5), [Gradient-Free-Optimizers](https://github.com/BrunoScaglione/Gradient-Free-Optimizers), [NLopt](https://github.com/stevengj/nlopt), [DSPy](https://github.com/stanfordnlp/dspy), [textgrad](https://github.com/zou-group/textgrad), [JAX](https://github.com/google/jax), [Ludwig](https://github.com/ludwig-ai/ludwig), [Verified Software Toolchain](https://github.com/PrincetonUniversity/VST), [alpha-beta-CROWN](https://github.com/Verified-Intelligence/alpha-beta-CROWN), [Lyapunov_Stable_NN_Controllers](https://github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers), [pysmt](https://github.com/pysmt/pysmt), [fastsmt](https://github.com/eth-sri/fastsmt), [dPASP](https://github.com/kamel-usp/dpasp), [problog](https://github.com/ML-KULeuven/problog), [pgmpy](https://github.com/pgmpy/pgmpy)
    [dowhy](https://github.com/py-why/dowhy), [pyro](https://github.com/pyro-ppl/pyro), [Stich](https://github.com/mlb2251/stitch), [DeepProbLog](https://github.com/ML-KULeuven/deepproblog), [deepstochlog](https://github.com/BrunoScaglione/deepstochlog), [difflogic](https://github.com/Felix-Petersen/difflogic), [NeurASP](https://github.com/azreasoners/NeurASP), [nlprolog](https://github.com/leonweber/nlprolog), [Popper](https://github.com/logic-and-learning-lab/Popper), [prose](https://github.com/microsoft/prose), [egg](https://github.com/egraphs-good/egg), [egglog](https://github.com/egraphs-good/egglog), [Lean Copilot](https://github.com/BrunoScaglione/LeanCopilot), [ReProver](https://github.com/lean-dojo/ReProver), [LeanDojo](https://github.com/lean-dojo/LeanDojo), [MetaQNL](https://github.com/princeton-vl/MetaQNL), [psl](https://github.com/linqs/psl), [SATNet](https://github.com/locuslab/SATNet), [Fonduer](https://github.com/HazyResearch/fonduer), [Logic-LM](https://github.com/teacherpeterpan/Logic-LLM/tree/main), [AutoRAG](https://github.com/Marker-Inc-Korea/AutoRAG), [autokeras](https://github.com/keras-team/autokeras), [NNI](https://github.com/microsoft/nni), [tpot](https://github.com/EpistasisLab/tpot), [autogluon](https://github.com/autogluon/autogluon), [Snorkel](https://github.com/snorkel-team/snorkel), [gpt-prompt-engineer](https://github.com/mshumer/gpt-prompt-engineer), [symbolicAI](https://github.com/ExtensityAI/symbolicai)

    __Possible applications/use cases:__

    1. __Scientific ML (SciML):__ ML for Science (e.g. Alphafold) and Scince 4 ML (e.g., using ML to solve differential eqautions), making use of scientific modelling techniques (e.g., differential equations)
    2. __Knowledge-enhanced Models:__ architectures with domain-guided structure (e.g. Retrieval-Augmented LLMs or LLMs that use Knowledge Graphs)
    3. __Orchestration of ML models and traditional code:__ learn programs that are actually a blend of multiple ML models and traditional code pieces
    4. __Building custom ML models that dont fit into a common ML task:__ sometimes your input -> output mapping is not a common one.
    5. __Building towards AGI:__ complex architectures (e.g., loops and non-dfferentiable operators) that require advanced training techniques beyond good old gradient descent.
    6. __Working with less common data modalities:__ e.g., DNA data with Foundation Models 
    7. __Model Editing:__ fixing specific problems models are showing by making adjustments to the model architecture itself along with some training; instead
    of the typical approach of gathering a lot more data for the cases the model is not handling well.
    8. __Multi-Agent Learning:__ learn multiple agents as a team via fine-tuning and prompt tuning of programs containing llm calls and tool usage to minimize
    9. __Causal ML:__ ML working within a Causal Inference Framework
    10. __Supercharging codebases:__ substituting some software 1.0 parts for software 2.0 parts
    11. __Model-based Reinforcement Learning (RL):__ build model-based controllers mixing domain knowledge, control theory and RL; establishing controller priors and inductive biases before learning and composing hiererachical control loops. Integrates with multiple simulated RL environments, and lets you build a plugin for your own simulated environment.
    12. __Distributed AutoML__ easiest distributed training via AutoML that does automatic dataset construction and model building from raw data + heuristic causal program.
    13. __Incorporating formal constraints directly in training as part of the optimization__ invariants (the parallel in software engineering is property testing) are an establishes relationship betwenn output A and output B for different inputs A and B, that are problem-specific and dont require knowing output A neihter output B (aka dont require labelled training data). This can be done as hard (constraint programming) or soft constraints (semantic loss).
    14. __Mechanism Design__ coming up with dynamical systems and reward functions that make agents take learn policies that maximize a mechanism reward function.
    15. __Learned Formal Machinery: SAT/SMT Solvers, ILP Solvers, Theorem Provers, Program Synthesizers, Equation Solvers, Knowledge Graph Builders, Formal Parsers, Planners, etc__ can use ml to learn better heuristics or represenations that guide these formal tools.
    16. __Using Program Synthesis as fallback for when the ML model is not shure__
    17. __Reverse Engineering Systems__ because you know the underlying process is not messy (variables are meaningfull, modular, discrete)
    18. __Avoid Compounding Errors of model chaining__ when chaining independently learned models, errors compound. But learning/finetuning them together enables us to tame the componding factor.
    19. __Higher-level ML Libraries built on top of it__ e.g., domain-specific libraries and libraries that make it easier to get things working.
    20. __Provably X (e.g. Safe) ML Models__ certified learning and ml model verification to have proven guarantees regarding behaviours of interest.
    21. __ML Model Intepretability__ ability to disantangle, describe and build on top of learned latent feature + extracting symbolic expressions from the model.

2. __*AgentPool Project*: <tt>LeanEngineering.md</tt>__

    # __*AgentPool*: non-profit AI for Engineering__

    ## :trollface: __Authors__  

    * __Name:__ Bruno Scaglione
    * __Contact:__ bruno.c.scaglione@usp.br

    ## :8ball: __Overview__

    __*AgentPool* is an open source project idea that aims to build ENgineering Foundation Models via a heavy data-centric approach. We aim to formalize and strcture engineering knowledge to make it easier to LLMs digest. This approach has showed to be succesfull in the "Textbooks is all you need" paper.__
    
    A formal database of engineering methods that is way better than raw data from the internet, because it provides consistent structure that guides engineering work out-of-the-box. Normal LLMs have to infer this structure from the data which is pretty hard (because of incosistent materials and lack of connection between related topics), that's one reson why they end up not being so good at science/enginering for example.

    Wait a minute, but isn't building such a database kind of intractable? Yes, that's why we just kick-start by making a large enough dataset containing core examples, structure it must follow and examples on how to learn from database feedback, so that we train an LLM Agent to do it for us. The working will be as follows:

    **Input:**

        <
            bootstrap dataset: large enough dataset containing core examples (raw data --> strctured DB data mapping), structure it must follow and examples on how to learn from database feedback,
            pre-trained LLM,
            benchmark of engineering tasks
        >

    **Processing:**

    1. LLM agent A will then add more things to the database
    2. Another LLM B will train on the new database
    3. B will be evaluated on engineering tasks
    4. If B gets better: A := B
    5. Else: 
        1. Generates multiple candidate database entries
        2. If human engineers find that one is good: A is fine-tuned on it
        3. Else:
            1. Human Engineers do the work
            2. A gets fine-tuned with human-made data
    6. Repeat.

    Remark: the database entry can only be added to the DB if it follows a certain format. If the format is not correct, the DB will return an error with feedback. The LLM then needs to correct its output, using feedback information, until it can be added to the DB.

    **Output:**

        <
            structured db of engineering knowledge,
            llm that can do engineering (by being trained on strctured engineering data and being able to the db as a knowledge-base at inference time)
        >

    ## __Additional Details__

    This project is inspired by the [*Lean Theorem Prover*](https://leanprover.github)<sup>1</sup>, and makes us of the math database it provides. 

    The core of the project: a library of _Knowledge Cards_. _Knowledge Cards_ would be all that surrounds understanding & implementing a method. Specifically, they would be a wrapper around:

    1. Miscelleneous natural language & image/video explanations, 
    2. Specification of when the method is most powerfull
    3. Formulas (along with units, explanations (in natural language) for symbols & the overall use of the formula)
    4. Proofs
    5. Algorithms (hand-coded or ML models)
    6. Examples
    7. Data, 
    8. Config,
    9. Dependencies (definitions, theorems, formulas, algorithms, tools, libraries & entire _Knowledge Cards_), 10. "is dependency for" (downstream _Knowledge Cards_)
    11. Performance benchmarking (performance rating (e.g., avg running time for some environment, memory usage), debuggability, scalability, etc); 
    12. Evaluation specification: evaluation algorithm, metric & threshold that verifies that the _Knowledge Cards_ is producing correct output given valid input (can be formal analytical verifications or empirical measurements).

    All of this in a formal sequential format, that aims to achieve some goal/output, given some inputs. Each _Knowledge Card_ has its inputs & outputs formally specified; these cards can be seen as a generalized algorithm (because it can contain explicit computing steps (algorithms) but also implicit/data steps (natural language & images/video)). _Knowledge Cards_ can be embedded within other _Knowledge Cards_, within a pipeline (high level computational graph), to achieve higher-level goals.

    ### __Categories of methods & supporting knowledge__

    #### Method categories

    1. __Supporting method__
        1. __Algorithmic: pure Math/Computer Science:__ is decoupled from the world, can be verified with math specifications (e.g., format conversion, operations, etc) and its goal is to solve formal mathematical problems analytically or numerically.
        2. __Scientific:__ attached to natural real-world processes & measurements (they have units) (e.g., Physics, Chemistry, Biology) and its goal is to provide dynamical system simulators (makes use of algorithmic methods).
    2. __Engineering method:__ attached to any real-world processes & measurements and its goal is to solve high-level problems, by formalizing these problems into suited forms and leveraging supporting methods.

    #### Embedded knowledge categories

    1. __Mathematical:__ definitions, theorems, proofs.
    2. __Scientific:__ scientific models of the universe/world.

    ### __Use cases__

    1. __Engineering Practice:__ solving engineering problems. _Input_: problems description in pdf format. _Output_: solution in PDF format AND/OR software repo with an implementation.
    2. __Engineering Education:__ navigating & visualizing tree of concepts. Can look at pre-requisite concepts that are needed to understand some high-level concept. Can also see how different fields interplay.
    3. __Research:__ helps interdisciplinary research by showing relationship between fields & State-Of-The-Art for a given task (theoretically by looking at the theory and/or empirically by doing __Engineering Practice__)
    4. __Building Open Source Software:__ automatically synthesize & update engineering software tools.

    ## :tractor: __Hypothetical *Lean-engeneering-powered* AI System Architecture__

    ### __1. Database of Knowledge Cards__

    Main classes of _Knowledge Cards_:

    1. __Non-application specific__
        1. __Core methods:__ implements methods that solve problems.
    2. __Application specific__
        1. __UI:__ deals with processing user input & rendering to screen, to give the user a very pleasent experience.
        2. __Infrastructure:__ everything that is not Core methods or UI, but is necessary for the application.

    ### __2. Configuration Manager__

    UI that receives user preferences such as:

    1. Code quality preferences (e.g., linting, design patterns & types of tests)
    2. Tools preferences

    It also helps users with chosing preferences by:

    1. Showing examples of programs generated from different configurations
    2. Giving default configurations for different types of teams/projects
    3. Showing popular configurations

    ### __3. AI-powered steps__

    #### __1. Interactive Specification Builder__

    An application that interacts with the user with the goal of producing formal specifications<sup>2</sup> for problems. The user gives as input an informal specification (e.g., text), then the Interactive Specification Builder returns a formal specification for it and if it is incomplete or complete; the user makes adjustments until the specification becomes complete & the user is satisfied.

    #### __2. Knowledge Card Builder__

    From a formal specification (output of __Interactive Specification Builder__), it builds a custom high level _Knowledge Card_, making use of existing _Knowledge Cards_.

    #### __3. High-level Compiler__

    An Optimizer maps a high level _Knowledge Card_ to an _Implementation Card_ which is closer to a repo. _Implementation Cards_ are hardware/OS-agnostic optimized and lower-level descriptions of _Knowledge Cards_. They specify implementation details such as architecture, parallelization procedures, variable names, tools preferences & code quality preferences (e.g., linting, design patterns & types of tests). They have the liberty to merge & separate algorithms to build more optimized modules.

    #### __4. Low-level Compiler__

    _Implementation Cards_ are mapped to repos with a Compiler. Here, hardware/OS-aware optimizations occur.

    #### __5. If Succesfull: adds Knowledge Card (the one it just built) to the DB__

    ## :calendar: __Why Now?__

    Some important factors:

    1. __Machine Learning (ML) Foundation Models.__ Major success with ML Foundation Models such as GPT-4 & Stable Diffusion are laying the foundations for the use of natural language & vision as general interfaces for accomplishing many different tasks. These Models are already powerfull on their own (e.g., Alphacode can do some pretty nice code synthesis for simple tasks), now imagine fueling them with all engineering knowledge.

    2. __State of Automated Reasoning.__ [*Lean Theorem Prover*](https://leanprover.github) is the major success story & shows advanced reasoning capabilities, being capable of proving various complex theorems & formally (& interactively) verify programs.

    ## _Footnotes_

    1. [*Lean Theorem Prover*](https://leanprover.github): a proof assistant powered by its own digital library of math theorems, implemented as a programming language (Lean).

    2. Formal Specification: a specification which contains all information necessary & is unambiguous. In this context, it must be built by succesvie interactions of the specification builder with the user. Because the user can be assumed to not know how to specify the problem but can be assumed to know how to verify a specification.
</details>

<details>
  <summary><b>Personal Stuff</b></summary>

## :pencil: __Personal Stuff__

### __Notes__

>1. Kafka and spark/koalas also can be containarized (e.g. using Docker/Podman/Contaierd/runc (+ Processor Acceleration  (e.g., NVIDIA Container Toolkit))) and deployed in k8s cluster
>2. Ansible, Terraform/Terragrunt/terramate or Pulumi, Crossplane can basically install anything in remote machines (from VMs, VPNs and Load Balancers to interpreters, compilers and higher level software)
>3. Distributed Data Mesh seems to becoming a trend to substitute Monolith Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb)
>4. Webservers (Serving FrontEnd) need to implement SSL/mTLS and register a DNS. If reverse proxies are in place, this is offloaded from webserver to them
>5. All micro-services need a service discovery method (pure (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) networking and k8s already solve this for you with internal Service registries type things)
(ohter options are to use yourself Service Registries, and if services are not in a cluster (usually for simple 1 backend server and 1 webserver applications), get DNS)
>6. FrontEnd sits inside browser code. Tells to browser want it wants to do (based on browser Rendering language (HTML), Programming Laguage it understand (JS)
and APIs (accesed direclty through JS)) and the browser does it.
>7. Server-side-rendering (SSR) (Tools: Nextjs) is being used a lot because facilitate indexing (all initial HTML is ready) and delivers the whole page
(but still has to run JS code to be interactive) content faster (because making
requests from within cluster is faster than requests travelling fro cleint to cluster; to construct the HTML page)
>8. CORS needs to be enabled in application servers (So that FrontEnd hosted in other machines can have their requests accepted)
>9. Apache Beam is a wrapper for data processing engines: wraps Apache spark/koalas, Storm, Flink and other processing backends in a higher level API that unifes batch and strem processing. Alternatives to beam: Pathway
>10. Cloud environments: like dev, staging, prod; dont need need to be on separate clusters. K8s offers namespaces to separate environments in a single cluster.
The possibility that an dev application crashes the node and consequently crashes prod application is also adressed by defining specific nodes to each environment.
>11. K8s is commonly used with multiple master nodes per cluster to guarantee high availability. Then usually a Load Balancer directs external traffic (kubectl & clients) to master nodes (master node api's) and also directs traffic from worker nodes to master nodes.
>12. A single Kafka cluster can be used for multiple stages of the pipeline. Each stage is a topic. Consumers can also be producers.
>13. All models operating on dynamic world, should be treted as time-series models.
>14. Dynamic DNS is used a lot as a form of load balancing.
>15. Telemetry == information that captures the state of an ML application (logs and Monitoring metrics)
>16. Observability is Fine-grained Monitoring with the objective of debugging the system.  (''observability makes an assumption stronger than traditional monitoring: that the internal states of a system can be inferred from knowledge of its external outputs. When something goes wrong with an observable system, we should be able to figure out what went wrong by looking at the systems logs and metrics without having to ship new code to the system. Monitoring centers around metrics, and metrics are usually aggregated. Observability allows more fine-grain metrics, so that you can know not only when a models performance degrades but also for what types of inputs or what subgroups of users or over what period of time the model degrades. For example, you should be able to query your logs for the answers to the questions like: ''show me all the users for which model A returned wrong predictions over the last hour, grouped by their zip codes'' or ''show me the outliers/anomalies requests in the last 10 minutes or ''show me all the intermediate outputs of this input through the system''. To achieve this, you need to have logged your systems outputs using tags and other identifying keywords to allow these outputs to later be sliced and diced along different dimensions of your data.'')
>17. Monitoring (ML system) > Observability (ML system) > Interpretability (ML Model)
>18. If you train your model on a huge dataset, you can avoid edge cases and input/output drifts, but this is just theoretical. In practice it is unfeasable, therefore you always need to deal with cases your model rarely or never seen before
>29. Examples of vanilla predictive acc problems to deal with, that are not studied much in academia:
    * 29.1 Deal with data problems that affect acc:
        * 29.1.1 imbalanced data (has way more of one label than other)
        * 29.1.2 small data
        * 29.1.3 missing data
        * 29.1.4 noisy data
    * 29.2 Anomaly detection problems
>20. Streamlit, Gradio and others make it super easy (pure python) to built demo frontends (they build the html file from python specification & run a webserver for you)
>21. In some situations, define parameters for label collection (e.g.,  for recommendation system, time window for label collection. ''After a certain time window, if there is no click, the label is presumed to be negative. Choosing the right window length requires thorough consideration, as it involves the speed and accuracy tradeoff. A short window length means that you can capture labels faster, which allows you to use these labels for monitoring and continual learning. However, a short window length also means that you might prematurely label an item as no click before its being clicked on.'').
>22. RPC seems better than REST 4 inter-service communication (takes advantage of HTTP 2 to be faster and makes you feel like you are simply calling a function which is just executing smewhere else. Official docs is great). Note: RPC uses Protocol Buffers under the hood which gives you a language-agnostic human-readable format f and conversion to language implementations that serialize (putting information into a standard format (can be a human-readable and not efficient format (e.g., JSON) or a non-human readable and efficient format (e.g., Parquet)) that can be easily parsed.
>23. Data privacy laws might forbid you to store data (Business Data Lake (tools: managed solution or based on open source distributed noSQL DB like cassandra or scilladb), ML Data whareshouse , Feature Store, or anything that stores user data in a DB)
outside the country (keep this on mind when selecting clusters with your cloud IaaS)
>24. MLFlow is awesome (and open source): can do a lot of the MLOPs functionalities like Experiment Tracking, Model/Prompt Registry, CI/CD
>25. Low-level code (executables, byte code, .apk, etc) can be decompiled to the language of interest (e.g., this enables people to get source code of mobile apps
and potentially cause you some security problems)
>26. TensorFfow Lite is awesome as a runtime on hardware constrained devices (Java API for Android, C++ API for Embedded Devices)
>27. You can configure Kafka to use SSL/TLS
>28. You can also choose to do preprocessing at the data source (e.g., microcontroller) and send the data direclty to the ML Data whareshouse , or even more preprocessing and feature engineering and send it to the Feature Store direclty (the problem is that you might intereate a lot on the features, so you would have to change the code running on the data source device), or something in between.
>29. Modern microcontrollers with internet connection have Bootloaders that support Over-the-Air Updates. But even if they dont you can implement an application bootloader (second stage bootloader) which is what the bootloader thinks is your application, but actually it will just read some protion of flash data taht tells it
which application (address) to run and the run it (this is done in C by casting the memory address of the application we want to run as a function pointer and thenc alling it, like how JIT Artifacts (main I/O files) are used in JVM Interpreters)
>30. Serverless Deployments (the easier version of PaaS where now you dont even need to make a container, you can just send functions): way to build and run applications and services without having to manage infrastructure and low-level stuff. The application still runs on servers, but all the server management is done by third party service (AWS). We no longer have to provision, scale, and maintain servers to run the applications. Practilly this means we dont SSH into their machines. Can be done giving our github/gitlab/gitea (self-hosted) repo (e.g., Heroku) ou with Containers (All clouds support). The best way is with containers, in this case we only send an image and get from AWS the DNS that resolves to the IP entrypoint which takes us to the machine hosting our application. We can later even get a real DNS (without AWS name on it) and point to the AWS one.
>31. Worflow Orchestrators (Tools: worflow orchestrators such as: General workflow Orchestrators: Argo, Prefect, Airflow, Dagster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau, dagger; (2) Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro, Couler, metaflow, sqlflow; (3) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo), mleap, flyte, sematic; (4) Data-driven Workflow Orchestrators: Pachyderm; (5) Notebook-based Wrkflow orchestrators: Ploomber) do not substitute spark/koalas jobs code, it just uses this code within its higher level DAG of tasks execution model. A task can be the deployment of a spark/koalas distributed environment, execution of spark/koalas jobs or undeployment of the spark/koalas distributed environment.
>32. Kubeflow pipelines (_Note:_ built on top of Argo), Metaflow and Flyte are ML workflow orchestrators, analgous to what Airflow or Argo is for data. ''Provides a framework for treating each ML process as a node in a graph that can be run in an underlying infrastructure''. ''The workflow orchestration component (C3) orchestrates the tasks of the automated ML workflow pipeline. For each task, the required Artifacts (main I/O files) (e.g.,  images) are pulled from the artifact store (e.g.,  image registry). Each task can be executed via an isolated environment (e.g.,  containers). Finally, the workflow orchestration component (C3) gathers metadata for each task in the form of logs, completion time, and so on''
>33. Kubeflow vc MLFLow vs MetaFlow:
    * 33.1 Kubeflow: ''is a collection of open source tools that aims to run ML processes on top of Kubernetes/OKD. It has a tool for hyperparameter optimization (Katib), serving (Fairing, KFServing), and pipelining (Pipelines)''
    * 33.2 MLFlow: ''is a framework for doing ML experiments. Notable use-cases include logging and tracking. Lately its adopting more use-cases such as project and model organization. You can use MLFlow within Kubeflow, I dont see them as competitors''
    * 33.3 Metaflow: ''is a task-orchestration framework. You need to write Metaflow-specific code for it to run (think Airflow or Dagster). However, its built with ML experiments in mind as reflected in its API''
>34. CookieCutter and Yeoman are great for generating project strcuture templates (templates) for various kinds of software projects
>35. Training data pipiline & inference data pipeline  should be under the same umbrella (tool or framework) because they are very similar and need to be in sync with each other. This maximizes code reuse & minimizes sync errors.
>36. On feature importance/attributions: ''Attributions are useful when a fraction of the features are important to a prediction, the attribution method localizes these features, and the human checks if the important features are as expected''. Also, when you use an explanation/interpration tool you should know the algorithm (and assumptions powering it) behind it, because results can vary between algorithms and consequently tools (aka there isnt a right or wrong, you just need to know which algorithm uses the assumptions and has emprirical evidence that makes more sense to you). (Good way to think about it: the level of importance of a feature X is measured by the entropy decrease between (P(Y=y) and P(Y|X=x). This is exactly how feature importance pops out of decision trees naturally, because they are naturally optimizing for this.))
>37. Critical Decision-Making Systems: Cant get wrong certain kinds of inputs, more important than aiming perfect accuracy in non-critical situations (some-time there arent many of these) (the models's decisions can have life/death impact on humans?) &rarr; tesla example: egde cases (log-tail) have a lot of life/death potential. Cant get too many instances wrong of a certain class (e.g a system that detects very well a lot of problems, but always fails in detecting hernia) cant get deployed like this. Unless you change the AI-Human interaction pattern to be a second-opninion intead of automating doctors. Unfornetely (Actually forntunately, becouse we dont want everyone dying now) these types of cases are also cases where there is much less data. So this requirement becomes very diffcult to achieve
>38. Strong Key and Encryption/Decryption Security:
    * 38.1 In Hardware: 
        38.1.1 Strong-box keys (specific hardware handles keys). Some devices have dedicated hardware modules (separe processor, memory, storage) that stores keys and do encryption/decryption usig these keys (for applications). (E.g., some Android devices have these, all SIM cards have them)
        38.1.2 Hardware Security Module (HSM): like dedicated hardware modules, but these are devices on their own that can be connected to the network and service multiple machines on the network. It is an alternative to Key Management Services (even more secure) that you can get from your cloud also, but then you are responsible to make the key management infrastructure.
    * 38.2 In Software: Hardware-backed keys (more generic, keys are stored somewhere outside the OS). This is generally done through Trusted Execution Environments (TEEs). TEEs are basically a (security desgned hardware architecture, new separate booting program, lighweight OS (independent of main OS)) tuple that runs processing of highly confidential data and has a high level interface (through a shared memory location) with the main OS. If the main OS is comprimised, it cannot get the highly confidential data.
>39. Secure MPC := MPC with Homomorpic Encryption
>40. Causal Inference in a Nutshell:
    * 40.1 Interventions (layer 2 - requires causal graph information): 1. Defining causal graph via domain knowledge (causal priors/assumptions) or causal discovery; 2. Testing: Try to break assumptions of causal model with statistical & sensitivity tests; 3. Define causal estimand (e.g., P(Y=y|do(T=t))); 4. Identification: from causal estimand (contains do operator) to statistical estimand; 5. Estimation: Estimating statistical estimand (probabilistic functions); 
    * 40.2 Conterfacuals (layer 3 - requires modelling of unobserved variables): 1. Defining causal graph via domain knowledge (causal priors/assumptions) or causal discovery; 2. Testing: Try to break assumptions of causal model with statistical & sensitivity tests; 3. Define counterfactul (e.g., P(Y == yb|ta,ya)); 3. Re-estimating parent random variables from ya evidence; 4. Compute Do(T == ta) intervention: 4.1 Identification: from causal estimand (contains do operator) to statistical estimand (does not contain do operator); 4.2. Estimation: Estimating statistical estimand (probabilistic functions)
>41. If you have a learned sequential decision making system (e.g., RL) and you are gathering data from it, beware. If the data gathered is very biased (e.g., Supervised Learning recommender system recommends top 3 ads to user, then user clicks on ad just because it was recommended for him (would'v clicked in any ad) but the system doesnt take this into account andupdates its parameters to favour that ad in the stituation (past user actions, user profile) it encountered, then in similiar situation will recommend that ad again ... summary: only a few ads get shown because the system only exploits what went right and doesnt gather data on what would the clicks be if it had recommended other ad, doing Imitation Learning (Supervised Learning to learn the policy) will only estimate well a small portian of the domain (feature space), meaning that you will have a bad estimator in the majority of points. However, this data is still very usefull for modelling the environment (transition, reward, observation & noise functions) to then do Sequential Decision Making on top (Planning, RL or Control).
>42. Feature Validation Concerns
    * 42.1 Computational cost: ''First, a company might have hundreds of models in production, each model uses hundreds, if not thousands of features. Even something as simple as computing summary statistics for all these features every hour can be expensive, not only in terms of compute required but also memory used. * Tracking too many metrics, i.e. constantly computing too many metrics, can also slow down your system, and increase both the latency that your users experience and the time it takes for you to detect anomalies in your system''
    * 42.2 Too many false alarms: ''n theory, a small distribution shift can cause catastro, remixphic failure, but in practice, an individual features minor changes might not harm the models performance at all. Feature distributions shift all the time, and most of these changes are benign. If you want to be alerted whenever a feature seems to have drifted, you might soon be overwhelmed by alerts/notifications and realize that most of these alerts/notifications are false positives''
    * 42.3. Distrubution changes due to human-error in multiple processing steps: ''Even if you detect a harmful change in a feature, it might be impossible to detect whether this change is caused by a change in the underlying input distribution or whether its caused by an error in one of the multiple processing steps''
    * 42.4 Uninterpretable feature: feature that come from some dimensionality reduction method (e.g., PCA)
>43. Streaming featured datapoints to Monitoring System: 3 problems
    * 43.1 ''Representation differences. Class ratios across windows may not be the same (e.g.,  the fraction of positives in one window may be very different from the fraction of positives in another window).''. Solution: Startified Sampling. Making groups of known predictive accuracy and then sampling from them accroding to their size. The smaller the group the better because they will have less variance, however too small makes the acc of the group unrealiable.
    * 43.2 Delayed labels with varying delay. Solution: Startified Sampling. Making groups of known classes and then sampling from them accroding to their size. The smaller the group the better because they will have less variance, however too small makes the acc of the group unrealiable.
    * 43.3 ''Varying sample sizes. Bad because you get windows with very unrelaible acc estimations and other with unncessaririly high realiability. The number of data points in each window may vary (e.g.,  the number of requests received on a Sunday is less than the number of requests received on a Monday)''. Define window parameters (size, offset) as to make periods of time with peaks or lows be braken down into two windows, in this way every windows gets a peak and a down and overall sample size is aprox the same.
>44. Having Interpretability improves a lot model debugging
>45. Model Compression
    * 45.1 ''Varying sparsity disproportionately and systematically impact a small subset of classes and exemplar'' (. Two ways to think about it. 1. Simple Uderfitting: Imagine a ground thruth function that is a straight line (low complexity) and then very wigly (high complexity), if you have a wigly model it can take care of both, but if you have a more linear model it cannot take care of the wigly part and will have a lot of error on it; 2. Directing capacity to high density regions: normally, the model already tries to model better (direct capacity) to regions with higher number of examples, and then when you restrict its parameters, it does this even more, amplifying error on low frequency regions (where dangerous bias lies in usually))
    * 45.2 ''The majority of weights (90% of all weights) are used to memorize very rare examples in the dataset'' (this happens rare examples region is prone to overfitting (the dataset), because it has few datapoints. And overfitting beaside being bad for generalization (over datasets) is very bad for memory, it requires a lot of parameters, thus big models). ''This has far ranging implications. Most natural image, NLP and audio datasets follow a Zipf distribution. If we want to model the world, we need to design train models that can efficiently navigate low-frequency events''
>46. Out-of-distribution generalization:
    * 46.1 Small-data regimes: higher chance of happening, because: 1. Tendency to happen Data Distribution shift (Note: sudden shifts are often indicative of data bugs)s; 2. These shift can direct a lot of mass in training data scarce-regions
    * 46.2 Big-data regimes: lower chance of happening, becuase you can basically cover all the domain with a good amount of mass.
>47. _Correlation does not imply Causation -- Simpsons paradox:_ aggregate measures (that dont take explicitely into account confounders) are not usefull for making any causal conclusions. Aka correlation does not imply causation)
>48. _On Under-represented regions_
    * 48.1 ''Privacy trade-off with fairness - differential privacy disproportionately impacts underrepresented attributes''
    * 48.2 ''Early-stopping disproportionately impacts performance on less common and more challenging features''
    * 48.3 ''Underrepresented attributes disproportionately impacted by the introduction of stochasticity''
>49. General Workflow Orchestrators (e.g., Airflow, Argo (created to address Airflow problems)) have the job of updating the artifact that power software systems. While the system is running in prduction, workflow orchestrators run in parallel, and use a separate cluster to run a pipeline of tasks (that run after certain condition) to build updated artifacts that will be used by the production system. They are awesome because they centralize management of tasks, give a very nice UI, and provide support for storage tools (e.g., Ceph, S3) (where you storore I/O of your workflow), processing tools (e.g., spark/koalas) and can orchestrate containers/pods (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit))/Kubernetes/OpenShift/Kubesphere) (where processing runs inside). Basically any set of tasks that needs to run on a certain conditions to change some state should be controlled by Airflow. Notice that these tasks are diffrent from services:
    * 49.1 Tasks vs Services
        * 49.1.1 Tasks:
            * 49.1.1.1 Runs sometimes, when conditions are met (previous tasks were finished, some state value, timer-based, etc) (ephemeral)
            * 49.1.1.2 Changes state of the System by producing some new artifact
            * 49.1.1.3 Tighly Coupled with each other
            * 49.1.1.4 Typically Self-contained code
            * 49.1.1.5 Triggered by one or more state (artifacts) conditions
            * 49.1.2.6 We know exactly when it has to be triggered
            * 49.1.2.7 Infra-oriented: will be triggered directly or indirectly, by a programmer-set timer; and, in the end, has the objective of updating our services (basically a generalization of typical CI/CD. Typically CI is triggered by a push, integrates code, tests and CD does tests and deploys code to target environment. In this generalized sense: CI means running a task after a certian condition, producing an artifact; and CD means putting this artifact in operation). 
        * 49.1.2 Services:
            * 49.1.2.1 Run indefinetely
            * 49.1.2.2 Serves a client
            * 49.1.2.3 Loosely Coupled with each other
            * 49.1.2.4 Typically depends on other services
            * 49.1.2.4 Triggered by one condition: the request
            * 49.1.2.5 We dont know when it will be triggered
            * 49.1.2.6 User-oriented: will be triggred, directly or indirectly, by user actions.
>50. ''Often, schedulers are run on top of Container Orchestrators (e.g., Run spark/koalas job scheduler on top of K8; or Run AWS Batch scheduler on top of EKS)''
>51. ''Empirical studies of NN behaviour:
    * 51.1 There are 'easy' and 'hard' examples
    * 51.2 Hard examples are easily forgotten (near decision boundaries), and are more useful for training than easy
    * 51.3 Training seems to have two phases; early learning of patterns which generalize, later memorization of exceptions
    * 51.4 Implicit regularization from over-parameterization helps learn patterns''
>52. Leveraging bonus data methods (the situation: we have a discrimintive model; the goal: if we have more useful data on some inputs (x + x_bonus) how do we leverage x_bonus to adapt our current model and make a more accurate decision?. _Note_: can also be framed as having a lot of missing data)
    * 52.1 Imputation
        * 52.1.1 Heuristics-based Feature Imputation
        * 52.1.2 Sampling from Unsupervised Generative Model (Maximum-Likelihood Estimate, a.k.a. MLE)
    * 52.2. Learning-based
        * 52.2.1. Bayesian methods (update our current hypothesis belief distribution)
        * 52.2.2. Transfer Learning. (Ensemble on top of y_hat & x_bonus to correct the output)
        * 52.2.3. Meta-Learning (Learn mapping function from x_bonus to parameters of x --> y mapping)
        * 52.2.4. Colaborative Filtering (Can estimate based on other xs, with similar x_bonuses, predictions)
        * 52.2.5. Supervised Generative Models (they have a bonus of handling missing data inference natively)
    * 52.3. Knowledge-based:
        * 52.3.1. Knowledge bases (E.g Knowladge Graphs)
        * 52.3.2. Expert Systems
    * 52.4. Learning-based + Knowledge-based
        * 52.4.1. Ensembles
        * 52.4.2. Learning on top of Knowledge Representations
> 53. ''Dont increase your data by more than 10x at a time'' - AndreNG.
> 54. Distributed processing engines: ""spark/koalas & Dask have no built-in GPU acceleration. Need RAPIDS Accelerator for accessing GPU resources. Ray supports application code that runs on GPUS (e.g., Tensorflow library) but does not make use of it itself. Ray basically extends the python interpreter by coming up with an interface that lets the user do thread-level, process-level and distributed parallelim way more easier and efficiently than with pure Python (My note: Ray can be used as: (1) a single machine parallel processing framework (is for Python what OpenMP/OpenCL are for C++) and (2) a distributed computing framework). spark/koalas & Dask (My note: Dask is like a framework that integrates popular DS tools with high performance (parallel Acceleration with CPU threading & low-memory footprint) that are Data Science focused, and have their own interfaces and abstractions for doing distributed computation. There is also integratio libraries that help integrate workflows that use more than 1 of these: 'For example, spark/koalas on Ray does exactly this - it "combines your spark/koalas and Ray clusters, making it easy to do large-scale data processing using the Pyspark/koalas API and seamlessly use that data to train your models using TensorFlow and PyTorch."' RAPIDS project is basically making GPU libraries (compiled CUDA code into native .so + binding code) that mimic existing only CPU libraries (e.g., RAPIDS spark/koalas Accelerator).
> 55. Knowledge of an AI System == Data + Priors. If you cant get more data, you need stronger priors; else: trust more the data.
> 56. VSCode has extensions for [editing files inside VMs](https://code.visualstudio.com/blogs/2019/05/02/remote-development) & [Containers](https://code.visualstudio.com/docs/devcontainers/container)
> 57. There are 2 succesfull patterns for open source companies:
    * 57.1 Open Core with Payed SaaS Option (Managed Cloud) (e.g., MongoDB & Confluent)
    * 57.2 Open Core with Payed Support for On-premise or IaaS Deployments (e.g., RedHat)
> 58. Kubernetes 
    * 58.1 Namespaces: has namespaces. This means you dont need to get a new cluster of machines to run another set of workloads. Kubernetes can run both sets of workloads in the same machine cluster, each one of them in isolation. 
    * 58.2 Service Discovery & Routing: all packets from other pods are redirected to the host pod A (by making it the default next IP). All dns packets will be handled by kube-dns DNS server (By making it the priority DNS server in the cleint pods) and a service IP is returned to the client pod. All non-DNS packets that arrive to the host pod A (packets which have a different destiny IP, for now, the service IP) will be redirected to another host pod B in the same machine as a suitable service pod (another machine relative to the client pod & host pod A); this is done by kube-proxy Proxy. Kube-proxy is in constant communication with master node and knows the state of the k8s deployment thorugh etcd, therefore it can map (next IP (that will be resolved to a MAC address): my host pod; destiny IP: service IP) --> (next IP: the host default pod in the same machine as a service pod (host VM); destiny IP: service pod). When the packet arrives at ""the host default pod B in the same machine as a service pod"" kube-proxy works again; it before hand had changed the OS networking config to forward the packet to the virtual adapter of the ""service pod"" since they are in the same virtual network. When the response packet comes back it actually comes back to the host pod, and then kube-proxy acts again by forwarding it to the client pod. But how does it know the IP of the client pod corresponding to that packet? When it first sent the outgoing packets it stored a tuple (IP of client, client PORT used for the connection), therefore, when the response packet arrives it retrieves the IP by the PORT in the packet (this is the same principle by which router implement NAT).  
    \
    Ok, but this doesnt explain how tha packet goes from the physical machine to the VM right? Well, kubernetes doesnt care about physical machines, this problem is already solved with VM network setup. kube-proxy (indirectly through OS networking config) redirects the packet to the VM with destiny IP of the pod, but in practice, under the hood, the packet from VM A will be redirected to its gateway which is the deafult VM host and the default VM host (physical machine) knows which physical machines (on the physical cluster) have which VMs on them (How does it know? VM networking setup configured the networking stack of the OS), so it redirects to the default VM host (physical machine) containing the destiny VM; and this is all done via a VM tool such as vagrant where you can deploy VMs & configure all VM hosts (physical machines) to do exacly this!
    * 58.3 Stateful Applications: k8s can mount external storage (persistent volumes) to a specific pod (doesnt need to be in the same machine).
    * 58.4 Pod placement optimization: k8s can optimize placement of pods so that pods that communicate more with each other are in the same node (IP packets are transimited instantaneously) and pods that use a lot of storage are acompanied by pods that use little or no storage.
    * 58.5 Logging: containers just need to output normally to stdout & stderr, kubernetes worker (kubelet) will copy these streams to files in the main pod (host) & then kubernetes can retieve these logs upon an api request (which you probably wont be doing directly, you will be using the cli (kubectl) to make these api calls) https://logz.io/blog/a-practical-guide-to-kubernetes-logging/
    * 58.6 Controller and Operators.

        ""
            In Kubernetes, most of the operations happen in an asynchronous manner.

            For instance, when one creates a ReplicaSet object (picking a simpler object), this is the sequence that happens:

            We send the request to the Kube api-server.
            The kube-api server has a complex validation
            Ensures that the user has the RBAC credential to create the RS in the given namespace
            The request is validated by all the configured admission controllers
            Finally the object is just written to ETCD - nothing more nothing less
            Now, it is the responsibility of the various Kubernetes controllers to watch the ETCD changes and actually execute the necessary operations. In this case, the ReplicaSet controller would be watching for the changes in ETCD (e.g. CRUD of ReplicataSets) and would create the Pods as per the replica count etc.

            Now, coming to Operators, conceptually they are very similar to Kubernetes controllers. But they are used with third-party entities. In Kubernetes, there is a concept of CRDs, where vendors can define their own CRD which is nothing but a custom (e.g. Vendor specific) kubernetes object type. Very similar to the manner in which Kubernetes controllers read to the CRUD of Kubernetes objects, these operators respond to the operations on the corresponding CRDs. E.g. Kong operator can create new API entries in the Kong API server when a new API CRD object is created in the Kubernetes cluster.
        ""
    * 58.7 Dynamic Resource allocation (DRA) is an api that lets pods change their resource allocation at runtime.

> 59. High-level DevOps tools
    1. Microservice Frameworks: Openshift/Kubesphere (language-agnostic, for kubernetes) and Spring (for java, infrastrcture agnostic) are microservices frameworks that make it easy for you to build, deploy and maintain good microservices. Spring is more opnionated and comes with more built-in stuff, but you need to use java. OpenShift/Kubesphere provides an easy way (GUI) for you to run commands on a remote pod (using k8s APIs), analogous to SSHing to a remote VM. You can also use VSCode extension to edit documents inside the pod
    2. Web frameworks: e.g., Next.js (for javascript), Django (for python) and ASP.NET (for c#); these make it easy to build good frontends and application servers together, instead of separately, therefore they are based on SSR, HTML pages are deliveres ready to the browser. This means that the browser requests for the a page, the server builds the page on the fly and sends to the browser the ready html. The server also does the job of application server, sending on-demand data to the client. A major difference between Django and Nextjs is that Nextjs is a full http server (app and web server) on its own, while Django is just the logic code that a http server runs as a library, so you need a python http server (e.g., gunicorn/uvicorn) wrapper that calls django functions. Nextjs is a Framework that lets you define web server and app server funcionality, and then it builds a nodejs HTTPS server for you that does the role of both app server and webserver (SSR and/or static). Very similar to Django in python world, but Django doesn't come with HTTPS server functionality, it fits as a library within an python HTTP Server like Uvicorn (with Gunicorn worker). So, for Django, the HTTP server is actually built by Uvicorn.
    3. Mobile Frameworks: e.g., Managed like Firebase or open source like Supabase.
> 60. You can run system monitoring pods (get % of processor, memory, storage used, networking usage) on a OSprovided you with these functionalities (system calls). Remember that the host is just another container (it is only special in the sense that its the only one when that we choose to physical networking capabilities with other physical machines, therefore being a proxy for pods; also, we setup up & run other containers from inside it)
> 61. AWS Lambdas: is basically running a container job/task triggered by some event & with scaling guarantees. So AWS is doing the trigger & scaling part for you, so you dont have to worry about the listening to triggers (either listening to requests or listening/requesting messages coming from a stream transport or message queue) & scaling (increasing number of pods & instances dynamically)
> 62. Data version control tools:
    * 62.1 For small scale:
        * 62.1 Git Add-on
            * 62.1.1 dvc/fds. dvc/fds acts on top of git (their internal files are tracked by git & they use git) & solves the problems of large files (with external storage) & artifact tracking (by linnking a subset of commit IDs (most of the commits wont be "dvc/fds commits" because they will not be new experiments, just bug fixing & small tweaks) to <command to run & make-like file (treated as a pipeline) to generate artifacts, command to run & make-like file (treated as a data pipeline) to evaluate artifacts & generate metrics, location of artifacts, labels of artifacts, location of metrics, labels of metrics>) (artifacts: functions, elt, models, datasets, pipelines, prompts, filters; metrics: model acc, pipeline latency, etc). Note: not shure if you have to put artifact directory inside gitignore or dvs already does it for you. The problem with dvc/fds is that is slow, especially for large datasets, because its storing everytime the entire file as a new version. Also, if you want to do some big data and/or heavy processing as data CI, it wont work well for you, because it pipeline support is just for pipelines running locally. Note: now has a built-in GUI (dvc/fds studio). Dont have really Data CI Pipelines, because the pipelines are not trigger automatically when data changes, it just offers a way for you to specify a pipeline. "DVC is a great tool, that after all gives us super-data-management-powers but has some limitations. When working with simple DVC remotes, it’s hard and inefficient to download individual files from large data directories, and even harder to upload a few new files to an existing directory."
            * 62.1.2 Git LFS (Very simlar to dvc/fds)
        * 62.2 DB
            * 62.2.1 Dolt (good alternative if your dealing with tabular datasets & are not doing experimentation. It is more storage-efficient than dvc/fds because it stores patches, not entire new files. Is faster than dvc/fds because it uses a DB, dvc/fds deals directly with getting files from storage. Finally, it doesnt do things locally, which lets you separate concerns & keep your dev environment lean. Bag thing is you need to migrate your data to dolt, only deals with tabular data, doesnt storage-scale)
    * 62.2 For big scale
        * 62.2.1 Virtual Storage System
            * 62.2.1.1 Virtual Data Lake
                * 62.2.1.1.1 LakeFS (a better DVC for automatic workloads, scales way better than dvc because doesnt use uselles local cachaing and doesnt rely on git. Also, doesnt store entire file again, just changed parts (COW-style, deduplicated versions of files))
                * 62.2.1.2.1 Nessie
            * 62.2.1.2 Virtual Lakehouse
                * 62.2.1.2.1 Delta Lake
        * 62.2.2 DB
            * 62.2.2.1 TerminusDB (git-like distributed DB)
        * 62.2.2 DB with native Workflow Orchestration Integration
            * 62.2.1 Pachyderm (a more heavy weight solution (more reource consumption) that "has its own git, but just for data", and also is a workflow engine for the implementation of data CI pipelines in a dedicated cluster. Also, has a GUI. Has built-in support for streaming data (streaming data arrives, gets added to raw dataset, which trigger pipeline to run only on new data). Note: give you the felling of working with a git-like tool locally, however, datasets are not stored locally, they are stored in the cluster. Also, doesnt store entire file again, just changed parts (COW-style, deduplicated versions of files))
       
> 63. Distributed Processing tools (e.g., spark/koalas, Dask, Fugue, Ray) are used within tasks of a Worflow Orchestrator (Tools: worflow orchestrators such as: General workflow Orchestrators: Argo, Prefect, Airflow, Dagster, Mage, kestra, Cadence, Azkaban, Nomad, Bacalhau, dagger; (2) Virtual ML Workflow Orchestrators: zenml, CLAIMED, kedro, Couler, metaflow, sqlflow; (3) ML Workflow Orchestrators: kubeflow pipelines (_Note:_ built on top of Argo), mleap, flyte, sematic; (4) Data-driven Workflow Orchestrators: Pachyderm; (5) Notebook-based Wrkflow orchestrators: Ploomber) by deploying distributed processing environments, executing jobs on these environments & undeploying them. _Note:_ Fugue is an uprising tool that is a higher level ditributed processing tools that provides a single api for leveraging Dask & spark/koalas!. _Note 2:_ Ray operates on top of plain python, therefore its generally a lower-level distributed processing tool that ends-up being used by higher level tools (e.g., Prefect) or integrated with them (e.g., spark/koalas on Ray (which is a bit misleading because its actually spark/koalas + Ray running side-by-side)).
> 64. 
    * 64.1 Chaining LLMs can be dangerous because wou wilbe multplying the acc to get the final acc of it altogether. E.g., if a single inference in an LLM has 0.9 acc, when you chain 5 of them together you get 0.9^5 == 0.59 acc. So always take into account that the steps you put in the chain, higher the chances that it will fail at some step during an inference. SO evaluate the steps separately and then multiply the accuracies to see if the final acc statifies you.
    * 64.2 IO Validation: always need to validate format of input & output of LLM
    * 64.2 Battle against Prompt Injection. ""We took the following steps that we think can help: The output of our LLM call is non-destructive and undoable
        * No human gets paged based on the output of our LLM call
        * The LLM isn’t connected to our databases or any other service
        * We parse the output of an LLM into a particular format and run validation Artifact Lineage Stor against it
        * By not having a chat UI, we make it annoying and difficult to "experiment" with prompt injection inputs and seeing what outputs get returned
        * Our input textbox and allowed outputs are truncated
        * We have rate limits per user, per day
    If someone is motivated enough, none of this will stop them from getting our system to do something funky. That's why we think the most important thing is that everything we do with an LLM today is non-destructive and undoable-and doesn't touch user data. It's also why we're not currently exploring a full chat UI that people can interact with, and we have absolutely no desire to have an LLM-powered agent sit in our infrastructure doing tasks. We'd rather not have an end-user reprogrammable system that creates a rogue agent running in our infrastructure, thank you.""
> 65. open source tools for Data Wharehouses & Data Lakes
    * 65.1 Data Wharehouses: https://www.freecodecamp.org/news/open source-data-warehousing-druid-apache-airflow-superset-f26d149c9b7/ (e.g., Druid, Hive)
    * 65.2 Data Lakes: https://www.alibabacloud.com/tech-news/data-lake/61-delta-lake-hudi-iceberg-open source-data-lake-technology (e.g., Hudi, Delta Lake, Minio + Drill, )
> 66. Your Cloud Storage & Compute should be in the same avalability zone to minimize cost & latency
> 67. High Performance Python
    * 67.1 Resource Profiling of a program: [Scalene](https://github/gitlab/gitea (self-hosted).com/BrunoScaglione/scalene)
>68. Stdout redirection to file. It iseasy to do in the bash terminal, but what happens under the hood? Well the bas terminal is a program just like any other and it
starts the process for you program of interest bt making a sys call giving some params through the resgisters (may be a pointer to some place in memory with more memory capacity, which then stores more parameters), one of these params is the path of the file (which may be absolute or relative). Another parameter is how to deal with stdout: the default is write to a place in reserved memory space for the OS (a part of the process' allocated memoery) that will be read by a rendering program and rendered to screen when this program uses the graphics driver; another option is to redirect to a specified file, then internal config of the OS for that process is changed as to write the output to that desired file (the sys call handling software/driver will be reading a config).
> 69. Redis vs etcd: 
    ""Hi Michael, a fellow devop here, with detailed experience with both etcd and redis in production for a large website.
    You're comparing apples and oranges.
    \
    Redis is a blazing fast in-memory key-value store that can be compared,
    performance-wise, to memcached. Its major selling points over memcached
    as a fast in-memory cache are its ability to store typed data (so lists,
    hashes, etc etc) and to implement easily a lot of different programming
    patterns out of the box. It has no consistency and its disk store
    capabilities are abysmal; it will buffer writes and if redis crashes
    (which never occurred to me, in 3+ years of operations) you WILL lose
    data upon restart. It has some master/slave replication capabilities,
    and an attempt at high availability and clustering with sentinel which
    I've always found to be dodgy.
    \
    On the other hand, etcd is a CLUSTERED k/v store with automatic and
    strong failover, which aims to be consistent between failures. Its read
    performance is decent but I can guess it's 1-2 orders of magnitude lower
    than the one you can achieve with redis, and its write performance is
    even less amaizing. Its best use if for important data that should be
    reliably and consistently read across a cluster with 100% uptime. I said
    it, not 5-nines, systems like etcd should aim at 100% :) Thus it can be
    used as a cluster coordination tool or as a configuration store (say you
    need to share state between hundreds of servers...) - the exact same use
    Zookeeper or Doozer are made for.
    \
    So I'd say etcd and redis are excellent choices for their respective
    roles, but they do not compete in the same sport :)""
> 70. Istio/Kiali/linkerd2/Cilium/Consul (Form the clients pod point of view, rejects or accepts packets; but can do a bunch of nice stuff under the hood)
    * 70.1 How it works: 
        Istio/Kiali/linkerd2/Cilium/Consul/kuma is a wrapper for Envoy proxy that does kube-proxy's job & more before the packet reaches kube-proxy itself. Intead of the proxy being at another pod (host pod) and not looking at the application prootocol (like kube-proxy), it is a sidecar container and looks at the application protocol.
        \
        Istio/Kiali/linkerd2/Cilium/Consul alters the networking config of your pod with an init-container, it makes your pod redirect all (that do not from the side itself) outgoing & ingoing packets to a side-car container called Istio/Kiali/linkerd2/Cilium/Consul-proxy. But how does this redirection works since containers in the same pod share the same IP? Well, they share the same IPs, but a pod (an ths all containers inside it) can have a lot of IPs, one for which adapter which is for a different network. So, you can use this to treat containers in the same pod as if they were different pods, you just make a netowork that connects two different adpaters (need to create 2 virtual adapters before this) of the same pod together. This gives you the same way of redirecting packets, but with the advantage that you guarantee that the proxy will be in the same machine as the app pod. 
        \
        Ok, now the packet arrives at the Istio/Kiali/linkerd2/Cilium/Consul-proxy sidecar, what happens now? It needs to know the IPs corresponding to the pods of the service (the service the client pod wants to use). It knows becomes a headless service was configured, which makes the kube-proxy return directly the IP's of the pods insteaf of a service IP (which headless services dont have). Ok, but when the packet arrives the host pod with destination IP: 'Pod IP Here', how it is forwarded to the right physical machine instead of being forwarded to the router? Kubernetes also modifies host pod networking config, to forward packets going to those pods, to the physical machines correponding to these pods.
        \
        Wait a minute, but how did the packet arrive at the sidecar in the first place? Since, kube-proxy is returning a list of pod IPs instead of one service IP? Well, I suppose the client Pod choses a random one. And then since the chosen IP wont be inside the client pod's networks, it will forward it to the host pod (the gateway).
        \
        Conclusion, Istio/Kiali/linkerd2/Cilium/Consul uses headless services in combination with envoy sidecar proxies instead of using service IPs & kube-proxy.
    \
    * 70.2 Capabilities:
        * 70.2.1 Security
            * 70.2.1 On-transit encryption: implements SSL/TLS
            * 70.2.2 Communication rules: can define exactly how services can communicate with one another. If something out of these rules is attempted by a pod, rejects packets and returns error to the client.
        * 70.2.2 Routing: useful for A/B tests, canaries, etcs
        * 70.2.3 Observability: gives detailed metrics on each pod IO.
        * 70.2.4 Filtering: can be used as a deboucing method for UI pods, to avoid sending a lot of same requests in sequence.
> 71. dbt + piperider is a lightweight tool that helps you interact systematically with a DB (DAG of SQL queries) to transform data from a data wharehouse & put it back in the same wharehouse. You could simply just use your Data Wharouse client for this but thins start getting messy if the transformations are more complex, because then you need proper versioning, testing, packaging, etc. Note: if you have big data, need fast & more complex processing, you should be doing ETL and using a distributed processing (e.g., spark/koalas) tool instead.
> 72. K3S, Kubeedge and Microk8s are kubernetes modifications tailored for the edge. But they are a bit different: while K3S is only for the edge, kubeedge is for the cloud & the edge, effectively extending & substituting standard kubernetes when you need some part of your system to run on the edge! They solve the main problems of using standard kubernetes to deploy to the edge:
    * 72.1 Too heavy: Kubernetes software itself consumes a lot of compute resources, which the edge doesnt have much
    * 72.2 Assumes logical machines (nodes) are in a logical network: edge devices are not in a logical network, a logical netwok needs to be built
> 73. Containers (Tools: Container Image builder (e.g., Docker, Podman, Contaierd, Buildah, Moby, pack, ko) + Minifier (Tools: Slim) Processor Acceleration (Tools: NVIDIA Container Toolkit)) just have native access to the CPU. To give them access to the GPU for example, you need to give them a GPU Container runtime (e.g., for NVIDIA GPUs: this is provided by the NVIDIA Container Toolkit)
> 74. RAPIDS project provides GPU-accelerated libraries that mirror loved non-GPU libraries. E.g., Pandas --> cuDF; Scikit learn --> cuML; NetworkX --> cuGraph; GIS --> cuSpatial.
> 75. Helm that stores charts is the k8s analogous of image registry that stores images.
> 76. Tools that make k8s deployments easier. From taking less responsibilities to more as levels go up.
    * 76.1 Level 1 (Gives you a lot of facilities): plural
    * 76.2 Level 2: FaaS (Builds microservices for you): Open source: OpenFaaS, OpenWhisk, KNative, Nuclio, OpenFunction, Fission; Proprietary: Modal, Baseten
>77. Distributed File systems (e.g., HDFS, Gluster, moosefs) dont store a file in a machine, it divides the file into blocks and stores each block in a node, because then, it can parallelize loading into memory.
> 78. Why Hive is not a DB: "Its a SQL-like interface for HDFS". "Hive is a data warehousing package/infrastructure built on top of Hadoop. It provides an SQL dialect called Hive Query Language (HQL) for querying data stored in a Hadoop cluster. Like all SQL dialects in widespread use, HQL doesnt fully conform to any particular revision of the ANSI SQL standard. It is perhaps closest to MySQLs dialect, but with significant differences. Hive offers no support for row level inserts, updates, and deletes. Hive doesnt support transactions. So we can't compare it with RDBMS. Hive adds extensions to provide better performance in the context of Hadoop and to integrate with custom extensions and even external programs. It is well suited for batch processing data like: Log processing, Text mining, Document indexing, Customer-facing business intelligence, Predictive modeling, hypothesis testing etc." 
> 79. Working with aproxiamate data types (e.g., half-precision (16 bit) float) for ML is not much of a problem because training of ML models is highly stochastic.
> 80. Webscokets are an application protocol alternative to HTTP for duplex commmunication and is much more lightweight, letting the application to a lot of custom "protocol work" that it prefers, that bad thing about this is that it becomes very application dependent & thus not usefull to be exposed as a service.
> 81. Email security:
    * 81.1 Confirm that email domain is one of the person/companies oficial emails by having a source of truth for these
    * 81.2 Given domain name is trusted, verify that email realy came from that domain (resolving the name to IP address)
        * 81.1 DKIM: received email should be signed with private key that pairs with public key in DNS record for that email domain, This is verfified by decrypting the encrypted proof of the email and seeing that the result is the claim (e.g., <claim: foo, proof: private-key-encrypted foo>) already is sufficient to be a signature.
        * 81.2 SPF: see if origin sender IP address is listed among authroized senders in the DNS records of the sender's email domain
> 82. Microservice frameworks:
    * 82.1 Java: Spring, Vert.x, Quarkus, Micronaut, Axon
    * 82.2 Golang: GoMirco, Echo, Fiber 
    * 82.3 Nodejs: Molecular, Koa, LoopBack 4
    * 82.4 Python: Falcon, CherryPy
    * 82.5 C#: ASP.NET
> 83. Unified DB Interfaces (they lets clients interact with a single unified API for getting data)
    * 83.1 Just Interfaces that connect to multiple data sources: Trino, presto, Apache Drill, mindsdb, cube, vespa, EvaDB, Alluxio, GlareDB
    * 83.2 Actual DB systems (Under the hood they are using multiple specialized DBs)
        * 83.2.1 Data Lakehouses: Data Lake + Data Wharehouse: Hudi
        * 83.2.2 All DBs (Data Lake + Data Wharehouse + Vector DB + Time-series +...) DB: Deep Lake, AIStore
> 84. cassandra or scilladb column-family is the analogous of Postegres Table, however, rows in column family can have totally different schemas and are access though keys (key-value where value is row). Internally, the key isnt mapped to a row, but to an <index, set of columns> which put together give you a row. cassandra or scilladb stores columns as its main data object. Cassadra has its own query language CQL whihc is similar to SQL.
> 85. On WSGI (e.g., Gunicorn): "There was a time where there was an explosion of Python web frameworks, and the situation was non-ideal -- either the web server bit was bundled together with the web framework bit, or the web framework was only compatible with X web server. So if you use one framework, you're basically stuck with the web servers it's compatible with, and the server could suck.
\
So a bunch of people got together and said, well why don't we agree on a common interface between the web framework and the web server, so that all web frameworks can be compatible with all web servers, and then we can swap them out as we wish. Thus WSGI was born.
\
If you don't want to play nice with WSGI, you don't really have to. Just write your own custom HTTP server to work with your framework. You just can't switch to a different web server.
\
Node is kind of a bit of a different ecosystem. For starters you basically have Express which is more or less the only choice, So there's no real reason to have a common interface between web servers and web frameworks when everyone just uses Express."
> 86. Why Flask Development Server cannot be used in production: "The bundled server is a development server. It's not designed with production environments in mind.
\
* It will not handle more than one request at a time by default.
* If you leave debug mode on and an error pops up, it opens up a shell that allows for arbitrary code to be executed on your server machine(think os.system('rm -rf /')).
* The development server doesn't scale well."
> 87. Why Web Servers (e.g Gunicorn/Uvicorn) is not sufficient and you need Software Reverse Proxies (e.g., NGINX) in front of it or why NGINX is not sufficient and you need Web Servers behind it?:
"Here are a few reasons why using WSGI/ASGI with Nginx is preferred over direct communication between Nginx and Python applications:
\
Separation of Concerns: Nginx is optimized for serving static files and handling HTTP requests and responses, while Python is optimized for handling complex business logic. Separating the web server and application layers allows each to focus on what it does best.
\
Scalability: When a Python application is deployed behind Nginx, it can be scaled horizontally by adding more instances of the application. Nginx can then be configured to balance traffic between these instances, ensuring that requests are distributed evenly and efficiently. (_My Note:_ What about when using k8s? doesnt k8s already solve load balancing for you? k8s uses them under the hood, abstracting them away from you, just asking which load balancer tool you want to use. However this doesnt mean you shouldnt use NGINX or some other reverse proxy as a microservice in the cluster, because load balancing is just one of the things reverse proxies do, they also do caching, security and protocol translation.)
\
Security: Nginx provides a number of security features, such as SSL termination, rate limiting, and IP filtering. By deploying a Python application behind Nginx, these security features can be utilized to provide an additional layer of protection for the application.
\
Flexibility: Using WSGI/ASGI allows Python applications to be run on any web server that supports the standard interface, not just Nginx. This gives developers the flexibility to choose the web server that best suits their needs, without having to worry about compatibility issues."
> 88. github/gitlab/gitea (self-hosted) Self-Hosted Runners: github/gitlab/gitea (self-hosted) provides environments that you can run your git hub actions CI/CD orchestration. Normally, this is a very lightweight worload that orchestrates interactions with CI/CD services, so you dont need a powerfull machine. However if you want a leaner approach you can avoid using CI/CD services, than you can also do everything (not just orchestration) in this machine enironment provided by github/gitlab/gitea (self-hosted), but of course picking a more powerfull environment to run the CI/CD workload that is now heavy(essentially substituting the need for you to use a managed CI/CD tool or host a CI/CD tool). This is better than running locally, but CI/CD tools are more prepared with: best practives, higher performance, built-in capabilities (e.g., installing tools, building, testing, genarating reports, deploying to multiple targets, integrations with other tools, treating CI/CD as a pipeline, monitoring, API, higher security)
> 89. CML is a very lightweight tool that only generates reports for your experiments (integrates nicely with dvc/fds) made to be run purely on the github/gitlab/gitea (self-hosted) actions runner. in my opnion, CML should be a just a capability inside dvc/fds.
> 90. Python Packages
    * 90.1 Package installation
        * 90.1.1 Distools vs Setuptools: Overall, distutils is the original package management system for Python, and it is included in the standard library. setuptools is a third-party package that builds on top of distutils and provides additional features and functionality. You can build python packages using either, but setuptools is recommended because its the evolution of distools. Distools is buil-in the plain python installation, setupttools in not, but python installer already install it aswell. You would only notice the absence of setuptools if you build python from source, in this case you would need to install setuptools
        * 90.1.2 Python wheel: pre-built python packages (menaing, that non-python compiled language code (e.g., cpp) was already compiled. Traditional python source installation build the wheel at installation time, but now a lot of pakage are already being distributed as wheels, which makes more sense (Note: output is .so libraries dynamically liked to system .so libraries that will be available at runtime). Note: the compilation occurs at setup.py using either distools or setuptools.)
        * 90.1.3 Pip vs apt-get to install python packages: use pip only if: using python virtual environments, when you need an old version of a package or when the package is only available in PyPI, else: use apt-get. What about when python packages have non-python dependencies? More detailed exlanation: 
            "PyPI is the Python Package index - repository of python modules.
            \
            pip is used to download and install packages directly from PyPI. PyPI is hosted by Python Software Foundation. It is a specialized package manager that only deals with python packages.
            \
            apt-get is used to download and install packages from Ubuntu repositories which are hosted by Canonical.
            \
            Some of the differences between installing python packages from apt-get and pip are as follows:
            \
            Canonical only provides packages for selected python modules. Whereas, PyPI hosts a much broader range of python modules. So, there are a lot of python modules which you won't be able to install using apt-get.
            \
            Canonical only hosts a single version of any package (generally the latest or the one release (PR to release branch)d in recent past). So, with apt-get we cannot decide the version of python-package that we want. pip helps us in this situation. We can install any version of the package that has previously been uploaded on PyPI. This is extremely helpful in case of conflict in dependencies.
            \
            apt-get installs python modules in system-wide location. We cannot just install modules in our project virtualenv. pip solves this problem for us. If we are using pip after activating the virtualenv, it is intelligent enough to only install the modules in our project virtualenv. As mentioned in previous point, if there is a version of a particular python package already installed in system-wide location, and one of our project requires an older version of the same python package, in such situations we can use virtualenv and pip to install that older version of python package without any conflicts.
            \
            As @Radu Radeanu pointed out in this answer, there would generally be difference in names of packages as well. Canonical usually names Python 2 packages as python-<package_name> and Python 3 packages as python3-<package_name>. Whereas for pip we generally just need to use <package_name> for both Python 2 as well as Python3 packages.
            \
            Which one should you use:
            Both apt-get and pip are mature package managers which automatically install any other package dependency while installing. You may use anyone as you like. However, if you need to install a particular version of python-package, or install the package in a virtualenv, or install a package which is only hosted on PyPI; only pip would help you solve that issue. Otherwise, if you don't mind installing the packages in system-wide location it doesn't really matter whether you use apt-get or pip"
    * 90.2 Building Packges yourself (Tools: https://packaging.python.org/en/latest/guides/tool-recommendations/) (Learning: https://packaging.python.org/tutorials/packaging-projects/)
        * 90.2.1 template/Template Generation (Tools: Pyscaffold)
> 91. Paralleization via threads: you spawn multiple thread at the same time to execute parallel portions of your compute workload, then you loop check if x of them have finished, so that you can execute (spaw another thread) for the next portion of your workload that depends on the outputs of these threads. GPU processing at the end is just parallelization via SIMD threads, where each threadexecuted on a vector processor (but CUDA's API makes you feel like you are making a lot of SISD threads).
> 92. Processor stuff
    * CPU Stuff
        * 92.1 DMA controller is a microcontroller programed to receive as input from CPU (CPU writes to DMA input registers which are in the Memory Address Space (abstraction that lets cpu think everthing (storage/registers/cache/memory) is main memory)): destination memory address, number of bytes to transfer, where to get the data (which IO Device and which place in it (e.g., file ID if interacting with Disk controller)), granted permission to use data bus. It then, on behalf of the CPU requests the data from the IO controllers/interfaces and routes it to the data bus. When the number of bytes is reached it: (1) sends to the IO controllers a signal that says it doesnt want more data; (2) reliqueshes its permission of using the data bus; and (3) generates a CPU hardware interrupt. Note: DMA is significantly slower than the CPU and will riliquish access to the data bus also while its running (when its not using it).
        \
        What the CPU does while DMA is running?
        ""
        You are correct that the CPU cannot be accessing the memory during a DMA transfer. However there are two factors which in combination allow apparent parallel memory access by the CPU and the device performing the DMA transfer:

        The CPU takes multiple clock cycles to execute an instruction. Once it has fetched the instruction, which takes maybe one or two cycles, it can often execute the entire instruction without further memory access (unless it is an instruction which itself access memory, such as a mov instruction with an indirect operand).
        The device performing the DMA transfer is significantly slower than the CPU speed, so the CPU will not need to halt on every instruction but just occasionally when the DMA device is accessing the memory.
        In combination, these two factors mean that the device performing the DMA transfer will have little impact on the CPU speed.

        EDIT: Forgot to mention that there's also the factor of CPU cache, which as long as the code that the CPU is executing is in the cache then it won't need to access real memory to fetch instructions, so a DMA transfer is not going to get in the way (although if the instruction needs to access memory then obviously a real memory access will take place - potentially having to wait for a break in the DMA device's use of the memory).
        ""
        * 92.2. CPU/DMA-Generalized Memory Interaction
            * 93.1 Address Translation and Routing. CPU uses virtual memory addresses and doesnt talk directly with memory and IO, it talks with the memory managemtn unit that will do the conversion to actual addresses and then converts these addresses to actual interface signals. It is a micrcontroller that runs software that looks into an table (called pagemap) that maps virtual addresses to actual ones. If were were only talking about using memory that problem would be solved, however, programs uses the CPU to write to IO interfaces (e.g., registers, buses) and its easier to consider to put these locations as an extension of the memeory address space as if they were memory aswell. This makes up the CPU address space which contains memory part and IO interface part. Therefore, after it has the actual address, it needs to map address to a specific IO input signal (e.g., input to the memory data bus, DMA registers, GPU registers, adapter input buffer, etc) and the implementation of this is just a basic "routing circuit" plugged into the output pinns of the memory management unit that activates (enables write) only the input signal on the desired location and the others are not. 
            * 93.2 Load and store queues. Even after address transaltion and routing, the CPU/DMA doesnt talk directly to the memory controller (if interacting with main memory initially), it interact with queues via a protocol (e.g., AXI4). These are special memories that hold memory requests (the signal that would be written directly to the memory controller) & responses. Whats the point of these queues?
                * 1. Enables multiple components of the memory system to fullfil the request, the faster one wins. e.g., memomry controller and cache.
                * 2. "In a real (multi-core) system, this protocol will of course not just handle read and write requests, but also other messages used for cache coherency and other things (i.e. cache line invalidation messages, read for ownership requests, possibly even atomic memory transactions, etc)."
        * 92.3. CPU idle states: when CPU knows that some peripheral (e.g., RAM) will take some cycle to give back it output, intead of checking at every cycle if the output is ready, it enters idle mode for a few cycles it is shure that it wont be ready yet and for the rest of the cycles it checks or gets out of idle when an interrupt arrives. The idle mode makes the logical core do nothing which saves energy.
        * 92.4 CPU states & power savings. "CPU cores have different states they can be in. Your system will decide what state to switch to depending on the workload. These states range from fully powered (often called C0) to deep sleep (C5, C6). In these states certain parts of the core are turned off or run at reduced performance. (Think of the CPU core as a house, with each room having a light switch so you can turn off the lights when nobody is using it.) Then there are also so-called p-states which regulate the frequency and voltage of the cores. So in addition to switching off certain parts of the core, they can also run at a lower frequency to save power. (For example, my CPU is clocked at ~3.4Ghz but mostly runs at 2Ghz or lower when I'm browsing and shitposting. The screenie also shows how my system is spending ~97% in C6 - switching C-states can happen dozens or hundreds of times a second. The tool is called i7z if you want to take a look for yourself.)"
        * 92.5 CPU kernel mode <-> User mode switches are triggered by specific software interrupts. If going from user mode to kernel mode: user program makes sys call, the CPU generates an auto software interrupt, which changes EIP to a general Interrupt Handler entrypoint program which gets the specific interrupt handler (Interrupt service routine) address from a mapping data structure called the Interrupt Descriptor Table (IDT), swithes to the mode of the process to kernel mode and changes EIP to that address to start executing interrupt handler code. If going from kernel mode to user mode: kenel-mode prgramcan simply use an intruction to change the state of the CPU. What about the OS/Driver programs that alway function on kernel-mode independent from user system calls (e.g., process management or device driver)? I think the CPU starts on kernel mode (bootloader needs to run on kernel mode), is hardcoded to run process management program periodically on kernel mode, and the processing management program makes the mode swiching.
        * 92.6 Heap location is defined by the program and then the OS handles actual memory mapping of it and dunamic memory allocation on demand: "The heap is usually created by the C runtine which is linked with your code (statically or dynamically). It decides an address in the virtual address, calls the operating system provided system calls to map pages and stores the address in some data structure which is used by malloc (and family of functions) as heap. All this code is either executed before calling main or is statically initialized in the binary."
        * 92.7 CPU-GPU Communication
            ""
            Typically CPU and GPU communicate over the PCI Express bus. (It’s not technically a bus but a point to point connection.) From the perspective of software running on the CPU, these days, that communication is typically in the form of memory-mapped IO. The GPU has registers and memory mapped into the CPU address space using PCIE. A write to a particular address generates a message on the PCIE bus that’s received by the GPU and produces a write to a GPU register or GPU memory.
            
            The GPU also has access to system memory through the PCIE bus. Typically, the CPU will construct buffers in memory with data (textures, vertices), commands, and GPU code. It will then store the buffer address in a GPU register and ring some sort of “doorbell” by writing to another GPU register. The GPU (specifically, the GPU command processor) will then read the buffers from system memory, and start executing the commands. Those commands can include, for example, loading GPU shader programs into shader memory and triggering the shaders to execute those shaders.
            ""
    * GPU Stuff
        * NVIDIA GPUs can talk to each other directly via NVLink bus
        * NVIDIA GPUs can access storage directly via GPUDirect Storage, avoid having to go via cpu memory first, before getting to gpu memory

> 95. What is the TOR and how does it work? TOR is a descentralized (part of web3) networking solution to protect the privacy of clients and services on the internet.
    1. TOR Browser. It is like an overpowered machine-to-site VPN, that instead of using a VPN server controlled by a company (centralized), uses 3 different "VPN servers" hosted by anyone (descentralized). When the packet arrives at the first "VPN Server" (note: TOR doesnt implement a VPN, I am just using the term to facilitate understanding) it has 3 layers (onion) of encryption applied to it (as opposed to 1 layer of machine-to-site VPNs). The packets were encrypted sequentially using public keys provided by random nodes in the network (the TOR client need a list of IP addresses of the network's nodes) at the client (browser). But how are the 3 public keys of the nodes requested if we want to pretoect the clients IP Address? One way is for the cleint request all three keys to the 1st node, the 1st node requests the 2nd and 3rd keys to the 2nd node, and finally, the 2nd node requests the 3rd key to the 3rd node. The the 3-layer-encrypted packet is sent to the first "VPN Server" which is the Ingress Node. The Ingress Node peals off the outermost encrytion layer and sends it to the next node ... until the 3rd node (Exit Node) finally applied its decryption to the final layer of encryption and gets the original IP packet, it then routes it normally. To see the contents of a packet a malicous actor would have to control all 3 nodes.
    2. TOR services. Allow users to publish their service without revealing their identity (IP address). It is basically to inverse of TOR Browser. Now instead of protecting the client you want to pretect th server. So you actually publish in DNS the IP Address of the 1st node and give your IP Address to the last node. Sequential (onion) encryption is done at each node and all three layers are decrypted at the service. But how do these nodes request their public encryption keys if you want to hide the IP Address of the service? One way is for the second node request the 1st and 2nd public keys to the 3d node. The first node request the the first key to the second node. "With hidden services, there is a slightly different action at the third step. The exit node does indeed strip the last layer, but it then re-encrypts it with a new inner layer, and hands that off to another node, who will add a second, with the guard node adding a third before passing it to the hidden service provider."
> 96. API gateway vs Reverse Proxy? No, use both.
    1. API gateway gives you a unified interface for the clients, allows services to use different protocols without changing client interaction with them, can address device-specific data needs (e.g., mobile app has slighly different data needs than a web app) without the client having to know about it, makes it faster for them because clients make multiple calls in parallel and the api gateway is way closer to the services so the trips are smaller, can implement security (e.g., authentication, TLS, IP filtering, rate limiting, IP hiding) and monitoring functionality (e.g., attaching traceid to requests, logging and producing netwroking metrics) common to all your services. Also implements retry and recovery mechanisms, caching and even service discovery. _Note_: doesnt need to this this all internally, can use helper services for some things (e.g., authentication server).
    2. Load Balancer: receives requests for a logical service and distributes them to the actual service replicas. In k8s, this is usually pretty hidden from you, but is implemented in every logical machine in the default pod (requests from other pods get rerouted to it via OS configuration)
> 97. Batch (async, highly parallel, dont need to know consuming rate)) vs Message broker/queue (async, parallel, dont need to know consuming rate) vs streaming transport (async, few-at-a-time, you need to send messages to the conumers at their consuming rate)) vs req/res (sync, one-at-a-time). Going to the right of the spectrum to address more latency bottleneck, and going to the left you address big data bottlenecks. 
> 98. Authentication/Authorization for Microservices (Managed services: Cognito, AuthO):

    1. Types of Authentication (Tools: auth, )
        1. Within app
            1. (Optional) Service-to-Service: the keys for both sides + client ID + client roles (pair to client and public to server) are provided by KMS at pod start-time. KMS authentication keys are "hardcoded" in k8s and on the KMS (only the public key). This KMS key setup is done via a KMS Frontend. The frontend ultimately uses "Frontend (Users) Authentication via API gateway".
                1. Via encrpytion protocol layer: 2-way SSL
                2. Via app protocol layer: JWT/JWE
            2. Frontend (Users) Authentication via API gateway: via user/email-password (and then can be facilitated by just sending cookie or JWT afterwards)
        2. App->External World: the keys for both side are provided by KMS. KMS authentication keys are "hardcoded" on k8s and on the KMS. The KMS key setup is done via a App Frontend. Differently from "Service-to-Service" where the KMS sends the keys to both parties at system start-time, now it sends to the keys to the Frontend's API gateway upon request, which ultimately gets sent to the Frontend, (these are the API keys user will use to make use of the API) and sends to the service (only the public key). The frontend ultimately uses "Frontend (Users) Authentication via API gateway". The client uses its key pair for authentication at first request, then receives a token (e.g., JWT) where he can continue authenticating itself without having to keep storing the key pair anymore.
            1. Via encrpytion protocol layer: 2-way SSL
            2. Via app protocol layer: JWT/JWE

    2. API gateway of an app: should support some authentication modes (note: makes use of authentication service (often built using LDAP protocol)):
        1. For frontends (works as app server): receives <user, password>
            1. JWT-based: returns signed-encrypted JWT
            2. Cookie-based: returns session cookie
        2. For programs (works as API): receives private-key-encrypted public key and public key;
            1. Dont return nothing, meaning that you decrypt the private key every request
            2. JWT-based: returns signed-encrypted JWT

    3. If you feel like you want to expose multiple logically diffent HTTP APIs, then you should have multiple API gateways. Service never get exposed directly to the internet.

    4. JWT notes: 
        1. JWT generally should be encrypted (making into a JWE)! If you dont want the encryption overhead then you need to use cookies and a session database.
        2. How does JWT token refreshing works? When client receives JWT token, it actually also receives another token called refreshtoken. The refreshtoken is used to get a new JWT token when the old JWT token expires. The authentication server makes a new JWT token based on 3 inputs: the claims (extracted from the old JWT), a salt and his secret. The refresh token simply sets another salt, which will generate an entirely different new JWT. Note: this salt used to make the JWT is stored in the JWT.

    5. Authorization: ok, once authentication is solved, services need to do authorization. Authorization (find out if the person that wants something is authroized to have that thing) is basically two things:
        1. For users (main use case)
            1. Mapping a user ID to all his roles and other relevant information
            2. Based on the user's role (and maybe other relevant info), decide if the request can be fullfiled
        2. For services (note: its best to do this via a service mesh like Istio/Kiali/linkerd2/Cilium/Consul, where all services IO go trhough proxies, and communication rules between services are enforced)
            1. Discovery client service name by checking DB that relates the public key of the athentication with the client service name
            2. Based on the service, decide if the request can be fullfiled
        
        Each service has its own rules, however, they often need data from another service's DB to check the rules. Some patterns:

            1. Keep the data where it is: serivice A that needs data of service B simply requests service B for the data. In this scenario, every service exposes a data HTTP API, where it serves data to other services. But what if a new service comes along (which the user didnt grant access to his data initially) and wants to get user data? We need the user to grant access right? This is where OAuth framework comes in, we will talk about it later.
            
                "Despite its risk of becoming a bit disorganized, this pattern can get you pretty far. Not having to deploy and maintain an extra piece of infrastructure for authorization can be a huge advantage, and if the services with data can handle the load from services that need the data, then stringing them together is a fine solution. I've talked to a few teams who've followed this general pattern but feel like they should replace all the piping with some kind of dedicated authorization service. I always make sure to ask what their real problem is. If it's latency, perhaps adding a cache in the right place will solve it. If it's that the authorization logic is growing disorganized in the services themselves, then perhaps you need to impose a standard policy format. (Oso is one solution to that; there are others too.) But if the problem is that your data models are becoming too complex, or that you're re-implementing the same APIs repeatedly, or that permission checks require talking to too many different services, then perhaps it's time to rethink the architecture."

            2. Gataway attached to an Authentication DB which has all user roles and relevant information. The gateway attaches the relevant user info to the request, that goes to all services. 
            
                "Gateways work best if your authentication data consists of a small number of roles (for example, every user can have just one role at one organization). When permissions start to depend on more than just a user's role in an organization, the size of the request can explode. Perhaps a user can have different roles depending on the type of resource they are trying to access (an organizer of a particular event, or an editor on a particular folder). Sometimes this data is simply too big to reasonably fit in a header, and other times it's inefficient to fetch all that data eagerly. If this is the case, stuffing all relevant authentication data onto a token or into a header doesn't quite cut it."

            3. Central Authorization Service. Serivices neither get the data neither implement the authorization rules, they simply outosurce this job to the Authorization Service, which then needs to be attached ato a DB of relevant user data and also know the authorization rules of all of the services.

                """
                
                Of course, this separation of concerns comes at a cost. All authorization data now has to live in one place. Everything that might be used in a decision — a user's membership in an organization, a document's relationship with its organization — must exist inside the centralized service. Either the authorization service becomes the single source of truth for that data, or you have to replicate and sync data from the applications to that central place (more likely). The authorization system has to understand the whole data model that underlies any permission: groups, shares, folders, guests, projects. If those models change often, the system can become a bottleneck for new development. Any change in any microservice might require an update to the authorization service, breaking some of the separation of concerns you might have sought when moving to microservices originally.
                
                There are other factors that make an authorization service tricky: deploying a service that will be responsible for securing every request means that you're responsible for achieving high availability and low latency. If the system goes down, every request is denied. If the system responds slowly to queries, every request is slow.

                ""

        Which one to use? "When speaking with engineering teams, my guidance is always, "build authorization around the application, not the other way around." For simple systems where maintaining lots of extra infrastructure is expensive, it can be best to keep the data where it lies, and string together your services with purpose-built APIs. Certain applications can grow to massive scale with only basic user roles that can fit on a JWT — in that case, perhaps an authorization gateway is optimal. Some companies with a variety of products, authorization patterns, and user types might prefer to make the effort to centralize their data into a dedicated authorization service."

    6. OAuth2/OpenID Connect (tools: ory hydra, dex, okta): 
    
    Basically a specification on how a company that stores user data can build an API for others to: acess that user data, upon user consent. This involves:

        1. Third-party client registering with your company: "basic information such as application name, website, a logo, etc. In addition, you must register its own redirect URI to be used for redirecting users back to the third-part client (can be a redirection to a web server, browser-based, or mobile app)"

        2. User getting authenticated and authorizing data access: authentication (the user authenticating with your company) and authorization (instead of typical rule-based authorization on the backend, now, user does the authorization in a frontend) upon the third-party redirecting to your company's OAuth frontend.

        3. Token release (PR to release branch) to the third-part client: your company redirects the user back to your the third-part client with an authorization code in https procotol.

        4. The third-party client sends a POST request to your company with the authorization code and authentication data (client ID, client secret, its own redirect URI) and receives back an access token with expiration date-time.
      
    The component of your company responsible for all these interactions is called the Authorization Server. 

    E.g., when you create an account in Tinder with Facebook data, by logging in only in Facebook.

        ""

            OAuth 2.0 is a framework where a user of a service can allow a third-party application to access his/her data hosted in the service without revealing his/her credentials (ID & password) to the application.

            OpenID Connect is a framework on top of OAuth 2.0 where a third-party application can obtain a user's identity information which is managed by a service.

        ""

    7. SSO: 
    
        1. Type 1: when engineers want to access a single page that displays relevant info from multiple internal apps. Via engineers setting up the authentication of the SSO to the services. All apps' gateways using the SSO redirect to the SSO gateway (or reverse proxy), the request to the SSO will now contain the app the brwser origanlly wanted to access in some header. For now on it is simply behaves as an API Gataway with connections to services that span multiple apps of yours. The problem is that, since it spans multiple apps it is external traffic to them, and needs to be hard authenticated to each app API gateway. This is simply: upon request by a frontend used by an engineer, the KMS generates a key pair, sends the public key to the API gateway and the private key to the frontend. The engineer then gets the private key and sends it to the SSO gateway via another frontend. This happens n number of times, where n is the number of apps.

        2. Type 2: when end-users dont want to make a new account for a new service, they prefer to leverage a previous app (that obvisuly they trust) where they already have an account. Via OAuth/OIDC. New app redirects to an app the user already has account, where the user exlicitely grants access to the new service to get his data for registering, and the new service can outsource authentication to the app the user already has account. The new app then gets the data and authentication result via HTTP API of the app the user has already an account.

    8. Tools that do everthing mentioned: open-source: authelia; managed: auth0.

> 99. Why are salts used in hashing? 
    1. Because they make same password have different hashes, which protects users with same password as another user with a breached password
    2. Makes job of attackers that want to guess passwords much more difficult, because salt are random and increase the password size, making the resulting password much mreo harder to guess than a common password that often is small as makes use of dictionary words, birthdays, etc.

> 100. Connection between linux namespaces can be hierarchical: e.g., networking namspace: you can create a namespace for your pod and a virtual adapter for your VM, and then create a virtual switch that connects your pod's virtual adapter to your VMs virtual dapter, effectively making a virtual netowork. This virtual network will be in some sense "inside" the virtual network of the VM, because packets being sent from it to the internet, have to go through the VM network; and packets coming from the internet have to come trough it aswell.

> 101. Transformers United Notes:

    1. DL Models that have revolutionized NLP, CV, RL

    2. Transformers in Language: The development of GPT Models, GPT3

    3. Transformers in Vision: Tackling problems in Computer Vision

    4. Decision Transformer: Reinforcement Learning via Sequence Modeling

    5. Mixture of Experts (MoE) paradigm and the Switch Transformer

    6. DeepMind's Perceiver and Perceiver IO: new data family architecture

    7. Self Attention and Non-parametric transformers (NPTs)

    8. Transformer Circuits, Induction Heads, In-Context Learning

    9. Audio Research: Transformers for Applications in Audio, Speech, Music

    10. Represent part-whole hierarchies in a neural network, Geoff Hinton

    11. Introduction to Transformers w/ Andrej Karpathy

    12. Language and Human Alignment

    13. Emergent Abilities and Scaling in LLMs

    14. Strategic Games

    15. Robotics and Imitation Learning

    16. Common Sense Reasoning

    17. Biomedical Transformers

    18. Neuroscience-Inspired Artificial Intelligence

> 102. Zero copy concept: avoiding unnecessary copies of data. 
    1. When people say "Zero copy formats": usually means that a format which at least serves as: file and communication format.

    2. Powerfull zero copy formats, what people are refering to when saying "Zero copy reads", define a format that serves also as the data structure. This is where Arrow comes in!. Non zero copy formats do a bunch of conversion overhead like: parsing a file into popular data structures, then mapping these data structures to a communication format (e.g., JSON), then the receiving process has to map communication format into file format or those multiple data structures.
    
    Arrow (and other zero copy formats like SafeTensors) effectvely enables multiple different applications like Kafka, spark/koalas, Pandas to work more efficently and well together, by agreeing on a zero-copy format and avoiding a bunch of format conversions.

    3. When people say only "Zero copy": usually means OS/hardware support that enables "The concept of copying data from one peripheral to another peripheral (usually disk to adapter via socket buffer) by kernel, without switching back to user-process is termed as zero copy". But to do this you need a zero copy format to begin with.

    3. Arrow Flight: RPC-based communication infrastrcture to make data-intensive req/res between services faster; which uses Arrow. The kafka of req/res. 

    4. Awesome in-depth explanation:

        ""
            Zero-copy is an optimization that has to do with how we copy data in the OS.

            The problem is simple. Many applications, Kafka included, have the job of basically:
                🔸1. reading files from the disk
                🔸2. serving that data over the network as a response.

            When we get into the internals of it all, we see that the actual bytes get copied a bunch of times:
                
                1. disk → OS read buffer
                2. OS read buffer → application buffer
                3. application buffer → socket buffer
                4. socket buffer → NIC buffer

            Four copies!

            That being said, some of these copies are more efficient than others because they use the DMA engine.

            DMA stands for Direct Memory Access.
            It’s a feature in computers memory controllers which allows certain hardware (sound cards, graphics cards, network cards, etc.) to access the memory (RAM) without the CPU’s involvement.

            Additionally, we have a bunch of user <->kernel mode switches which aren’t free either:

                1. user --> kernel mode
                2. kernel --> user mode
                3. user --> kernel mode
                4. kernel --> user mode after response is written out

            So in total - 4 context switches and 4 copies of the data.

            Do we need all of this?

            💡 Step 1: Disk --> OS Read Buffer.

                The OS read buffer may seem unnecessary here - we could just copy directly into the application.

                In a single isolated operation, the OS read buffer does not make sense - but throughout the continuous functioning of the system it shines.

                Why? Caching.

                Readahead caching is when the OS caches more parts of a file that you actually wanted to read.

                This is why linear reads on HDDs can be so efficient - the OS typically reads more than requested amount, and the rest is left in the cache. On the next read, you read from memory and not the disk, resulting in a big performance gain.

                🤖 Translated into Kafka - the next fetch request will directly hit the cache and avoid accessing the disk.

                So let's keep that buffer. 🤝

                As for the first context switch associated with it, it is fundamentally unavoidable. We will always need to go to kernel mode to read from the disk.

            💡 Step 2: OS Read Buffer --> Application Buffer.

                If your application is not doing anything with the data, why do we need to copy the bytes there?

                Exactly. We don’t. This is what zero-copy is.

                Kafka stores its data in the same binary format it responds to requests with.

                Zero-copy, misleadingly named, is the act of NOT copying data back to the application but rather directly copying it into the NIC buffer.

                This avoids copies 2) and 3), and avoids the two context switches associated with them!

                This is a very efficient optimization, especially when you're reading more than what the OS read buffer can take.

                🙈Imagine the worst case we had - reading multiples of what the disk read cache can store.

                This meant that:
                    💥1. the extra cached copy in the OS read buffer wouldn't get used on further reads (entirely useless copy)
                    💥2. we had to do steps 1) and 2) in a loop until the wohle data is read by the application.

            <contains modifications to the orignal content>
                The new flow is such that:

                    1. The DMA controller driver, namely DMA engine, reads from disk into the OS read buffer, using the DMA (CPU is free to do things that do not require memory interaction).
                    2. The application, using the CPU, copies very small amounts - file descriptors (references) into the socket buffer, by making a special socket system call (because normal socket system call would send this data to the NIC buffer, but that not what we want here).
                    3. Special instructions, namely scatter-gather operations, have the DMA engine use the socket buffer references to read directly from the OS read buffer and copy it to the NIC buffer.

                    Note: DMA controller is capable of doing memcopies to any location within the address space, by post requests to the same memory queue the cpu is posting its requests.
            </contains modifications to the original content>

            Misleadingly named - it's not quite *zero* copies, but it's much less than we started with!

            Four context switches and four copies down to just two of each!

            It's a great optimization that has been benchmarked to be up to 65% more efficient in certain cases.

            I am not certain of its exact efficiency gains in Kafka in particular, as it depends a lot on the application that's being benchmarked.
        ""

> 103. Apache Arrow (used in major open source data-intensive tools like pandas, spark/koalas, ray) is 2 things:
    1. Arrow Format (Data Structure): "language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead."
    2. Libraries for dealing with the Arrow Format: "Arrow's libraries implement the format and provide building blocks for a range of use cases, including high performance analytics. Many popular projects use Arrow to ship columnar data efficiently or as the basis for analytic engines."

> 104. libcloud vs Terraform: libcloud seems to be more low-level and best used for few imperative actions. Terraform is more robust, declarative and should be used for big cloud (cluster, IAM, cloud services like Storage, KMS and DNS) setups. Summary: most of time use Terraform, unless for small things.

> 105. Pydantic: gives python object oriented code the following features: @dataclass + type enforcement. What if I just use @dataclass by itself without pydantic? You get the nice readability that it provides for data objects, however, type is not enforced, an object cna be created using the wrong data types. Pydantic enforces types and doesnt allow the latter.

> 106. MicroVMs (e.g., Firecracker) are lightweight VMs focused on security. Often used for serverless services. Firecracker uses Kernel-based Virtual Machine (KVM) under the hood, which is an open source virtualization module that turns Linux into a hypervisor (instead of having a user program to the hypervisor work).

> 107. LLM notes:
    - "There's a reason why GPT-4 does super well on LSAT, MCAT, and CodeForce tasks; they trained on the test data."
    - Using GPT-4 with temperature=0 doesnt give you deterministic outputs, it gives you something close to it. Because floating point operations are not associative (because of this mistake: when you add a very small number to a big one, without knowing anything else (which is the case of the processor) then you will discard the small number, but little did you know that another negative big number would cancel the first one, effectively making the small number significant afterall) in a practical setting. This happens because of three things:
        1. You need at least same Processor/Compiler (of runtime or model). So you need all the machines that compose your cluster to be the same and the compiler version that compiled the runtime or model to be the same. Ok this is tricky but doable. The problem really starts now.
        2. The problem is we are always using GPUs and you cant know the other of execution a priori due to the threading nature of GPUs.
        3. Even if you could predict out exacly the threading schedule of a specific gpu on machine A, in practice, the request can go to any of multiple machines, and you also dont known a priori to which it will go because the load balancing algorithm (running on the load balancer) is dependent on real-time-events (which are caused by other external requests).
        4. To make it even worse: requests are generally minibatched to fully leverage GPU parallelism, but the batching also depends on real-time-events (which are caused by other external requests)
    - Contrained generation and hirerachical decoding and task-specific embeddings (appending the task to the document at training and inference) should be used in most crtical use cases.
    - RAG:
        1. Problem that query and document might not be different beasts (e.g. question and a doc contianing the answer)
            1. Joint Training
            2. Hypothetical Document Embeddings
            3. Re-ranking
        2. RAG can be used for retrieving information and tools (if you have too many tools)!
    - LLamaindex is like a virtual vector database: it connects to actual data sources (virtual part), indexes e resolves queries (vector db part) to be used for RAG. But it does more than this, it also helps you build agents and different type of applications on top of LLMs.
    - Quantization levels:
        - Level 1: simple mapping of higher rpecision data type to lower rpecision data type, capping the region of the higher precision range that is not covered by the lower precision range.
        - Level 2: mapping of higher precision data type to low precision by using: (1) scaling factor with outler clipping (finding the clipping treshold is called calibration) + variance correction as a multipler in the activation.
            - low precision variance correction factor (called double quantization)
            - high precision variance correction factor
        - Level 3 Quantisize only Wieghts, not biases
        - Level 4: use mixed precision in your models, quantizing less more important weights
            - more important weights == larger weights
            - more important weights == wights that on avg contribute to higher activations on the test distribution
        - Level 5: train/finetune quantizing activations on the fly (using zero calibration methods, making aa calibration dataset before hand using all activations or making a p/layer on the fly calibration dataset out of all activations of layer_i of the batch of datapoints)
        - Level 6: 
            If symmetric quantization & high precision variance correction factor: variance scaling factor == dequantization (instead of dequantizing all weights w_i, can just deqantize the y)!
            Else: dequantize on the fly important weights, the other leave them quantized with variance scaling factor
        - Level 7: train/finetune being aware of quantization affects -> looking for wider minima instead of narrow minima
        - Level 8: train/finetune in low precision natively (downside is that you dont get the a larger more accurate model out of training). 
            - freeze large portion and update low-precision weights on the rest
            - update all weights, but with a small portion only allowing low-precision updates
            - do low-precision updates on all weights
            - do higher precision updates on weights but implement their low precision projecton
            - quantize more before heavy layer computation and dequantize after
        - Level 9: leverage quantization as a regularization method.

> 108. On Embeddings for semantic similarity: Distance function might not work well since embeddings are not exactly semantic; they were trained for certain downstream tasks but it does not mean they are semantic. Specifically, if you take one sentence, flip on negative, run OAI embeddings, chances are it will be scoring close to 0.9, though the meaning of the sentence is flipped." This happens because the downstream tasks of the larger model used to derive the embedding model did not contian any reasoning task that required it to detect that the meaning is flipped, or the there wasnt negative/positive examples in the training data that would force it to learn this. Therefore this problem can be solved via more complex reasoning tasks in the larger model and better data. Additionally, I think instead of having a single embedding, we should have 3 embeddings: <structure, semantics, format> where structure aswers "what are the elements that are being talked about in this text", semantics answers "what are the relationships between these elements?" and form answers "what is the format in ehich this information is presented?". The dependency between any pair of these 3 is not high, so you can think of them as being independent.

> 109. EMbedded DBs (Tools: RocksDB): they are just libraries, they are not a server. It makes them faster, but fuses (couples) services into a single one, this goes in the opposite direction of modularization, thus is bad for maintanance, scaling and fault-tolerance.

> 110. Awesome Python auxiliaries (used in CI/CD) libraries: 
    1. Helpers
        1. tqdm (print a progress meter for your loops); 
        2. Box (making python dictionaries into objects);
        3. Rich (beatifull printing)
        4. icecream (bettter than print())
        5. ijson (iterative json parser)
    2. Performance
        1. aiomultiprocess (async with asyncIO + multiprocessing with multiprocess forfull usage of your cores); 
    3. Static Analysis
        1. Linters: ruff, pycodestyle, pylint
        2. Formatters: Black, flake8
        3. Typification: 
            1. Static: 
                1. Manual Declaration: MyPy, pyre-check, pyright
                2. Automatic Declaration: MonkeyType, beartype
            2. Dynamic: Pydantic, schema
        4. Cleanups: pycln
        5. Python Upgrades: pyupgrade
        6. Security: 
            1. Pure Code: bandit
            2. Executables: BinAbsInspector, x64dbg
            3. Container Images: trivy, grype
        7. Additional
            1. Sorting: isort
            2. Version upgrades: Bump My Version
    4. Testing: ("If you can refactor with ease you probably have enough tests.")
        1. Testing itself: Behave, Lettuce, Robot, Pytest, Moto, Unittest, Hypothesis, Nox, Tox
            1. Dependency injection: returns, injector, mock
        2. Test coverage: Coverage, Codecov
    5. Local Pipelining: Joblib
    6. Shell scripting: Invoke
    7. Funcional and safe programming: hy, coconut, more-itertools, returns

> 111. Protobuff/Avro: like a non-human redable json (which makes it smaller). A generic format for storing structured data, and libraries for parsing this format and serializing to it (data structure --> file).

> 112. Notes on using multiple node balancers for higher availability:

    ""

        Using round robin DNS is not that great for high availability - if one server goes offline, clients will still try to connect to it and wait for a timeout.

        There are other ways to achieve this.
        1) Active/Passive load balancers
        Basically one load balancer handles all traffic for one IP address.
        If that balancer goes down, the passive node jumps in and takes over the IP.
        Keep in mind that load balancers are pretty much only forwarding traffic, so for small to medium sized sites this can work out OK.

        2) Active/Active load balancers
        The same traffic IP is configured on both (or many more) load balancers.
        Incoming traffic gets sent to all load balancers but a algorithm chooses which balancer should respond, all others discard that traffic.
        Simple way to think of it, you have two load balancers:
        When the requesting IP ends with an even number then load balancer A answers, otherwise load balancer B answers.

        Of course your infrastructure must support this and there is overhead due to traffic getting sent but discarded.
        More information, e.g., here: http://community.brocade.com/t5/SteelApp-Docs/Feature-Brief-Deep-dive-on-Multi-Hosted-IP-addresses-in-Stingray/ta-p/73867

    ""

> 113. Scientific Workflows
    1. Workflow Descriptors and Orchestrators: Cromwell
    2. DSLs: Nextflow

> 114. Database schema changes: 

    ""
        Unless the tools you're using have built-in support for database schema migrations (like ActiveRecord/Rails does, albeit rather unsafely) - you need to do it yourself in your application.

        The usual technique is to have a table in your DB that contains metadata about the schema and application. You might structure it in key/value form, or as a single row with multiple cols; it doesn't matter. Either way, this table should store a schema version.

        When you connect on app startup, before doing anything else with the DB, you should query the metadata table for the schema version. If it's less than the current version in the application, your application should apply schema change scripts from the old version to the new version

    ""

> 115. Notebooks in production

    1. ""There is also the "use external python source code files in your notebooks" method (i.e. %load). It creates good separation of production code from investigative code.""

> 116. Change data capture (CDC): refers to the process of identifying and capturing changes made to data in a database and then delivering those as events form downstream components or system to consume

> 117. When ML is usefull?

    ""

        Intrinsically hard problems: For problems for which we simply do not know how to solve them programmatically, machine learning can discover patterns and strategies to solve them regardless. This is particularly common for tasks that mirror human perception, such as natural language understanding or identifying speech in audio and objects in images, but also predicting music preferences. These tasks are complex and we usually do not really understand how they work. Pre-machine-learning attempts usually have made very limited progress for such tasks. Machine learning may not work for all hard problems, but it can be worth a try, and many amazing machine-learning achievements have shown the potential.

        Big problems: For some problems hand-crafting a solution might be possible but it would be so complex and large that maintaining it manually becomes infeasible. Resolving conflicts between multiple rules and handling exceptions can be especially tedious. For example, we might attempt to manually encode music recommendations, but there are so many artists and tracks that rule-based heuristics might become unmaintainable; similarly manually curating indexes of websites has been tried in the early days of the internet, but it simply did not scale, opening the door for (ML-based) search instead. In such cases, it might be easier to automate a solution that learns rules than to write them manually.

        Time-changing problems: For problems where the inputs and solutions change frequently, machine learning may be able to more easily keep up if suitable data is available. For example, in music recommendations, individual preferences and music trends change over time, as does what music is available in the first place as new music is release (PR to release branch)d. Change is constant and even if we had hardcoded recommendation rules it would be tedious to update them regularly. In such cases, it may be easier to build a (complex) ML-enabled system that can automatically update itself.

    ""

> 118. OS Signals:
    ""
        A signal is a software interrupt delivered to a process. The operating system uses signals to report exceptional situations to an executing program. Some signals report errors such as references to invalid memory addresses; others report asynchronous events, such as disconnection of a phone line.

        The GNU C Library defines a variety of signal types, each for a particular kind of event. Some kinds of events make it inadvisable or impossible for the program to proceed as usual, and the corresponding signals normally abort the program. Other kinds of signals that report harmless events are ignored by default.

        If you anticipate an event that causes signals, you can define a handler function and tell the operating system to run it when that particular type of signal arrives.

        Finally, one process can send a signal to another process; this allows a parent process to abort a child, or two related processes to communicate and synchronize
    ""

> 119. "A key insight in MapReduce-style computations is that moving computations is cheaper than moving data. Given the massive size of some datasets, it would be too expensive for a process to read all data over the network. This typically even holds if computations require large machine-learned models in a model inference service. It is therefore common to perform computations on (or near) the machines where the data is stored and transfer only the much smaller intermediate results. Replication of data provides additional flexibility, since the computation can be executed on any machine that has a replica of the relevant data."

> 120. Distributed Training Notes
    > 120.1 Data-Parallel: "An additional key innovation to save bandwidth for large models is not to transmit updates for all model parameters in each synchronization step (this would have been 700 gigabyte for GPT-3 after each batch), but only transmit parameter differences for the (usually few) parameters with changes above a certain threshold. In addition, to balance the load, the parameter server itself can be distributed with multiple servers each storing only a subset of the model parameters"

> 121. Compilers:
    1. You install a compiler (which is a typical user-mode program) according to your OS. Or the compiler executable supports different programs versions for each possible processor, or the installation is done by a package manager that queries the OS to know the processor and installs the right executable. But then how does the package manager run executable get installed? It either is an executable supports different programs versions for each possible processor, or a human choose the right installation for that specific processor.
    2. The compiler can build to several <OS/no OS; processor> combinations. One of these combinations is the host combination and it can discover which
    is the host processor by querying the host OS.

> 122. First-order Logic (FOL) vs Higher-order Logic (HOL): https://www.quora.com/What-are-the-orders-as-in-first-order-or-second-order-of-mathematical-logic-and-why-do-they-matter

> 123. How much data is needed to fit a model with x parameters with statiscal significance (assuming iid data)? A heuristic widely used is 10x.

> 124. Model Openness Framework gives a taxonomy of how open source is a model repo release.

> 125. Research tools:
    - sioyek: smart pdf visualization

> 126. What are policy agents (e.g., OPA)? a service for online testing/monitoring of security/compliance of your services. The tests are specified via policy documents for each service. All the data needed for the tests is stored in the policy agent and then the policy agent does the tests with this data and provides the results.

> 127. DL is the right tool for learning mappings without exactly correct contraints for which the underlying process is compositional (most of processes are):
    - guiding search (heuristic)
    - perception (formalizing the world)
    - translation (going from 1 data modality/form to another that are projections of the same world concepts)
    - learning good representations (embeddings)
    - making surrogate models (less detailed/interpretable models but that still retain good predictive power)
    - recommendation
    - feature exractin

    Which makes them work well when their are embedded with a verifier in the loop (human or formal system). We are not yet in the point of autonomy, we are in AI as Assistant/Copilot phase. AI as Assistant/Copilot can be super usefull in a broad range of apps!

    A big problem of DL is it learns non-generalizable features (shortcut learning, not talking about overfitting the training set, talking about overfitting the entire dataset) from spurious (due to biased data collection, random chance or local optima) observed feature-target dependencies together with generalizable ones (underlying concepts). Note: this isnt actually binary, there is a spectrum of feature learned ranging from zero generizable (memorizing a specific input), passing through a not fully generalizable (only works for a subset of the input space, i.e., subfunctions), to fully generalizable (works in the entire input space).

    You would think that having massive data on top of regularization would basically solve this problem, but what can also happen is the model can learn submodel routers + sub-models internally that work aonly for their working region. Worse, it can be recursive, these submodels can have their internal router + subsubmodels ... So when it sees an input, it does: (1) how similar does this input look to my meomrized special cases?; (2) I'll apply some interpolation of the special case solutions weighted by the similarity score sobtained before.
    
    A good idea is to find a differentiable way of penalizing this pattern duting training and test their predictive power on an ood test set. Multi-task models are also a way of avoiding this since the ratio of memorized program/right program scales fast, so yo memorize a a program for doing lets say 5 tasks, you would need a huge model

    The problem with these LLMs is that dont know exactly their training data. There are a few exceptions such as Falcon LLM and LLM360 which have made their datasets public.

    Another non-trivial problem is when a model overfits certain regions and underfits others. The noisier the dataset, the greater the impact of this. 

    With open source LLMs, getting making an ood dataset is easy, just "invert" the logits (i.e., probability of the most probable next token switched with the probability of the less probable next token, etc). The problem is that this will generate totallly incoherent text with is jus tuseless for evaluation.

    How to get ood coherent text? Well, one idea is to istead of getting the exact oppositve joint ditribution that the model learned, we get someting in between. We establish an ood factor that will modulate how much inverted the logits will be. If the ood factor is 1, then we get the iverted distribution outputing incoherent text, if it 0 we get the original LLM, if it 0 < x < 1 we get ood by the factor of x and coherent text by the factor of (1-x). We keep deacreasing the ood factor strating at 1 until text starts getting coherent. Then we can buid an autocomplete dataset, or just sample the questions/instructions generated to get a question dataset.

    There is another problem, LLMs dont have the right system 2 primitives for it to learn to do symbolic manipulation. Languages requires learning hierarchical concepts, but logic and math don't, they require symbolic manipulation and exact answers, planning requires statefulness, problem solving requires variable computation and internal feedback loops, etc.

> 128. Spiking NNs == 1 bit activation NNs

> 129. Fully open source LLMS: LLM360, OLMo.

> 130. LLMs cant be used for critiquing correctness but can be used for critiquing style.

> 131. Propositional Logical Reasoning is a special case of probabilisitc reasoning (Bayesian or Markov Netowrks) where variables are binary

> 132. the value (or postrior ditribution) of parameters is fixed for all datapoints. A Latent variable value (or posterior distribution) is attached to each datapoint (P(Z=z|X=x) = (P(X=x|Z=z)P(Z=z))/P(X=x)).

> 133. [AI Ethics Standards](https://ethicsstandards.org/repository/)

> 134. Storage patterns:
    1. Block Storage: what distributed DBs use (e.g., Cassandra).
    2. File Storage: what single-machine (e.g., Linux) and distributed filesystemsuse (e.g., Gluster, Lustre)
    3. Object Storage: what object storage services use (e.g., S3), data lakes use (because tey are built on top of object storage) distributed key-value stores use (e.g., Redis). Note: under the hood S3 is implemented with a distributed key-value store.

    At the lowest of levels, everthing boils down to block storage, because that how driver/disk controller implements storage (i.e. they receive a file path, then have a file path -> file ID mapping, then having an ID -> block locations mapping, then retrieve and rebuild the file).

    Also distributed databases, need to partition their data into several nodes via block storage. In this case, the master node stores metadata of where a specific part of the data is. An evident problem is scaling, because you have to repartition the data for every new node.

    However, this is not a good interface for users working with a bunch of files with hierarchical structure, a hierarchical system of files make much more sense. Therefore we create a metadata layer which stores: path -> file ID mapping (and if distributed cluster: path -> location (which node) -> file ID mapping). You might think that this is file storage, but its not. This is object storage. Object storage scales very well.

    Then what is file storage? Well, storing a bunch of path -> file ID mapping can become pretty heavy when paths get big (hierarchical strcture of files is deep). What if instead we store root ID -> subdirectory IDs -> subsubdirectory IDs ... -> file ID? It becomes way lighter. But remember: only if the paths can become very large, if paths are expected to be small, the hierarchical data storage overhead is not worth it.

    Object and File Storage will have the same file system interface, but object storage is just using it as an interface.

    All types need a way to adapt to increasing storage capacity.

    Therefore:
        1. For single machine file systems: file storage on top of object storage is used
        2. For distributed file systems: file storage on top of file sotraage on top of object storage is used
        3. For distributed databases: block storage on top of file storage on top of block sotrage is used
        4. For distributed key-value stores: object storage on top of file storage on top of block sotrage is used

    But if we are building block or object storage systems, why we need to be constrined to working on top of a file storage OS? Well, you don't, you can use specilized OS extensions (e.g., ZFS) or even entire OS specific for storage (e.g., TrueNAS CORE)

> Latent variable models are super usefull for generative modelling where we aim to impose certain structure that makes learning parameters way easier if we know latent variables. Because it enables us to take advantage of bootrapping for alternatiing beteen learning latent variables and parameters. The problem is that its pretty sensitive to parameter initialization.

> 135. Crossplane is a Kubernetes-native IAC tool with primary interface being a GUI. Kubernetes-native means it's mande for setting up K8s cluster and it runs as a k8s operator.

> 136. Porter: docker but for repos. A tool for packaging repos to make it easier for it to be distributed and locally deploy its assets. E.g., can be used to package a platform.

> 137.  RAG == Query Rewriter -> Context Retriever (with approximate/semantic caching) -> Prompt template -> LLM call (with approximate/semantic caching)

> 138. LLM Caching
    1. Semantic Caching -> avoids unnecessary computation: sits between LLM client and LLM service, if the query is very similar (in terms of similar embeddings) to a query stored in the cache, then it counts as a cache hit. Useful if you are expecting users to do simialr queries.
    2. Prompt Caching -> avoid unncessary communication: sits at the LLM client, made to start a session where a fixed prompt piece is appended to every call, this avoid sending the same prompt piece over to the service at every call. Useful if you are expecting users to keep resuing parts of a prompt (e.g., system message).
    3. KV Caching -> avoids unnecessary computation: sits inside the model, internal representation of previous token geration is cached so that we can avoid doing a constly recomputation of these abstract representations, instead we can do a lightweight computation (previous internal represenation, new input token -> current internal representation).

> 139. Emotions (e.g., fear, anger, envy etc) are just a set of heuristic namespaces (aka intuition or system 1) for critical types of survivial situations an agent can encounter. Happyness/Sadness are just dense proxy reward functions. Surpise/Disgust/other quick reactions are just non-verbal communication mechanisms.

> 140. MMLMs
    1. Current standard is for: (1) multimodal inputs: to use a ViT (Bert but for images -> divided images into patches -> adds 2d positional encoding -> linearly projects it to the token space of the transformer -> trained with self-supervised learning (i.e. predicting patches analogous to how BERT predicts token holes) or supersived learning (predicting image classes)) or ViT equivalent for other modalities to get the embedding for the modality (note: this embedding is a sequence of latent vectors, because it's a tranasformer not an MLP (a way I like to think: the trnasformer embedding tells a story from multiple perspectives and the MLP embedding tells a summary)), then project it into the LLMs token embedding space (remember that the transformer architecture has a matrix embedding layer at the beggining, so you can pass just one-hot encoded vectors to it as token representations) via an adapter (usually an MLP) and finally feed the obtained seuquence of vectors normally in the frozen-weights LLM (all with the same position encoding) in between special <x_modality_start> and <x_modality_end> tokens; (2) multimodal outputs: use specific tools (e.g., text2image, text2video, text2speech, text&voice2speech, text&image2segmentedimage, etc). Plus, if you want to change the LLM, just need to retrain the adapter! (though finetuning the ViT also will be better (but more costly)) 
    2. Problems with the current standard
        1. Enforcing image tokens to enter the representation space of text tokens. The representation space of text tokens was learned via only dealing with text data. There might be important representations that only surface from other modalities.
        2. Transformer model doesnt seem to natively suit images, because images are 2D not 1D. So, representing an image as a sequence of vectors seems worse than as a matrix of vectors (what CNNs do). maybbe thre is a way to take advantage of the pros of both architectures? Same thing for videos, but videos are 3D.
        3. At the x modality embedding step, low-level features are ignored. Low-level features are important for things like semantic egmentation and OCR.
    3. Alternative methods
        1. Add a new dimension to all tokens that indicate the token's modality (e.g., GPTAny, Whisper). Solves problem "1" and can natively output multimodal without tool usage.
        2. Do multiple-resolution embeddings (hierarchical embeddings) at ehe x modality emedding step, to provide the LLM with feature of all levels (e.g., CogAgent, LLava-NeXT). This can be done in multiple ways: (1) giving slices (with padding where necessary) of the image to the ViT; (2) using a higher-resolution ViT, etc

> 141. Modern router do these functions:
    1. Traffic gateaway with NAT
    2. LAN DHCP server
    3. LAN DNS server

> 142. LLM Merging
    1. Only works when merging LLMs that were finetuned on the same base LLM because architecture needs to be the same.
    2. Should be used to build a better base model from a bunch of finetuned ones.
    3. Intuition example of why it works: 
        - before: 
            - Model A (dealing with angry user dataset): when this finetune-specific latent var X in NN position i=11221;j=2332 is high and other latent variables of layer i=11221 are of any value: shurely ouput this sad token and when X is medium output sad token with a good chance (e.g., outputs in a sad tone when user is angry)
            - Model B (empathizing with user dataset): when this finetune-specific latent var J in NN position i=11221;j=4253 is low and other latent variables of layer i=11221 are of any value: shurely ouput this sad token and when J is medium output sad token with a good chance (e.g., outputs in a sad tone when user is sad)
        - after: when latent var Z in NN position i=11221;j=2332 is high and latent var J in NN position i=11221;j=4253 is low: Where Z is high when X AND H are high, and J is high when W AND G are high. Where W, G are latent variable important for other types of token inference. Therefore when X AND J (+ noise) is high then shurely ouput this sad token, *and when X OR J (+ noise) is high output sad token with a good chance*.
        - now lets assume the distribution P(user_anger_level, user_sadness_level) looks like a 2d symmetric exponential decay where density acumulates towards de (user_anger_level=0, user_sadness_level=0) and gorun truth sad token appear outside of some decision threshold radius r from (0,0). To better leverage this ditribution your accuraccy frop needs to match de probability drop, i.e., drop
            - Model A: loses a rectangle - (rectangle U readius) of width r which accounts for around 50% of the datapoints -> aprox. 50% acc 
            - Mode B: loses a a rectangle - (rectangle U redius) of width r which accounts for around 50% of the datapoints -> aprox. 50% acc
            - Merged Model (assuming mean interpolaiton): loses a small triangle - (triangle U radius) outside of the decision treshold radius that accounts for less than 50% of the datapoints -> aprox. > 50% acc
         -conclusions: 
            - merging helps when merged models have unique usefull learned features and cut curve of the merged model loses less probability mass then the cuts of each original model, even after accounting for the merging noise.
            - optimal merging interpolation can be calculated by: (1) modelling the distribution of datapoints, treshold curves and merging noise; (2) find an interpolation function that produces a cut curve (e.g., mean interpolaiton produces a triangle cut) that minimizes lost probability mass.

> 143. GPU
    ""
        1. MFU
            1. with NVIDIA GPUs if you're above 50% MFU on a multi-node setup with a large model you're already doing fantastic
            2. recent advancements in more efficient scalability solutions keep on increasing MFU
            3. slow networks and inefficient frameworks or untuned configuration lower MFU
        2. most NVIDIA GPU-based compute nodes these days have 8 GPUs
        3. Bandwith of GPU cards (with usually 8 GPUs communicating with each other) (Even with this really fast comms the network still creates a bottleneck and leads to a short idling of the gpus. To solve this issue the advanced algorithms implement an overlap of comms and compute:  each model is made of many layers and each layer can transmit the gradients it has computed, while the next layer is computing its gradients)
            - A100 @ 300GBps: 16/300 = 0.053 secs
            - H100 @ 450GBps: 16/450 = 0.035 secs
    ""

> 144. Optimizing the cold start problem in model deployment:

    ""
        To summarize, we managed to optimize the cold start time to some extent, and we were happy with the results:

        - No cloud provision delay with standby instances.
        - Faster container image pulling with on-demand and peer-to-peer streaming.
        - Accelerated model loading time with distributed file systems, peer-to-peer caching, and streamed loading to GPU - memory.
        - Parallelized image pulling and model loading enabled by service framework.
    ""
    
### __AI & Friends: summary of AI & other relevant fields of study__

1. AI:
    - definition:
        1. Highest Degree of automation/software: Inteligent Software
        2. Automation of complex goal seeking behaviour (in some cases, simplified to tasks), framed mathematically, done computationally;
        3. Automatically discover the right kinds of information processing (right programs) to achieve certain output;
        4. Building Autonomous/Interactive Agents.
        5. Inteligence not generated by evolution (which doesnt optimize fitness, optimizes removal of unfitnness/robustness. Thats why humans are naturally generalists, not specialists.)
        6. Inteligence == Algorithms; AI == Algorithms run by our computing devices, not biological brains.
    - objetive: intelligence: reach goals better and efficiently;
    - concept: agent perceives and acts on the world;
    - tools: mainly math and computer science.

    1. Planning:
        - definition: making a plan (sequential actions (decisions) to reach a kind of goal efficiently) knowing the world works (how to represent the world's state and how the state evolves (deterministically/stochastically/due to control action);
        - objetive: want to reach goals better and with more efficiency;
        - concept: agent perceives (sensors) and acts (actuators) on the world;
        - tools:
            1. Classical AI (deterministic & discrete systems): ainly search, logic & hardcoded expert knowledge;
            2. Nowadays (deterministic & discrete systems + stochastic & continuous systems): mainly search, optimization, dynamic programming (e.g.,  planning on MDPs), learning (e.g.,  Alphazero) and dynamic systems & control theory (e.g., optimal control for a stochastic dynamical system).

    2. ML:
        - definition:
            1. Modelling a real world process, aproximating to a degree of simplification how a systemthe world works (the real mathematical
            relationship between variables). Also, the choice of variables is defined by the modelling. This done in ML by using ground truth data from the world, a.k.a instances of these variables. The learned model is useful to: infer a subset of these variables, when thay are not known a priori or do planning on top of it.
            2. Starting stupid and improving at some task just by looking at examples/samples (the programmer doesnt know the way the learning algorithm will find best for doing the task).
            3. Extracting desired information from data.
            4. Learning the algorithm that generate the data of interest (actually, that is most likely to have generated the data of interest, aka, that generates the distribution of interest (joint P(X,Y) or P(X); or conditional P(Y|X)))
            5. Automatically identifying the core computation done in the real-world and run it in our computers.
        - objetive: make machines improve at some task by giving more observations to it.
        - concept: optimizing a model family (which has assumptions on the process that generates the data of interest) using observations;
        - tools: mainly linear algebra, probability, statistics, calculus;
        - notes: some general use cases: can automate natural tasks (humans good at, e.g,. image recognition), specialized (e.g.,  text translation, credit loans) or artificial tasks (e.g.,  recommender systems, network optimization), guide search (e.g alphafold, alphazero), generate samples from the data distributio(generative models, e.g.,  GANs, Gaussian Mixture Model), make sequential decisions or control (RL), learn a model of/algorithm that describes the world (e.g.,  model-based RL, System Identification, Self-Supervised learning, Static Business Inteligence), accelerate processes (e.g.,  accelerate large scientific simulations), find causal structure (causal inference), learn representations (manifold learning, embeddings), estimate future events to anticipate planning (Supply chains, Dynamic Business Inteligence).

        1. Supervised, Self-Supervised (predicting subsets of your own X data) and Unsupervised ML:
            - definition: find an algorithm (in the space of possible algorithms according to the model family assumptions) that estimates the process that generates the data of interest, by minimizing a loss function defined on the training data (observations).
            - objetive: apply algorithm to input data in order to infer other variables (estimator).
            - concept: software module taht you give data + desired behaviour (learning algorithm) and get the model (where inference can be done);
            - tools: mainly mainly linear algebra, probability, statistics, calculus.
            - notes:
                1. Supervised and Self-Supervised is basically Pattern Recognition and Representation Learning currently;
                2. Optimization is search in the space of parameters of a function to find it's minimum value.
        2. RL:
            - definition: discover an sequential decision maker algorithm (or function) that maximizes utility (rewards) for the agent on the training data (experience), by improving its policy.
            - objetive: want to reach goals better and efficiently, by finding an optimal policy (function that maps state to action).
            - concept: 1st: agent perceives state of the world and does actions that change the state (events can also change state), and gets rewards as a function of (state, action or events). 2nd: controller under some type of dynamical system (discrete/continuous, deterministic/stochastic, etc);
            - tools: mainly linear algebra, probability, statistics and calculus.

2. Computer Science:
    - definition: design efficient infomation processing systems, and efficient and better algorithms to be run in these systems;
    - objetive: designing efficient hardware that enables other devices to communicate (drivers) and that makes types of coputational;
    workloads run faster, and programing algorithms that take advantage of underlying hardware and are aware of the program or uses thats going to use it;
    - concept: multiple hierarchical levels of abstraction and turing machine are very important;
    - tools: mainly digital eletronics, algorithms, data structures, advanced math, computational complexity;
    - notes: technically, AI should be inside Computer Science (CS), however, AI has grown so much that it overloads to much the already gigantic field of CS.

3. Robotics
    - definition: builds physical agents that perceive (through sensors) and act (through sensors) on the world to reach a kind of goal;
    - objetive:
        - achieve task-specific metrics
        - task-agnostic objectives: flexible and strong mechanical structures, well positioned, cheap and sufficient actuators, cheap and sufficient sensors, smart and fast information processing using some type of computing hardware;
    - concept: mainly coordinate transformations, dynamical system modelling & control laws, kalman filters, microcontrollers, actuator drivers, actuators, sensors
    and physical structure;
    - tools: advanced math, dynamical systems & control theory, eletronics, computer science.

4. Control Theory:
    - definition: provides control action (a.k.a. input) to a known dynamical system in a manner that makes the state of the system have a desired bahaviour over time.
    - objetive: have guarantees regarding desired state behaviour: achieve setpoints fast while being stable (not oscillating or exploding).
    - concept: controller that receives error and provides control action (a.k.a. input);
    - tools: mainly dynamical systems (differential equations basically) and calculus.

5. Probability Theory:
    - definition: provides mathematical guaruantees regarding random variables.
    - objetive: operate on an uncertain world where stochasticity comes from partial observability or inate stochasticity
    (we dont know if the world really has this);
    - concept: guarantees (theorems) regarding random variables and stochastic processes;
    - tools: advanced math.

6. Statistics
    - definition:
        1. explain the data in a methodic way/not lying to yourself.
        2. mathematical of inference from data sampled from generative models
    - objetive: hypothesis tests; modelling: estimators (fitting deterministic or probabilistic models to training data to then do inference of non-observed variables).
    - concept: mainly hypothesis tests, bayes theorem (hypothesis tests run in parallel, using a prior over hypotheis and generating a distribution over hypothesis)
    and estimator bounds;
    - tools: advanced probability math.

7. Causal Inference (Provides a more general framework that uses Statistics & ML)
    - definition: aswering questions about real world processes: understand causal effects (direction of the causal arrow) or inference ability betwwen them
    (going backwards in causal arrow)
    - objective: know intervation causal effects on target variables and reason on counterfactuals
    - concept: mainly using domain knowledg to compose causal graph and then either gathering experimental data (randomized control tests) and/or using observational
    data. The goals is to remove all non causal associations, to then just deal with modelling/estimation of causal association between variables. Can be seen as
    disantangling effects between variables.
    - tools: do calculus, probability, statistics & ML

8. Other fields that get used (roughly in order of importance, from top to down):
    - Optimization
    - Numerical methods (i.e. algorithms that aproximate a macro scale solution for complex scenarios (cant or hard to solve analytically) by levaraging computation
    on micro scale (differntial form of the governing laws, which is the actual way nature is working))
    - Information Theory
    - Graph Theory
    - Signal processing
    - Geometry/Topology
    - Neuroscience
    - Computer Vision and Graphics
    - Computational Liguistics
    - Game Theory
    - Statistical Physics
    - Abtract algebra
    - ...

### __Brainstorming__

* Engineering is all about:
	1. Modelling real-world processes
	2. Parameterizing tradeoffs
	3. Maximizing an objective within the Model

* Happiness is the product of you having goals with sparse rewards and your brain needing non-sparse rewards for learning

* To increase acc on certain kinds of input: classify several times the input, each time adding a different noise/semantics-preserving transformation. 
And then choose the majority decision. Sort of like an random forest.

* RL + GAN 4 active learning

* Guided/Restricted Learning in overfitted networks (msm funcao de Robust ML)
	-> conhecimento do expert guia  aprendizado da máquina

* continuous annotation (can be more than just labelling)
	-> keep varyinng the input and annotation (can be more than just labelling) it as you vary the input vai variando o input e aprendendo como o expert 

* meta-learning for inter subject knowledge transfer ?
e combinar com outras fontes de dados (especifico ao paciente)
deve conseguir ajudar bastante

* NN trained to rpedict if another NN will be right or wrong. This can be usefull for uncertainty estimation (includes ood detection).

* ML for Sensitive Data:
	- Bootstrapped models with fake data
		- make fake data with aprox same distribution and getting labels
		froma boostrapped ml model trained with original data
		- lets say old model got 95% accuracy
		- new models trained with fake dataset will have a ceiling of 95% real accuracy, which is when it
		has 100% fake accuracy
		- goal:produce set of diverse fake models that have aprox 95% accuracy (tryning to get wrong, what the original got wrong --> so actaully 
		getting it right)
		that together (combining their predictiong into one prediction)
	
	- Altering the inputs every epoch
		- how, in order to not affect to much accuracy?
		- by not altering features that the system is getting better at learning and altering less and less and epoch go on
		- ex: compare model from epoch 2 to model from epoch 1? Which examples it now gets equally bad or worse? ow to identify features of examples?
		Maybe by running them through the model and see the spikes of activity. What are the most relevant features? Maybe we should not change

* DL for performance engineering (hardware and software optimization that makes run faster)

* AI for building simulators is Scientific ML

* NN's:

- many architecture can be interpreted as ode solvers
- neural odes are basically a formalization of we are already basically doing in a discrete and non memory effcieint way
- learn derivative intead of value of data;
- architecture that is pushed to learn the native computation behind the process you want to learn
- get out of time domain and work with operators (ex: laplace)


* For uncertainty estimation (includes ood detection)/ active learning. 2 networks: 1 normal feedforward supervised learning and 1 generative network of the inputs
the generative is trained, then for each sample that is used in the supervised learning one --> the generative is changed such that
it now does not model the data distribution, but models the data that shoould be fed to the other network distribution. A region with a large data/initial prob mass
ratio should have little mass now and distribute to the other parts of the distribution (regions model needs data). This way the network can ask for examples (active learning) in regions it has few data (and therefore doesnt estimate well in these regions) Currently these systems are tested/deployed and where it shows a lot of errors --> the team gathers more data on these situations.

* RL:

- its important to learning hierarchical reward functions

- reward funtions are hard-coded because we are focusing on narrow tasks.
Actually, humans learn reward functions in a conntext of hierarchical tasks.
AI already outperforms humans in writting reward functions (Eureka Paper).

- why cant RL construct a model of the world and do search (thinking) instead of just taking action instantly using its current policy when learning?


* ML:
- nees to operate at the same spatial and temporal scale as humans. 
for exampples: humans operate at a low entropy space. This needs to be encoded in
ml algorithms. since it is currently not, high entropy malicious inputs can fool it for example"
One simmple solution would be to put an extra dimension in the output of the classifiers --> "other class" and provide examples of other classes to the model, 
instead of restricting it to just a few classes.
Self-Supervised learning/Unsupervised Learning promises a lot regarding this problem. Because if you learn (model) the distribution of your input, you can 
then use this learned model for supervised tasks, and effectively iperating on the effective space of the problem.

* Training with encrypted data:
	- "Soft encrypting" data by just applying a linear transformation (determinant not zero) to the data. Why isnt this done?
	- or: 
		1. tRain normally model1 with the original data giving erights w1
		2. Can apply a stronger non linear transformations, train the model on each of them and see the final weights w2_i
		3. Train a NN to predict the delta weights (w1 - w2_i) given the transofrmation (need a way of representing them)
		4. Apply some non linear transformation, train the model with it and then subtract the delta weight predicted by the NN

* Internally maoddeling other agents seems very important --> Inverse RL is adressing this in a way.

* We need to understand all the priors/inductive biases us humans are using in the world, to implement these in our systems.
CNNs did this with translational invariance, but there are much more priors to implement.

* Train Supervised learning ML not just on examples of particular class, but also lots of examples of whats not neither of the classes

* Why shouldnt every task train with the inputs coming from a semi-supervised model that learns input embeddings?
It shure works in text (word embeddings). You then get represenations much more predictive than raw representations. You could say that
this is already done in NNs, well yes, but then you need much for data for your specific task (which you usually dont have). The point of
generating embeddings (pretraining) is to reause this knowledge in a lot of subtasks that will need this base knlowledge.

* Also, why doesnt transformer output embeddings instead of probabilities over all possible words? Could just see the word with the closest embedding of the output

* Downscaled model experimentation: analogous to similarity analysis in engeenering. Instead of training big model several times (long running times), train
equivalent small model (with less data) several times to find best features and hyperparameters. Then, use the scaling equivalence and train the big model.

* Apply time-series modelling to vanilla supervised learning in dynamic datasets. Nowadays, we retrain our models peridocially, this approach is a reactive one. 
It would be
better if we could predict how will the estimator algorithm (model) change (due to input drift and algorithmic drift). Also, meta-learn that difference that hat 
drift affects
the parameters is an option (saw a peper that does this).

* ML models operating on dynamic data should be treted as time-series. They should model not onlt dependency between inpt and output, but also how
this dependency is changing over time. Then it can predict how the model will end up changing in the next time iteration, and correct itself before retraining.

* Problems such as feedback loops in Supervised Learning are actually ocurring because the problem should be adressed as a sequential decision makign problem,
thus RL would be a good framing (RL can be viewed as a generalization of Supervised Learning). But policy shouldnt be just mapping from state to action
, it shpould map from (action[t], state[t]) to action. E.g.,  recommender system has to know which decison it took that generated the current input (the recommendation 
shown)

* In RL the rewards such be herarchical with respect to time. The system cant achieve final goals if it it has done stupid things in the beginning. It should learn
how to do simple decision making at the first steps. And then for further steps it can build up on the knowledge of the first ones and then it goes. But how can we 
define
subrewards for subdecion making tasks? The algorithm defines itself through inverse RL.

* Active learning idea: fine grained mesh on decision boundaries. Train simple ML classifier on small dataset, and then get more data (or prioritize training) on 
decision boundary regions ()

* To solve problem of eneven acc across slices of input data: incorporate deviation from even acc across slices of data in the objective function

* Data Augmentation 4 structured datasets: need to find a way to encode expert knowledge in a way that gives us possible augmentation transformations

* NN that meta-learns on the input to make learning nonlinear function more native. One network that maps input to parameters of a traditional network that maps 
parameters to output. An then you have a npther standalone
tradional network that maps parameters to output. An ensemble netowork maps input, and the two sets of parameters to the final set of parameters of another 
tradional netowork that
maps input to output.

* Learning Taylor series of the function you want to learn. Inference advantage: can give fast not accuracte results and slow more accurate results.

* Symetrical Activation units. E.g., f(u) == if 0 < u < 10: u else 0. Could let NNs aproximate functions more easily. Currently to aproximate one part of a function it 
learns function in each part that correct what the last parts left behind, but as you go you need to sum all parts behind you. This is not scalable. With activation 
units with an interval of actuation this is already done (the bad side is that you would have to content for a fixed degree of aproximation (the size of each part 
you are aproximating, but
this could be a parameter learned aswell.))

* RL Policy that receives state and action as input and spits a scalar value. The sclaer value is larger if the action it took lead to high rewards in the few nexts 
steps. In inference, you optimize do gradient descnet in action space.

* Combine a MLE and a Risk Estimation in an Ensemble and treat Risk as labels (have to gather this data from somewhere, like asking to weight these risks or measuring 
impact on comapnies when they made these errors)

* To avoid Averaging problem (when average prediction is not acceptable (e.g., video frame prediction, spits blurry images)): ML should inncorporat something like prticle filter which lets various consistent paths to be explored

* Density Estimation/Generative modelling with explcit P(X) is one of the keys for ML: because enables uncertainty estimation (includes ood detection) (model knowing what it doesnt know).
This can be done as a separate module in the case we have a dicriminative model or can be already done in the case where we do gnerative modelling with explicit P(X,Y)
This in turns causes:
	- good active learning procedure
	- dont make predictions to cases it doesnt know well

* Activation functions that constraint the domain of the linear unit --> mak more sense in the function aproximation view

* \*\*Self-Supervised Learning and RL in Complex Real World Videogames/Realistic Simulations (e.g., red dead redemption, skrim etc) will be very important to advance AI! 
Videogames that mimic the life humman beinng have, from lerning intuituve physics, to
language, labels of the world, reasoning, planning, games (other agents)****

* Intead of retraining everytime we identify distribution drft, we built the ability to adapt to these drifts directly in our models during training. In inference, 
the model is estimmating
the distribution and adapting its parameters on the fly. Can use importance sampling.

* \*\*NNs are generally fully connected, but we can make them more efficient by finding submodules within it, that are losely coupled with each other.
Its like transforming a NN into an Ensemble of Mini NNs automatically. This can be also very usefull for multi-task settings, like transfer learning. 
Modules can be incorporated in other tasks (like functions of a library can be used by multiple programs).****

* Fine-grained regularization: only regularize function in regions where there is few datapoints, but enough to make a difference in our target distribution the 
model will be
operating on. Because whn you do normal regularization you are regularizing non-supurios non-linear regions aswell.

* Bayesian Causal Learnining: prior on causal graphs/latent variables and posterior on causal graphs/latent variables. Liklooh term translates to evaluating causal 
graph
on high quality data or experiments (asumming causla inference method is not shitty, this can be verified by a simulated environment)

* Add State/Memory (Causal Graph depends on the State of the System) notion to causal inference (maybe this is what Causal RL talks about auhuahuha): enable causal 
loops. E.g., you might punch a guy (do(youPunch == 1)) --> punch landed (youLanded == 1) --> the guy got angry (himAngry == 1) 
-> he punched you back (himPunch == 1) --> his the punch landed (himLanded == 1) --> you got angry (youAngry) then you punche again (youPunch == 1 if you just do 
exploitation or do(youPunch == x) if you do pure exploration, and in the middle you can get good data (rnaodm-like) to do causal inference with observational data) 
(The system gets observations to infer states and keeps history in a memory)

* Annotation (can be more than just labelling) for Supervised Learning: instead of each datapoint being lablled by one person and feeding one hot vector as ground truth, make more people label it
and give true p(y|x) ground truth that reflect uncertainty/optimal bayes error rate. Its better because the algorithm wont have to go back annd forth (high variance) 
with respect to its parameters.

* In addition to ground thruth (where our models estimation p(y_hat|x) must have low distance to the ground y distribution), use contraints aswell in loss function 
(where our models estimation must have low distance in just some dimensions of y distribution). This is usefull in the context
of weak supervision and injecting domain knowledge into the system.

* Bias mitigation:
	1. Weight initialization: initializes weights in a way that estimates protected classes higher than their non-protected peers. To prevent that a bad fit 
(local biased optima) causes negative biase towards it
	(positive bias is ok kk)
	2. Computation: binary features cannot be multiplied by weights until there are enough datapoints on both its values (x_i == 0 and x_i == 1). This will obligate 
the model
	to consider them equal and avoid different results for protected and non-protected categories.

* Difficulty level input enrichement: when humans label datapoint Y also label the difficulty of the datapoint. This information makes the model know these datapoints 
probably have higher noise
and that decision boundaries should pass close to them. Also, you can prioritize which datapoints to revise annotation (can be more than just labelling).

* We should eb aiming for controllable systems, not interpretable! We dont need to know inner working if we can control it in the way we want. E.g., most people dont
know how a car works but they dont have much concerns because they can control it in the way they want with high level controls. Interpretability is just one way of
getting contrallability. And controllability requires that we can predict hte behaviour of the system.

* Idea: avoid fitting noise, but doing it better than general regularization: if a region of X has high frequency patterns: two options: noise or complex ground thruth. 
If it
is complex ground thruth, more data will just make the pattern more clear, and if it is noise the pattern will evaporate because datapoints will cancel themselves. 
Therefore
when fitting a function through SGD, the gradient should be aware of the amount of datapoints. If the amount of datapoints is high (according to some metric) then 
gradient can be higher
because the chances of being noise are lower; and if the amount of datapoints is small the gradients should be clipped because its likely that thats noise.

* Discriminitive models: being immune to input shift
	1. Training: train with aprox uniform P(X=x) and estimate P(Y=y|X=x)
	2. Inference 
		2.1 Do P(X=x|Y=y) == P(Y=y|X=x)P(X=x)/P(Y=y) through some aprox inference method like MCMC (one MCMC run for each Y=y value) or variational inference
		(Output is an aproximate joint distribution); 
		2.2 Do P(Y=y|X=x_inference) == P(X=x_inference|Y=y)P(Y=y)/P(X=x_inference) through some aprox inference method like MCMC or variational inference. 

* On test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)s: 
	1. Problem: when you evauate against a test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) multiple times, you are effectively cheating and overfitting it (This can happen by you tweaking your algorithm
or by simply if you just evaluated a lot of random models and chose the best (what happens in Kaggle competitions))
	2. Solution: Intead of evaluating on whole dataset, evaluate on sub test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant) (n samples from the test set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant)). At each evaluation you evalaute aginst a different subtest set (Note: test set needs to be sufficiently large so that the predictive power metric calculated on it is statistically significant). 

* Causal Models a prior to avoid stupid/dangerous exploration in RL. In setting when an RL system can have dangerous consequences if some actions are taken, it cannot
simply explore these actions. It needs yo know that some actions are very stupi in certain situations. A causal model can serve as a initial guide.

* Instead of developing a single model, developed various versions of it. The defaul version is with unifrom data (achieved by weighting datapoints/importance weighting), 
and then versions where the distribution is more skewed are also stored (you could say that the distribution could skew arbitrariry, thus having too many possibilities,
but you can check the features and draw range on them to pin down these possible skewed distributions.) Durinf inference, the distrubution of the data is constalty being
verified, and accoording to it, selects the model with most similar training data distribution.

* Hierarchical models: Model A maps (W,t,D) --> P(X,Y), P(X,Y) is model B in at time t. Model A models the change in model B as a function of time t, some external features W and dataset D.
In this way, we can intead of retraining, because of distribution drift, to get new model; we do reinference, which is way cheaper.

* Intead of laballer just annotation (can be more than just labelling) datapoints we sampled, we artificially create datapoints in larger quantities, and they labell these alwell, and do generative 
modelling (modelling the joint distribution). With this, they migh label datapoints that have 0 probabiliy of occurring in the real P(X=x,Y=y) we are modelling 
(e.g., for image classification: an image with random pixels). However this doesnt matter, because by doing this we are modelling a P*(X=x,Y=y) that the 
conditional/dicriminative P(Y=y|X=x) will be the same for all x where P(X=x,Y) has >0 probability.

* AutoML Monitoring: Meta ML System. A babysitter for the actual ML System. Its job is to monitor the actual ML System by retrieving data from users that might 
indicate failures of the ML System
(e.g., by scrapping the web, sending emails, etc), automatically extract failures of the system, change the system in the direction of minimizing these failures, 
and redeploy. Basically Automating
ML Monitoring.

* Interpretability/Explanability tool:
	1. Assuming you have a trained model A.
	2. You code a funtion that maps x --> y with some random parameters. More formally, you choose a function/model family by coding.
	3. The tools optimizes the parameters that maximizes the likelooh of ^Y (model's esitmates). The user can even hardcode features that he wants to use.
	4. A Business expert can handcode the function and then compare with what the tool spitted, then examples, that produce the buggest differences between
	these functions, are selected. This will show convince new ingights to the business experts and can also reveal weird errors of the model.
	5. Can also enable Scientific ML/System Identification: when we have a bunch of prior knowledge of the system, and we just want to learn some parameters of it.

* The problem with Implementing RL in Production is that it needs to do a lot of stupid actions before it starts learning the good ones. Need an algorithm
that We can Leverage Supervised Learning as a Prior for RL. Basically use Imitation Learning (RL as Supervised Learning) where we make a dataset of state x --> action a
of an expert. The we let this system go on its own making its own decisions and gathering its data. We humans do this aswell, we study a little bit before jumping 
into something
that can have bad consequences.

* Calibration Training (e.g.,  for classification -- separate a calibration training set, and after model development is fineshed, we train the model to learn 
its uncertainty, at each batch: 1. count the number of errors of hte main model, feed this count to an auxiliary calibration model that maps erro_count, 
datapoints --> binary_error for all of the datapoints (need to sum to error count), effectevely allocating errors across datapoints (where it thinks he got 
the prediction wrong), and this model is trained with a typical multiclass loss function that measures the distance of the predicted error vector to the 
actual error dsitribution vector across. After the auxiliary calibration model is trained: you n of datapoints, choose number of errors, fix one of the 
datpaointsto inside a region of interest, randomly select the others, run inference; do this multiple times, then get the expected value of the error 
count for the datapoint of interest and get uncertainty_value == (expected_count/(m/n)))

* Causlaity and DL: the problem with DL without a Causality Framework to support it is that we are trying to fit P(X,Y) directly instead of fitting the process 
that generates it.
We should be fitting pairwise causal functions between features (like causality does). So, units in a neural network should have a casual arrow going or coming 
from a units in the 
previous or next layer. And each of these units can be then modelled normally by a typical NN. So you can think of as a Causal NN of typical NNs underneath. 
The harder question becomes?
how to find out the direction of arrows? Then you need causal discovery algorithms, that will typically statistical tests for independence.

* NNs are function aproximators, but int the end what you get is a bunch of computation in the forms of weights and non-linear activations to get your output. What
if we could a better (more compressed) analytical form of this function? Think in reverse, if some process is y == 2*x but w dont know this and aproximate via NN, 
we would rpobably
get complicated computation like that mimics 2*x in the end, not a simple 2*x and all the rest of the weights being 0 (because there are much more ways the other 
option can occur).
So whats the incentive of arriving at simplified expressions? An idea is to mange training. Idea: if redudant compuation is done at layer i in the form of two or 
more units, in layer i+1 we will see that
thes layer i units will be used with same weights by all units of layer i+1; so the is observed in training management, and layer i fusion of these units is done. 
In reality you wouldnt have
two. This can be directly incorporated in the objective function, whihc will include a hyperparameter term,so you could differententiate wrt to a hyperparameter 
that enbales/disables unit fusions.
But why do this in training?Couldnt we do after training as model compression? Well yes, but then you will have a large cost of unncessary computation in training, 
on the other hand, redudant computation is a natural regularizer, which is nice.

* Inverse Taylor Series: nowadya we our NNs use first order aproximations. What if we use higher order aproximmations, following the Taylor Series, and then inspect the NN
to identify which pattern it is following, that to say, it is the taylor series of which funnction, and then instead of using the computationally heavy aproximation function
we use the analytical one! 

* RL need to learn mroe from control theory & robotics. Agent do not learn directly action(state) function. They actually learn a action'(state'(state)) function and a action(action', state) function, where action'
is a high level actions and state' is high level description of the state, and this is key to taming the commplexity of learning in large spaces; and this can go on on more levels of abstration! 
Then They plan on higher levels of abstraction and have built-in controllers to implement them subcontiously! Whe a player is playing football he is doesnt think about whihc forces he need to exert, but where he wants to go, where he wannts the ball to go
, etc. However, control theory and robotics currently hardcode these levels of outer-loop setpoint definition, with ML we can learn them!
A practical way to think about this is with robotics & control theory: having 3 loops. The most outer loop is motion planning (idea), which has a motion planning controller that is offline configured with a goal and contraints, and online 
receives as input state estimation and outputs an x-steps ahead trajectory to follow as a setpoint for guidance inner loop. Guidance Inner Loop (implementation 2) has a guidance controller that online receives motion planning trajectory as setpoint
and state estimation as input, and outputs state setpoint for low-level control inner loop. Low-level control (implementaiton 2) receives online the state setpoint from guidance and state estimation, and outputs control action. 
This effectivelly separates idea from implementation. Sometimes idea is correct (e.g., player wants to do a dribble) but implementaiton is not good (e.g., player makes mistake in dribble) and if you dont decouple idea from implementation
you will might dicentivize making the dribble if you dnt have enough data.
One option is to sprinkle ML on top of this framework for learning parameters & latent variables we dont know. Another more ambitious option would be to learn the loops themselves (e.g., the algorithm automatically learns 10 consecutive outter loops with their respective controllers)

More ML Models should be built to receive p(x) instead of x. This is usefull for features with noise and for feeding output into input for dynamicla systems
p(x)_{t+1} := p(y)_{t}.

* Reward function in RL: 
- instead of having a fixed reward function thoughout experience, having a varying reward function. The reward function starts very non-sparse (which facilitates learning but also might induce
non desired behaviour) and as experience goes, it gets more sparse, at the end it is just rewarding the goal itself! E.g., for football, you start with a function that rewards a player if he the ball is closer to ther other goal
and by the end you are just rewarding scoring goals (which is what you actually want).
- estimate the reward function, observation function & dynamical system --> then learn and plan in simulation 

* DL Scaling Laws. For each hyperparameter you want to scale:
	- Initial idea:
		1. Plot n points test-set performance vs target hyperparameter size of the best model (after hyperparmeter tuning of other hyperparameters), from small to medium scale.
		2. Fit a model (generally power-law) and extrapolate ceiling performance at larger sizes of target hyperparameter
		3. For each other hyperparameter: plot its value vs target hyperparameter size
		4. Fit a model (can be very weird) and extrapolate for larger sizes of target hyperparameter
	- Notes:
		1. Hyperparmeter tuning of other hyperparameters needs to be done efficiently (not grid-search), e.g.,  by sampling.

* Principles of Intelligence:

    1. Bootstrapping (Recurrent Self-Improvement)

        * Bootstrapping is one of the keys to Inteligence: "it is easier to improve when you are better"
        - Our world is basically building blocks on top of more fundamental buildinng blocks, forming a hierarchical structure. Building blocks are functions. If you try to learn directly last layer function, youget exponential explosion in parameter (hypothesis) space size, but if you do one step at a time you get aproximately constant parameter (hypothesis) space size
        - This is the reason we Inteligence needs good priors to begin with, we need to rule out (or uncetivize) the majority of hypothesis that we know are crap, so that we can converge to something good with tractable amount of data
        - A good way of getting priors is using existing working systems, one great existing working system is humans! Thus Supervised Learning is essential as as a starting point for our AIs, then they can improve upn it. Other forms of bootstrapping: learning dynamical systems & controllers from agent behaviour, searching for priors directly (e.g., convolution prior in CV), embedding domain knowledge in models, self-play, etc

    2. Learning: starting bad and improving by fitting data.

        1. Compression (representational bottlenecks): you need to impose limitations to representational power, so that the AI can focus on learning important features that generalize. Best example of this is that the major architectures all use encoder-decoder architecture.

        2. Hierarchical Representations (what NNs enable): because our world is hierarchical, it is building blocks built on top of lower level building blocks.

        3. Optimization: changing things in the direction of achieving some goal.

        4. Curriculum Learning: learn in increasing levels of diffisculty.

        5. Fine-grained feedback signals: the more in the beggining of the learning process, the more fine-grained feedback signal needs to be. Because if not, you will end up in of the endless local optima.

    3. Reasoning via Logic + Heuristics: logic enables us to propagate truth, it it enables us to produce high level statements from lower level ones. However, the search space is intractable for interesting problems. We need good hueirstics to guide the search.

    * Principles of Autonomous Intelligence:

        1. Uncertainty Estimation (includes OOD Detection) and Experimentation: it has to know what it doesnt know (or not very shure) and then gather data on these aspects.

* Principles of Autonomy:

    1. Energy harvesting: needs a way to continously get energy from the environment without external help.

    2. Self-preservation: modelling itself as an agent in the world and wanting that agent to continue to exist.

    3. Agency via sensors and actuators: being able to sense and act on the environment without external help.

    * Principles of Autonomous Intelligence:

        1. Uncertainty Estimation (includes OOD Detection) and Experimentation: it has to know what it doesnt know (or not very shure) and then gather data on these aspects, so that it can update its model to a more (interventionally and observably) accurate

* Shouldnt CNNs be outputing the position of features together with their values, before the fully conected layer? Because without this the fully conected layer
needs to hardcore every possible relative position between features that make up the final object

* Inverse Process ML: some p(y|x) tasks we know the p(x|y) (example: for p(program|graphic) we know(p(graphic|program) == f(program))), this make it possible to check if output is correct. You might say it can be seen as RL or Learning from feedback.

* \*\* Lifelong Fine-tuning: instead of getting NN A and freeze 80% for learning a related task; fix all of its parameters (units), and add new parameters (defined by scaling hyperparameters) at each layer
and then learn them. This can avoid catatrophic forgetting, by increasing the complexity of the model.

* \*\* Training with Explanations: instead of just giving (x,y) examples, also provide expplanations of why an x_i was mapped to an y_i.

* Shoudnt language Interpreters/Compilers also come with a transpilation mode? E.g., python interpreter also having a mode where instead of interpreting python code, it transpiles
python code to C. There are python tranpilers out there, but they work solely on python code; this takes away the advantage of knowing exactly how some high-level commands are implemented (of course
you can pick hardcode a good implementation, but generally this problem was already solved by the interpreters/compilers of the lannguage, which contain a very good implementation already).

* \*\* Building NNs
    1.  Bias Params in a NN should be initlaized such as the intermidiate aproximating functions (latent layers) are continuous. Because the final function you want to approximate wll generally be continuous, so you benefit you being more like it already.
    2. \# Hyperameter Tunining: estimate the ground truth expectation (E[p(y|x)]) frequency, using available data, ate grids of your domain. Then after an inital NN is built, you estimate its frequency on the same grids. Then the job is to get your NN's frequency close to the ground truth estimation on all grids.

* \*\* Transform a trained NN into a symbolic (closed-form) function by sampling a lot from it and using a symbolic regression engine. Can be very usefull for compression & explainability

* \*\* Educational Video Illustrating how function approximation works inside a NN, using a ground thruth interpretable function.

* \*\* is GPT trained on youtube transcripts? Seems like a good idea.

* \*\* NN post-training by training on each layer separetely considering all remaining layers constant (weights freezed)

* \*\* Controller Synthesis method: 
    1. Learn a NN-based model of the dynamical system
    2. Distill it into a simpler symbolic model
    3. Do automated controller design using control theory methods

* \*\* ML Optimizers:
    1. Need to recognize parameter osccilation and lower the learning rate

* \*\* Non-smooth2Smooth Model: An algorithm that receive as input: non-smooth model architecture (e.g., a python function with a bunch of ifs) and outputs a smooth aproximation of the gradient of that model (e.g., a python function with only continuous operations). JAX/FLAX can be used to get a non-smooth gradient calculator algorithm, and then the algorithm will kick in by mapping this non-smooth gradient to a smooth one.

* \*\* Watermark Generative AI Content: train simultaneously generative AI and classifier (that classifies between AI content & human content) as such that the classifier's acc gets very high & the generative AI's content does not get very derailed. This can make identification of AI content much easier.

* \*\* Shouldnt RL have a cost (negative reward) for each action taken (can also depend on which type of action)? Because then you disencentivize longer paths that get you the same reward as "equivalent" shorter paths.

* Is there a heuristic for optimal_number_of_params(input_dimension, training datapoints)? It's pretty important

* \*\* Wight initialization proposal: all weights := 0 and all biases := E_x[E_y[p(Y=y|X=x)]]

* \*\* NNs (awesome tool: https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2,2,2,2&seed=0.65499&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false):
- layer 1: its job is just to build independent hyperplanes so that layer 2 can build any hyperplane it wants
- feature functions:    
    - dense feature functions are favoured (as opposed to sparse ones) because its the way to perform complex "cutting" with just one bias (e.g., when the NN uses "having ears" and "having 4 legs" insteaf of "some combination of both having ears & legs" & "another combination (not free to choose which) of having ears & legs" as final features to classify between dogs & not dogs)
    - overlapping feature functions are favoured because they share common "base functions (function that will be used to compose both of after a specific cutting is performed and the architecture prefers this because it has limited base functions per layer) (e.g., edges that are base functions to geometric figures). And this does not necessarily mean that these overlapping parts will actually be usefull, sometime its just in the "iceberg that was cutted away from both of the biases of these units" and thus are not used to compose the feature functions of the next layer (in an ideal world these feature functions would decay to zero at the cut-out spaces, and then you woundnt need biases, but its much easier to just find any function that satifies the cut in space and then use biase to cut out the rest, so that wht the NN does).
    - at every layer you do a linear combination of the feature functions of the last layer, thus you can construct a new feature that is a contnuos analog to strict OR; AND or OR; but you cant do strict AND. E.g., we want to build a new feature that is a circle detector, and it uses discrete arc feature to compose it. With standard NNs, we give high equal eight to all arc features, bu then, we get the same intensity for full circles with low brightness as for not full circles with great brightness. We actually want the full circle with low brighness to have higher circle intensity than the other. How to do this? We need other type of layer that isnt linear (there is nice work around implicit layers!). However current NNs can circunvent his simply using more computation. They make a set of low brightness arc features that detect low brighness arc to use to build the low brighess full circle feature and they make the high brighness arc features to detect high brigness arcs and use them to build the partial circle high brighess feature. And then in te next layer you just give higher weight to low brighess full circle. Another problem is how to build th partial circle feature, because translational invariance of CNNs is not enough, we also need rotational invariance, but this this can be solved with architectures that considir this factor or by bruteforce learning all possible rotations by augmenting the data with rotations (not efficient, but easy).
- NNs (natural & artificial) work very well because they mimic or compositional world, where everething is a building block built out of smaller building blocks.

- control needs to be used for robots to remain on same state evolution function while they are thinking

- CLIP for video: giant contrasitive learning model trained with videos. Put audio and video on the same mbeddding space.

* \*\* LLM ideas:

- LLM should be trained/fine-tuned with RAG in mind, to be just a resoning machine (which also enables it to be much smaller). They should be penalized for outputing stuff that couldnt be resoned just based on input.

- Making an agent reflect on its behaviour and improve: in LLM orchestrators, you shoudlnt make vanilla LLM API calls, you should finetune it with knowledge of the workflow and put the workflow context into the prompt at each step call. Doing this enables you to then finetune the same LLM with with feedback on the whole orchestration process, which then will make each step better afterwards.

- LLM that learn model edtiing of another LLM

-  agents should be robust to making wrong decisions: if in some step of an agent program it makes a wrong decision, it shoudl detect that in the next step. How to detect: there should be a type of data specification that dictates what should the observations look like as we go in the reight direction, if observation dat astarts going to off the agent probably made a wrong decision in previous steps. But the wrong decision was made in which previous step? Then there needs to be some high level planner helping with this.

- There should be a meta embedding model that you rpovide: <content to embebed: text, type of embedding: text> and it outputs an ambedding according to the type you specified.
    - an embedding model just for hierarchy of concepts. Normal embedding models dont have the concept of hierarchy. But the concept of hierarchy is very usefull to use. Therefore, there should a be an embedding model trained on detecting similar hierarchical levels instead of similar semantics. E.g., cat would be more similar to dog than black cat.
    - an embedding model for confidence. Senteces with similar confidence level have similar representations.

- Data preprocessing for LLMs:

    - Before training the LLM with dataset D, aks it to decide which parts of the dataset are true and which ones are not. This can remove fake data automatically.

- LLMs powering RL: sing LLMs to meta-learn policies, given user explanation of the task that needs to be done. In this way, even with few episodes (training data), a good policy can be learned.
How to: train a meta model with examples (x == <few episodes, task explanation>, y == learned policy using much more episodes)

- Making GPT Handle HTML Natively so that it can geneate html pages from text instructions
    1. Transform all the data used to train GPT into HTML
    2. Resolve all the API calls within HTML
    3. Map Images files to Image Embeddings & Video files to Video Embeddings
    4. Do the same self-supervised training with 80% of the dataset
    5. Do supervised Fine-Tuning with the remaining 20% of the data with pairs <x,y> := <text instructions that should lead to the generation of the html page (in html format), html page>
    6. Do the same RLHF

- Formatted Training: making LLMs substitute task-specific ML models. Restricts output to follow a certain format. In practice, what happens is that at each time step, when before
any token could be the prediction, what happens is that only a few tokens can be predicted; the probabilities of the allowed tokens are normalized and then you sample just like before.
This method makes it possible to train an LLM on any supervised task reliably. Note: the advantage of using an LLM is that you can insert prior knowledge of the task as text to help the model converge to the right place.
After the LLM is fine-tuned, you might have think you are having too much latency and dont need a general model; then you can do knowledge distillation by getting a lot of (x, ^y) tuples and training a smaller model on them!. 

- Self-finetune: expanding on "self-instruct" paper
    input: Pretrained LLM model as model

    set topics
    set model
    set n, m
    duplicate_topic == true

    finetune(model using manual data that teaches it how to generate datapoints for a specific topic) 
    finetune(model using manual data that teaches it how to generate topics)

    Repeat n times:
        while duplicate_topic:
            new_topic == Do(Self-topic(model)) # constructing a fine-tuning dataset for each topic
            duplicate_topic == duplicate_topics(new_topic, topics)
        topics.append(new_topic)
        Repeat m times:
            new_datapoint == Do (Self-datapoint(model, new_topic)) # constructing a new <x,y> tuple for each topic where x is a prompt engineered instruction
            topics.new_topic.append(new_datapoint)
        updated_model == finetune(model on topics data, with capacity expansion)
        model == updated_model

    output: Auto-finetuned LLM model as model

- LLM Architecture should be statefull : should keep track of (and be able to reset) state vector that enables it to have long-term memory (i.e. should have a LSTM inside it).

- Explainability & Real-time tuning by capability deletion: 
1. Use LLM agent B to experiment with tweaking LLM A's architecture & pretrained parameters in order to make it lose some capability c_i (which will be evidenced by LLM A not being able to output correctly in instructions that require capability c_i) and preserve all other capabilities C - c_i. Capabilities are hierachical: c_i == f(c_j, c_k, c_l)
2. Boost some capability c_k by doing the oppositite of capability deletion

- Should have baked-in to their architecture the amount of output tokens as an input. Maybe this could be done by getting an existing oss LLM and fine-tuning it with <<x,len>; y:text with len(text) === len> with architecture support for the new input "len". The instead of training it with a next token loss, we train it with a loss that measures the difference between y embeddings (using an external eembedding model) and of course, the difference in lenght of outputs (should not be linear, loss should ramp up when lenght gap increases).

- Finetuning shouldnt predict next token, should predict next topic, topic being defined as a high level representation (embedding) that captures the topic of the next n tokens; where n doesnt need to be constant (but its easier if it is). LLMs would predict in topic space (loss measures the difference between predicted topic), and ground truth topics can be first learned via autoencoders in a prefinetuning phase. Another non-fine-tuned LLM which job is to learn syntax (can be general as english syntax or specific as the the way an english writer writes) then maps <previous tokens, topic> --> <next token> with a spearate loss function for the difference between predicted token and groud thruth token (the one we are used to, fundmaentally categorical cross entropy). This effectively decouples semantics from syntax, and lets our main LLM (semantic one) focus really on learning concepts rather than worried about getting the right words.

- Being able to understand videos (multimodal because of frames + audio) is the next milestone for building next-generation FMs.

* Changing model family and hyperparameters in-training:
    1. Efficient hyperparameter optimization
    2. Fit symbolic regression to suitable parts of the model --> substitute the symbolic expressions in the model --> do some steps of learning --> Fit symbolic regression to suitable parts of the model --> ... until you are left with a compresses model with good acc.

* Documentation Generation tools (e.g., mkdocs) should enable support a feedback mechanism where people reading the docs can make commnetary on how to improve it. 

### __Questions__

* I dont understand why in word2vec (skip gram model) embeddings arent scale invariant. Meaning that the realtive norm of the embeddings would not mean anything (but in reality they do). Because when you apply the linear transformation on the mebedding to the output space & apply softmax, you are ignoring the scale for the error (loss) calculations. Therefore embeddings with same angle & different scales should be mapped to the same vector in the output space.

* LLMs: Masking labels. Masking labels in LLMs is a capability present in the newer LLM library versions and is very usefull because it makes your life easier for setting up training data. Lets say you want to train a chatbot: then you need to give as input the whole conversation & it needs to output the last AI assistant part of the conversation. So, you would have to generate a lot of training examples containing the conversation up until that point & the AI assistant response, and keep doing this until the end of the conversational data. But if you label mask the user parts, the library will do this for you or it actually modifies the loss function to make the loss 0 when predicting the masked words, which one is it?.

* Do ML Deploy tools like Seldon Core use ML Compiler tools like Apache TVM under the hood? Or they have their own implementation?

* Is there a GPU cgroup? Meaning that you can limit the hardware resources (GPU memory & Processing %) that a GPU process can use, like is already used for CPUs.

* Open source development questions for engineers:
    - When do you decide to build a new oss tool vs contribute to an existing oss tool?
    - How do you get sponsors for your oss projects?

* Would appreciate if anyone could talk about the drawbacks of using a self-hosted FaaS (e.g., OpenFaaS) relative to vanilla Kubernetes. I tried googling it, but didn't find much. Is it just the cost of running the FaaS overhead? Or these FaaS solutions don't allow you to access low-level Kubernetes features when you need more control? Thanks in advance!
</details>

</details>

